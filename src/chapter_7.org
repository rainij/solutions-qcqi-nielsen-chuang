#+title:  Chapter 7
#+author: Reinhard Stahn
#+setupfile: ./inc/setupfile.org
#+include: ./inc/latex-macros.org
#+property: header-args:sage :session *sage-chapter-7* :tangle chapter_7.sage

#+toc: headlines 2

* Setup
** Imports
#+begin_src sage
  from utils import commutator as com
  from utils_sage import Z, H, Ry, Rz
#+end_src

** Harmonic Oscillator
To simplify calculations we will use unit-less expressions. To do this let $\tilde{x}$,
$\tilde{p}$, etc denote physical quantities /with/ units and denote the unit-less versions
without the tilde, such that e.g.

<<unitless-position-and-momentum>>
$$
  \tilde{x} = \sqrt{\frac{\hbar}{m\omega}} \; x, \quad \tilde{p} = \sqrt{m\omega\hbar} \; p .
$$

Note that this implies $p=-\ii\partial_x$. The effect on $H$ and $a$ is the same as
setting $m=\hbar=\omega=1$ (we have e.g. $H=p^2/2+x^2/2$).

In this chapter let us use the python based computer algebra system [[https://www.sagemath.org/][sage]] to solve the
exercises (some of them). For this reason we define momentum and position operator in code
as follows:

#+name: position-momentum-operator
#+begin_src sage
  R.<x> = QQbar[]
  W = DifferentialWeylAlgebra(R)
  (x, dx) = W.gens()
  p = -i*dx
#+end_src

Having this we can define the Hamiltonian $H$ of the one-dimensional harmonic oscillator
and the corresponding annihilation ($a$), and creation operator ($a^\dagger$):

#+name: commutator-ladder-operators
#+begin_src sage
  H_qho = p^2 / 2 + x^2 / 2

  a = (x + i*p) / sqrt(2)
  ad = (x - i*p) / sqrt(2)
#+end_src

Some basic sanity checks:

#+name: commutator-ladder-operators-checks
#+begin_src sage :results replace :tangle no :cache yes
  assert com(x, p) == i
  assert com(a, ad) == 1
  "PASSED"
#+end_src

#+RESULTS[7274798eb8e917891bbfa50d7dfa80ec1b7a9b47]: commutator-ladder-operators-checks
: 'PASSED'

Note that from [[unitless-position-and-momentum][above]] we get

<<unitless-hamiltonian-and-annihilator>>
$$
  \tilde{H} = \hbar\omega H, \quad \tilde{a} = a .
$$

For convenience let us also define the number operator $N=a^\dagger a = H-1/2$. Clearly
the number operator has the same eigenstates as $H$. The corresponding eigenvalues are
shifted by $1/2$.

<<spectrum-harmonic-oscillator-1d>>
- Theorem (Spectrum of 1D Harmonic Oscillator) :: Let

  $$ \ket{0} = \frac{1}{\sqrt{2\pi}} e^{-x^2 / 2} , $$

  and

  $$ \ket{n} = \frac{(a^\dagger)^n}{\sqrt{n!}} \ket{0} , $$

  for $n\in\{1,2,\ldots\}$. These states form a complete orthonormal set of the Hilbert
  space $L^2 (\RR)$ of square integrable functions. Moreover they are eigenstates of $H$
  and $N$ with $N\ket{n}=n\ket{n}$.
  - Remark :: <<gaussian-integral>> It is well known that the gaussian function which
    defines $\ket{0}$ is already normalized (see [[https://en.wikipedia.org/wiki/Gaussian_integral][gaussian integral]]).
  - Proof :: See [[#exercise-7.4-proof][solution of exercise 7.4]].

** Optical photon quantum computers
The book showed that the beamsplitter $B=e^{\theta(ab^\dagger-a^\dagger b)}$ acts as
follows on the dual-rail representation

#+name: beamsplitter-dual-rail-basis
#+begin_src sage
  theta = SR.var('theta', domain='real')
  B = matrix([[cos(theta), -sin(theta)], [sin(theta), cos(theta)]])
#+end_src

This essentially follows from these commutation relations:

<<beamsplitter-commutation-relations>>
$$
  BaB^\dagger = a \cos(\theta) + b \sin(\theta)
  \quad \text{and} \quad
  BbB^\dagger = b \cos(\theta) - a \sin(\theta) .
$$

* Exercises
** Exercise 7.1
Using the fact that $x$ and $p$ do not commute, and that in fact $[x,p]=\ii\,\hbar$,
explicitly show that $a^\dagger\,a=H/\hbar\omega-1/2$.

*** Solution
This is just boring algebra which can be done much faster (and more reliably) by /sage/:

#+name: exercise-7.1
#+begin_src sage :results replace :tangle no :cache yes
  assert ad * a == H_qho - 1/2
  "PASSED"
#+end_src

#+RESULTS[38f2058bc713985fbab62335c0887638d917a285]: exercise-7.1
: 'PASSED'

** Exercise 7.2
:PROPERTIES:
:CUSTOM_ID: exercise-7.2
:END:
Given that $[x,p]=\ii\,\hbar$, compute $[a,a^\dagger]$.

*** Solution
The unitless result is $1$:

#+name: exercise-7.2
#+begin_src sage :results replace :tangle no :cache yes
  com(a, ad)
#+end_src

#+RESULTS[78c440463e894f8531e28cce10a7a932478e9b08]: exercise-7.2
: 1

Since the [[unitless-hamiltonian-and-annihilator][creator/annihilator is already unit-less]] the same is true with units.

** Exercise 7.3
Compute $[H,a]$ and use the result to show that if $\ket{\psi}$ is an eigenstate of $H$
with energy $E\geq n\hbar\omega$, then $a^n\ket{\psi}$ is an eigenstate with energy
$E-n\hbar\omega$.

*** Solution
Using $[a^\dagger,a]=-[a,a^\dagger]=-1$ (see [[#exercise-7.2][exercise 7.2]]) we deduce (assuming unit-less quantities)

$$
  [H,a] = [a^\dagger a, a] = (a^\dagger a - a a^\dagger) a = [a^\dagger,a] a = -a .
$$

Using this we get

\begin{align*}
  H a^n \ket{\psi} &= (Ha) a^{n-1} \ket{\psi} \\
  &= ([H,a] + aH)a^{n-1} \ket{\psi} \\
  &= (-a^n + a H a^{n-1}) \ket{\psi} \\
  &= a H a^{n-1} \ket{\psi} - a^n \ket{\psi} .
\end{align*}

Recall that we want to show that $Ha^n\ket{\psi}=(E-n)a^n\ket{\psi}$ for all natural
numbers $n\leq E$. Using the above calculation we can prove this by induction. In fact,
for $n=0$ the assertion is trivial. For $n>0$ we may use the above calculation together
with the assertion for $n-1$:

$$
  H a^n \ket{\psi} = a(E-n+1)a^{n-1}\ket{\psi} - a^n \ket{\psi} = (E-n)a^n\ket{\psi} .
$$

QED.

** Exercise 7.4
Show that $\ket{n}=\frac{(a^\dagger)^n}{\sqrt{n!}}\ket{0}$.

*** Proof
:PROPERTIES:
:CUSTOM_ID: exercise-7.4-proof
:END:

The exercise statement is rather "vague" (I try to be friendly here 😉), so let us prove
some definite statement like the [[spectrum-harmonic-oscillator-1d][theorem about the spectrum of the harmonic oscillator]]
instead.

To find the /ground state/ consider

$$
  \bra{\phi} N \ket{\phi} = \norm{a\phi}^2 \geq 0 .
$$

Equality holds iff

$$
  x \phi(x) + \phi'(x) = \sqrt{2} \cdot (a\phi)(x) = 0 .
$$

This is a linear ordinary differential equation. One solution is given by
$x\mapsto\exp(-x^2/2)$, which can be easily checked by plugging this in. We do not need
this fact (since it follows automatically later on) but the solution is unique up to a
scalar factor, which can be seen by observing that the ODE is equivalent to
$\partial_x\log(\phi(x))=-x$.

The factor $1/\sqrt{2\pi}$ in the definition of $\ket{0}$ [[gaussian-integral][ensures normalization]]. We thus
proved $N\ket{0}=0$. Next let us verify that $\ket{n}$ is an eigenvector. To simplify the
notation let us define $\ket{n'}:=\sqrt{n!}\ket{n}$.

$$
  N\ket{n'} = a^\dagger a (a^\dagger)^n \ket{0} .
$$

By using $[a,a^\dagger]=1$ ([[*Exercise 7.2][exercise 7.2]]) we may move the single $a$ to the right and then
use $a\ket{0}=0$ (this procedure is just simple example of mathematical induction). This
leads to

$$
  N\ket{n'} = n (a^\dagger)^n \ket{0} = n \ket{n'} ,
$$

hence establishing that $\ket{n'}$ (and $\ket{n}$) is an eigenvector for the eigenvalue
$n$. To see that the $1/\sqrt{n!}$ in the definition of $\ket{n}$ is indeed the right
normalization factor use the freshly proved equality $N\ket{n'}=n\ket{n'}$ and consider
(don't forget $[a,a^\dagger]=1$)

$$
  \norm{(n+1)'}^2 = \bra{n'} a a^\dagger \ket{n'}
  = \bra{n'} N + 1 \ket{n'}
  = (n+1) \norm{n'}^2 .
$$

Hence, since $\ket{0}$ is already normalized mathematical induction shows that $\ket{n}$
is normalized too.

It remains to show that the $(\ket{n})$ generate the whole Hilbert space (completeness),
since orthogonality already follows from the fact that they are eigenstates to /different/
eigenvalues of some (unbounded) self-adjoint operator. This essentially boils down to show
that finite linear combinations of the eigenstates are dense in $L^2(\RR)$.

We only sketch the proof since this reaches out to other parts of mathematics. First of
all observe that

$$
  \ket{n}(x) = \frac{1}{\sqrt{2\pi}} \; p_n(x) \; e^{-x^2/2} ,
$$

where the $p_n(x)$ are polynomials which satisfy the recursive relation

$$ p_{n+1}(x)=(2x+\partial_x)p_n(x), \quad p_0(x) = 1 . $$

These are the well-known [[https://en.wikipedia.org/wiki/Hermite_polynomials][hermite polynomials]]. From the recursive relation we directly see
(inductively) the /for our purpose important/ property that $p_n$ has degree $n$ (with
leading factor $2^n$). The density now follows from the following well known results

1. <<ex-7.4-mollification>> The compactly supported continuous functions are dense in
   $L^2(\RR)$. I do not have a good resource on that but one way to see this is by a
   method called /[[https://en.wikipedia.org/wiki/Mollifier][mollification]]/.
2. <<ex-7.4-weierstrass-approx>> For any finite interval $I$ the polynomials on that
   interval are dense in the continuous function on $I$ with respect to the
   $L^\infty$-norm which is stronger than the $L^2$ norm (since the interval is
   finite). This is the [[https://en.wikipedia.org/wiki/Stone%E2%80%93Weierstrass_theorem][Stone-Weierstrass Theorem]].

By [[ex-7.4-weierstrass-approx]] finite linear combinations of eigenstates can approximate any
compactly supported continuous function. Here it is important that any degree is
represented among the eigenstates. It should be obvious that the factor $e^{-x^2/2}$ does
not disturb this argument at all. Now completeness directly follows from
[[ex-7.4-mollification]]. QED.

** Exercise 7.5
Verify that Equations (7.11) and (7.12) are consistent with (7.10) and the normalization
condition $\norm{\ket{n}}^2=1$.

*** Solution
It is not hard to see that (7.11) and (7.12) imply (7.10):

$$
  a^\dagger a \ket{n} = a^\dagger \sqrt{n} \ket{n-1} = n \ket{n} .
$$

The normalization condition is not relevant here. So it is indeed true that it is
consistent with the three equations but for trivial reasons.

** Exercise 7.6 (Eigenstates of photon annihilation)
Prove that a coherent state is an eigenstate of the photon annihilation operator, that is,
show $a\ket{\alpha}=\lambda\ket{\alpha}$ for some constant $\lambda$.

*** Proof
Recall that $a\ket{n}=\sqrt{n}\ket{n-1}$ for $n\geq1$ and $a\ket{0}=0$. Hence

$$
  a\ket{\alpha} = e^{-\abs{\alpha}/2} \sum_n \frac{\alpha^n}{\sqrt{n}} \, a \, \ket{n}
  = e^{-\abs{\alpha}/2} \sum_n \frac{\alpha^{n+1}}{\sqrt{n}} \ket{n}
  = \alpha \ket{\alpha} .
$$

The claim follows with $\lambda=\alpha$. QED.

** Exercise 7.7
Show that the circuit below transforms a dual-rail state by

$$
  \ket{\psi_{\mathrm{out}}} = \begin{bmatrix} e^{\ii\pi} & 0 \\ 0 & 1 \end{bmatrix} \ket{\psi_{\mathrm{in}}} .
$$

if we take the top wire to represent the $\ket{01}$ mode, and $\ket{10}$ the bottom mode,
and the boxed $\pi$ to represent a phase shift by $\pi$.

- Remark :: The book contains a picture showing the circuit.

*** Solution
By definition the circuit acts like this (we use $e^{\ii\pi}=-1$ to simplify notation):

$$
  \ket{0_L} = \ket{01} \mapsto -\ket{0_L}, \quad
  \ket{1_L} = \ket{10} \mapsto \ket{1_L}.
$$

The matrix representing this linear transformation with respect to the basis
$(\ket{0_L},\ket{1_L})$ (the order is important) is

\begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix}

as desired.

** Exercise 7.8
Show that $P\ket{\alpha}=\ket{\alpha e^{\ii\Delta}}$ where $\ket{\alpha}$ is a coherent
state (note that, in general, $\alpha$ is a complex number!).

*** Proof
The phase shift operator is given by $P\ket{n}=e^{\ii n\Delta}\ket{n}$. Hence, using $\abs{e^{\ii\Delta}}=1$

$$
P\ket{\alpha} = e^{-\abs{\alpha}^2/2} \sum_n \frac{\alpha^n \, e^{\ii n\Delta}}{\sqrt{n!}} \ket{n}
= \ket{\alpha e^{\ii\Delta}} .
$$

QED.

** Exercise 7.9 (Optical Hadamard gate)
Show that the following circuit acts as a Hadamard gate on dual-rail single photon states,
that is, $\ket{01}\mapsto(\ket{01}+\ket{10})/\sqrt{2}$ and
$\ket{10}\mapsto(\ket{01}-\ket{10})/\sqrt{2}$ up to an overall phase.

- Remark :: The book contains a picture showing the circuit.

*** Solution
Assuming evolution from left to right the circuit implements $ZB_{\theta=\pi/4}$ (note
that the phase shift is $Z$ up to a global phase). Below you can see that then the
assertion of the exercise is wrong. We give two alternatives instead.

#+begin_src sage :results replace :tangle no :cache yes
  B0 = B.subs(theta=pi/4)
  B1 = B.subs(theta=-pi/4)

  # This shows that the original circuit does not implement H, not even if we neglect a
  # global phase
  f"sqrt(2)*H != {sqrt(2) * Z * B0}"

  # These alternatives implement the Hadamard gate:
  assert B0 * Z == H
  assert Z * B1 == H
  "PASSED"
#+end_src

#+RESULTS[04d7c5cbd0a5ca296a6d967664b6e4facfdcd1d6]:
: 'sqrt(2)*H != [ 1 -1]\n[-1 -1]'
: 'PASSED'

** Exercise 7.10 (Mach–Zehnder interferometer)
Interferometers are optical tools used to measure small phase shifts, which are
constructed from two beamsplitters. Their basic principle of operation can be understood
by this simple exercise.


1. Note that this circuit performs the identity operation:

   $$ B_{\theta=\pi/4}^\dagger \cdot B_{\theta=\pi/4} $$
2. Compute the rotation operation (on dual-rail states) which this circuit performs, as a
   function of the phase shift $\varphi$:

   $$ B_{\theta=\pi/4}^\dagger \cdot R_z(-\varphi) \cdot B_{\theta=\pi/4} $$


- Remark :: The book contains actual circuits drawings instead of algebraic
  expressions. The second expression neglects the global phase (as is common).


*** Solution
Recall that we have $B=B(\theta)=R_y(2\theta)$. Hence $B_{\theta=\pi/4}$ is a rotation
around the y-axis by ninety degrees. This maps the x-axis onto the reversed z-axsis. Hence
the (reversed) z-rotation in the middle effectively acts like a rotation around the x-axis
by an angle $+\varphi$. The net result is $R_x(\varphi)$.

Such reasoning easily leads to subtle errors. Therefore let us verify this using sagemath:

#+begin_src sage :results replace :tangle no :cache yes
  B0 = B.subs(theta=pi/4)
  phi = SR.var('phi', domain='real')

  assert B0.H * Rz.subs(theta=-phi) * B0 == Rx.subs(theta=phi)
  "PASSED"
#+end_src

#+RESULTS[d181afcb2e3cccc86669ae1b88ef476fc645b9a9]:
: 'PASSED'

It worked out 😎!

** Exercise 7.11
What is $B\ket{2,0}$ for $\theta=\pi/4$?

*** Solution
Following the book we basically identify states with products of creation operators and
then use the [[beamsplitter-commutation-relations][commutation relations]] for the beamsplitter and $B\ket{0,0}=\ket{0,0}$:

$$
  B\ket{2,0} = \frac{1}{\sqrt{2}} B (a^\dagger)^2 \ket{0,0}
  = \frac{1}{\sqrt{2}} \left(a^\dagger \cos(\theta) + b^\dagger \sin(\theta) \right)^2 \ket{0,0} .
$$

Now it helps that $a^\dagger$ and $b^\dagger$ commute, which implies

$$
  \left(a^\dagger \cos(\theta) + b^\dagger \sin(\theta) \right)^2
  = (a^\dagger)^2 \cos(\theta)^2 + (b^\dagger)^2 \sin(\theta)^2 + 2a^\dagger b^\dagger \sin(\theta)\cos(\theta) .
$$

Using $2\sin(x)\cos(x)=\sin(2x)$ we obtain:

$$
  B\ket{2,0} = \cos(\theta)^2 \ket{2,0} + \sin(\theta)^2 \ket{0,2} +
  \frac{\sin(2\theta)}{\sqrt{2}} \ket{1,1} .
$$

Plugging in $\theta=\pi/4$ we obtain

$$
  B_{\theta=\pi/4} \ket{2,0} = \frac{1}{2} \ket{2,0} + \frac{1}{2} \ket{0,2} +
  \frac{1}{\sqrt{2}} \ket{1,1}
$$

** Exercise 7.12 (Quantum beamsplitter with classical inputs)
:PROPERTIES:
:CUSTOM_ID: exercise-7.12
:END:
What is $B\ket{\alpha,\beta}$ where $\ket{\alpha}$ and $\ket{\beta}$ are two coherent
states as in Equation (7.16)? (Hint: recall that
$\ket{n}=\frac{(a^\dagger)^n}{\sqrt{n!}}\ket{0}$.)

*** Solution
Observe that

$$
  \ket{\alpha} = e^{-\abs{\alpha}^2/2} \sum_n \frac{\alpha^n}{\sqrt{n!}} \ket{n}
  = e^{\alpha a^\dagger - \abs{\alpha}^2/2} \ket{0} .
$$

There is one subtle thing to note here. The operator $(a^\dagger)$ is neither bounded nor
normal. So it is not clear if $e^{\alpha a^\dagger}$ is well defined as an
operator.

Let us give a sketch of how one might make the exponential function rigorous. Let us
define the (large) Hilbert space spanned by the $\ket{n}$ but with scalar product
(implicitly) defined by $\sprod{n}{n}_1=(n!)\inv$. With respect to this Hilbert space
$a^\dagger$ is bounded. Hence we can define $e^{\alpha a^\dagger}$ by the usual
exponential series (we need the space for convergence).

Another way to deal with the problem is to just see it as an abbreviation. In fact, we
only ever apply such operators to $\ket{0,0}$.

In the following let us abreviate $K=e^{-(\abs{\alpha}^2+\abs{\beta}^2)/2}$,
$c=\cos(\theta)$, and $s=\sin(\theta)$. Now let us go on with the calculation:

\begin{align*}
  B \ket{\alpha,\beta}
  &= K B e^{\alpha a^\dagger + \beta b^\dagger} \ket{0,0} \\
  &= K B e^{\alpha a^\dagger + \beta b^\dagger} B^\dagger B \ket{0,0} \\
  &= K \exp(\alpha B a^\dagger B^\dagger + \beta B b^\dagger B^\dagger) \ket{0,0} \\
  &= K \exp(\alpha[ca^\dagger + sb^\dagger] + \beta[cb^\dagger-sa^\dagger]) \ket{0,0} \\
  &= K \exp((\alpha c - \beta s)a^\dagger + (\alpha s + \beta c)b^\dagger) \ket{0,0} \\
  & = \ket{\alpha c - \beta s, \alpha s + \beta c} .
\end{align*}

Hence the operation of $B$ on coherent states is again given by a rotation matrix:

$$
  B: \begin{pmatrix} \alpha \\ \beta \end{pmatrix} \mapsto
  \begin{bmatrix} c & -s \\ s & c \end{bmatrix} \begin{pmatrix} \alpha \\ \beta \end{pmatrix} .
$$

** Exercise 7.13 (Optical Deutsch–Jozsa quantum circuit)
In Section 1.4.4 (page 34), we described a quantum circuit for solving the one-bit
Deutsch–Jozsa problem. Here is a version of that circuit for single photon states (in the
dual-rail representation), using beamsplitters, phase shifters, and nonlinear Kerr media:

: The book contains a drawing of the circuit at this place.

1. Construct circuits for the four possible classical functions $U_f$ using Fredkin gates and
   beamsplitters.
2. Why are no phase shifters necessary in this construction?
3. For each $U_f$ show explicitly how interference can be used to explain how the quantum
   algorithm works.
4. Does this implementation work if the single photon states are replaced by coherent
   states?

*** Solution to 1
Recall that $U_f$ implements the following operation on the logical qubits:

$$
  U_f \ket{x,y} = \ket{x,y\oplus f(x)} .
$$

Recall from box 7.4 that the optical Fredkin gate

$$ \exp\left(\frac{\pi}{2} c^\dagger c (a^\dagger b - b^\dagger a)\right) $$

can be used two implement two different logical gates:

- A =CX= gate in the dual-rail representation.
- A =CSWAP= (the /actual/ Fredkin gate) if operating directly on the physical qubits.

There are four possible functions (two constant, two balanced). Let us sketch how to
implement each of them.

$$
  f_1(x) = 0, \quad f_2(x) = 1, \quad f_3(x) = x, \quad f_4(x) = \neg x .
$$

1. $f(x)=0$ can be implemented trivially by the empty circuit.
2. $f(x)=1$ can accomplished by performing =X= on the second rail. This in turn can be
   implemented by a =CX= and an ancilla initialized to =1=. We can use the optical Fredkin
   gate in the dual-rail representation as mentioned above to do the =CX=.
3. $f(x)=x$ just needs a =CX= which directly be implemented by the optical Fredkin gate in
   the dual rail representation.
4. $f(x)=\neg x$ just needs a =CX= conjugated by =X= in the control. Both types of gates
   can be implemented as shown above.

So overall, we could do this with the /optical/ Fredkin gate alone!

*** Solution to 2
That $U_f$ can be implemented without phase shifts should be clear since controlled =X=
gates (this is what the optical Fredkin does on the dual-rail represenation as we have
just seen) are sufficient to implement any boolean function.

So let us turn to the question "why" no phase shifts are needed in the rest of the
circuit. More precisely we show that the circuit leads to the same measurement statistics
as the Deutsch-Josza algorithm (and so is identical for all practical purposes). The
circuit implements

$$
  B^\dagger \otimes I \cdot U_f \cdot B \otimes B .
$$

The original Deutsch-Josza algorithm is the same, but with $B$ replaced by $H$. The
initial state $\ket{01_L}$ is the same. But recall that $B=B_{\theta=\pi/4}=HZ$. The $Z$
would be implemented by a phase shift. But we actually do not need them since in this
particular setting at the intial state they just produce a global phase $-1$ and at the
final state it does not matter since we measure in the computational base anyway).

*** Solution to 3
Not sure what this exercise even means. If it means to show that the circuit actually
implements the Deutsch-Josza algorithm: then this was done in the solution to 2.

*** Solution to 4
No it does not work with coherent states. No matter how the measurement outcome is
interpreted there is always a non-zero chance of failure for some $f$ (recall that the
Deutsch-Josza algorithm is one of a few /quantum/ algorithms with a 100% success
probability).

To see this consider the constructions of $U_f$ from part 1 (we need to look at a
particular implemenation since the action on coherent states might be different for
different implementation). We only look at the cases $f(x)=0$ (constant) and $f(x)=x$
(balanced).

Let us assume that the intial state is $\ket{\alpha,\beta,\gamma,\delta}$.  Let us
abbreviate $\phi_{\pm}=(\gamma\pm\delta)/\sqrt{2}$. In the constant case the final state
is (see [[#exercise-7.12][exercise 7.12]] for the action of beamsplitters on coherent states)

$$
  \ket{\alpha, \beta, \phi_-, \phi_+} ,
$$

In the balanced case, in addition there is also an /optical/ Fredkin gate applied =a=, =b=
with control at =d=. In this case the final state is:

$$
  e^{\frac{\pi}{2} d^\dagger d(a^\dagger b - ab^\dagger)} \ket{\alpha,\beta,\phi_-,\phi_+}
  = e^{-\abs{\phi_+}^2/2} \sum_n \frac{\phi_+^n}{\sqrt{n!}} e^{\frac{\pi n}{2}(a^\dagger b - ab^\dagger)}
    \ket{\alpha,\beta,\phi_-,n} .
$$

Note that the Fredkin gate acts as a controlled beamsplitter, and for this particular angle we have

$$
  e^{\frac{\pi n}{2}(a^\dagger b - ab^\dagger)} \ket{\alpha,\beta,\phi_-,n} = \begin{cases}
     \ket{\alpha,\beta,\phi_-,n} & \text{for } n \text{ even,} \\
     \ket{-\beta,\alpha,\phi_-,n} & \text{for } n \text{ odd.} \end{cases}
$$

Note that there is always a non-zero probability that $n=0$ (even) is measured in $d$. In
that case the measurement statistics for the other three rails is the same as for the
constant case. In particular there are measurement results which match both, the constant
and the balanced case. Hence an exact algorithm with 100% success probability is not
possible. QED.

** Exercise 7.14 (Classical cross phase modulation)
To see that the expected classical behavior of a Kerr medium is obtained from the
deﬁnition of $K$, Equation (7.41), apply it to two modes, one with a coherent state and
the other in state $\ket{n}$; that is, show that

$$
  K \ket{\alpha}\ket{n} = \ket{\alpha e^{\ii\chi Ln}}\ket{n} .
$$

Use this to compute

\begin{align*}
  \rho_a &= \ptrace{b}{K\ket{\alpha}\ket{\beta}\bra{\beta}\bra{\alpha} K^\dagger} \\
  &= e^{-\abs{\beta}^2} \sum_m \frac{\abs{\beta}^{2m}}{m!} \proj{\alpha e^{\ii\chi Lm}} .
\end{align*}

and show that the main contribution to the sum is for $m=\abs{\beta}^2$.

*** Proof of the first part
Let us abbreviate $\xi=\chi L$ and $J=a^\dagger ab^\dagger b$. We have

$$
  \ket{\alpha}\ket{n} = e^{-\abs{\alpha}^2/2} \, (b^\dagger)^n \, e^{\alpha a^\dagger} \ket{0,0} .
$$

Hence we are interested to compute

$$
  K (b^\dagger)^n \, e^{\alpha a^\dagger} K^\dagger
  = K (b^\dagger)^n K^\dagger \, e^{\alpha K a^\dagger K^\dagger} .
$$

By the CBH formula we have

$$
  K a^\dagger K^\dagger = \sum_n \frac{(\ii\xi)^n}{n!} [(J)^n,a^\dagger] .
$$

For that reason let us look at

$$
  [J,a^\dagger] = [a^\dagger a, a^\dagger] b^\dagger b = a^\dagger [a, a^\dagger] b^\dagger b
  = a^\dagger b^\dagger b .
$$

Hence the iterated commutator is $[(J)^n,a^\dagger]=a^\dagger(b^\dagger b)^n$. Let
$P_b=e^{\ii\xi\,b^\dagger\,b}$. Then

$$
  K a^\dagger K^\dagger = a^\dagger P_b .
$$

Similarly

$$
  K b^\dagger K^\dagger = b^\dagger P_a .
$$

Therefore

$$
  K (b^\dagger)^n \, e^{\alpha a^\dagger} K^\dagger
  = (b^\dagger)^n P_a^n \, e^{\alpha a^\dagger P_b} .
$$

Finally, using $P_a^n\ket{k}=e^{\ii\xi\,nk}\ket{k}$, we obtain

$$
  K \ket{\alpha} \ket{n}
  = e^{-\abs{\alpha}^2/2} (b^\dagger)^n P_a^n \, e^{\alpha a^\dagger P_b} \ket{0,0}
  = e^{-\abs{\alpha}^2/2} (b^\dagger)^n \, e^{\alpha e^{\ii\xi\,n} a^\dagger} \ket{0,0}
  = \ket{\alpha e^{\ii\xi\,n}} \ket{n} .
$$

QED.

*** Proof of the second part
Using the first part we get

\begin{align*}
  \rho_b
  &= \sum_{mn} \ptrace{b}{K \ket{\alpha} \frac{\beta^{m+n}}{\sqrt{m!n!}} \ket{m}\bra{n} \bra{\alpha} K^\dagger} \\
  &= \sum_{mn} \ptrace{b}{\ket{\alpha e^{\ii\xi\,m}} \frac{\beta^{m+n}}{\sqrt{m!n!}} \ket{m}\bra{n} \bra{\alpha e^{\ii\xi\,n}}} \\
  &= \sum_{m} \frac{\beta^{2m}}{m!} \proj{\alpha e^{\ii\xi\,m}} .
\end{align*}

To show the final claim consider the function

$$ g(m) = \log(b^{2m}/m!) . $$

For simplicity let us assume $\beta\geq0$ (to avoid writing $\abs{\beta}$ all the
time). We have to find the maximum of $g$. I put the logarithm there to simplify
differentiating this function. By Stirlings formula we have

$$ g(m) \approx m (1 + 2\log{\beta} - \log{m}) . $$

The derivative is $g'(m)=2\log{\beta}-\log{m}$. This is zero iff $m=\beta^2$. Hence the
biggest contribution to $\rho_b$ comes from a state close to $\ket{\alpha\,e^{\ii\xi\abs{\beta}^2}}$. QED.
** Exercise 7.15
Plot (7.55):

$$
  P_{\mathrm{cav}}/P_{\mathrm{in}} = \frac{1 - R_1}{\abs{1 + e^{\ii\varphi}\sqrt{R_1R_2}}^2}
$$

as a function of field detuning $\varphi$, for $R_1=R_2=0.9$.

*** Solution
Here is the code to make the plot in =sage=:

#+name: exercise-7.15
#+begin_src sage
  R1, R2 = SR.var('R1 R2', domain='positive')
  phi = SR.var('phi', domain='real')

  power = (1 - R1) / abs(1 + exp(i*phi)*sqrt(R1*R2))**2

  def make_plot_ex715(r1, r2=None, interval=(0, 2), **kwargs):
      """Plot P_cav/P_in according to exercise 7.15 but with s=phi/pi on the x-axis."""
      s = SR.var('s', domain='real')
      return parametric_plot((s, power.subs(phi=pi*s, R1=r1, R2=r2 or r1)), (s,) + interval, **kwargs)
#+end_src

You can use it like so:

#+begin_src sage :tangle no :eval no
  show(make_plot_ex715(0.9, interval=(0.5, 1.5), aspect_ratio='automatic'))
#+end_src

#+caption: Fabry-Perot cavity, relative power $P_{\mathrm{cav}}/P_{\mathrm{in}}$ inside cavity vs $\varphi/\pi$.
[[file:images/exercise_7_15_plot.png]]

As we can see $P_{\mathrm{cav}}/P_{\mathrm{in}}$ is maximal at $\varphi=\pi$ with value
$(1-R)^{-1}=10$. Let us also note that at $\varphi=0$ and $\varphi=2\pi$ the value is
roughly $(1-R_1)/4$ if $R_1,R_2\approx1$. Moreover the width of the central peak is around
$R_1\inv$ as can be easily verified by plugging in the Taylor expansion of
$e^{\ii\varphi}$ at $\varphi=\pi$ into the formula for $P_{\mathrm{cav}}/P_{\mathrm{in}}$.

** Exercise 7.16 (Electric dipole selection rules)
Show that (7.60):

$$
  \int Y_{l_1m_1}^* Y_{1,\pm1} Y_{l_2m_2} \dd \Omega
$$

is non-zero only when $m_2-m_1=\pm1$ and $\Delta\,l=\pm1$.

- Remark :: More precisely: Let $a\in\{+1,-1\}$ and replace the middle function by
  $Y_{1a}$. Then the integral does not vanish if and only if

  $$
  m_2 - m_1 = a \text{ and } \abs{l_2-l_1} = 1 .
  $$

  That is, the sign of $\Delta\,m$ does depend on which of the two $Y_{1,\pm1}$ is chosen
  but the sign of $\Delta\,l$ does not.

*** Proof
All relevant functions involve relatively complicated constant factors. Since we are only
interested in the question whether the integral is non-zero or not, let us introduce a
specific notation for this exercise: We write $a\sim b$ if $a=cb$ for some /non-zero/ and
/constant/ $c$. By constant I mean that $c$ does not depend on $\theta$ or $\varphi$ (the
variables we integrate over).

Let $a=\pm1$ and note that $Y_{1,a}\sim\sin(\theta)e^{a\ii\varphi}$. By
$\dd\Omega=\sin(\theta)\dd\theta\dd\varphi$ the definition of the spherical harmonics in
terms of the [[https://en.wikipedia.org/wiki/Associated_Legendre_polynomials][associated Legendre polynomials]] we have:

\begin{align*}
  \int Y_{l_1m_1}^* Y_{1,\pm1} Y_{l_2m_2} \dd \Omega
  &\sim \int_0^{\pi} \int_0^{2\pi} Y_{l_1m_1}^* Y_{l_2m_2} \sin(\theta)^2 e^{a\ii\varphi} \dd\varphi\dd\theta \\
  &\sim \int_{-1}^{+1} \int_{0}^{2\pi} P_{l_1m_1}^*(x) P_{l_2m_2}(x) \sqrt{1-x^2} e^{\ii\varphi(m_2-m_1+a)} \dd\varphi \dd x \\
  &\sim: I_1 .
\end{align*}

The integral over $\varphi$ vanishes iff $m_1=m_2+a$. This already proves the first part
of the exercise. Let $b=(a+1)/2$. The question now translates to the task to find out when
the following integral vanishes:

$$
  I_1 \sim I_2 := \int_{-1}^{+1} (1-x^2)^{m_2+b} \partial_x^{m_2+a+l_1} (x^2-1)^{l_1} \partial_x^{m_2+l_2} (x^2-1)^{l_2} \dd x .
$$

Let us rewrite this in terms of the "normal" [[https://en.wikipedia.org/wiki/Legendre_polynomials][Legendre polynomials]]
$P_l(x)\sim\partial_x^l(x^2-1)^l$:

$$
  I_2 \sim I_3(b) :=
  \int_{-1}^{+1} (1-x^2)^{m_2+b} \partial_x^{m_2+a} P_{l_1}(x) \cdot \partial_x^{m_2} P_{l_2}(x) \dd x .
$$

Let us define the differential operator $D_n=\partial_x^n(1-x^2)^n\partial_x^n$. Moreover,
write $\tilde{P}_l(x)\sim\partial_x^{l-1}(x^2-1)^l$ (this is the Legendre polynomial of
order $l$ for $l\,>\,0$ where one derivative was "stolen"). Integration by parts $m_2+b$
times (in one or in the other direction) yields:

<<exercise-7.16-b-0>>
$$
  I_3(b=0) \sim \int_{-1}^{+1} \tilde{P}_{l_1}(x) \cdot D_{m_2} P_{l_2}(x) \dd x .
$$

assuming $l_1\neq0$, and

<<exercise-7.16-b-1>>
$$
  I_3(b=1) \sim \int_{-1}^{+1} D_{m_2+1} P_{l_1}(x) \cdot \tilde{P}_{l_2}(x) \dd x .
$$

assuming $l_2\neq1$.

As said these formulas are only valid for $l_1\neq0$ or $l_2\neq0$ respectively. This
however is not a big issue since these special cases are very simple. Let us briefly
consider the case $b=l_1=0$. It follows that $m_1=0$ and hence $m_2=m_1+1=1$ and
$l_2\geq1$. Hence the integral simplifies to
$\int(1-x^2)\partial_x\,P_{l_2}(x)\dd\,x$. Using integration by parts (once) we see that
this is only ever non-zero if $l_2=1$, proving the claim for this special case (we use
that $P_1(x)\sim\,x$ and take the orthogonality relations for the Legendre polynomials for
granted). The case $b=1$ is similar.

Let us come back to the above formulas for [[exercise-7.16-b-0][$b=0$]] and [[exercise-7.16-b-1][$b=1$]]. We have the following

- Lemma :: <<exercise-7.16-lemma>> $P_l$ is an eigenfunction of $D_n$. More precisely
  $D_nP_l=\lambda_{nl}P_l$ where

  $$
  \lambda_{nl} = \prod_{j=0}^{n-1} \left( j(j+1) - l(l+1) \right) .
  $$

  - Remark :: Note that $\lambda_{nl}=0$ for $n\geq\,l+1$.

Before we prove the Lemma let us see why this helps us. We only have to consider the case
$b=0$ since the other case is the same. Using the lemma we see that

$$
  I_3(b=0) \sim \int_{-1}^{+1} \tilde{P}_{l_1}(x) \cdot P_{l_2}(x) \dd x .
$$

Recall that the Legendre polynomial form an orthogonal (and complete) set of functions of
the Hilbert space $L^2(-1,+1)$. We will show that the above "skewed" version of the scalar
product is non-zero iff $\abs{l_1-l_2}=1$ (yes there are two values of $l_2$ for each
$l_1\geq1$).

The idea to see this is a case analysis. We use integration by parts below. Please observe
that the reason that no boundary terms are introduced by this is precisely because one of
the derivatives is "missing".

- Case $l_1\geq\,l_2+1$ :: Integration by parts yields

  $$
  I_3(b=0) \sim \int_{-1}^{+1} (x^2-1)^{l_1} \partial_x^{l_1+l_2-1} (x^2-1)^{l_2} \dd x.
  $$

  Clearly the term with the many derivatives vanishes if $l_1\,>\,l_2+1$. On the other
  hand it is a constant if $l_1=l_2+1$. Hence the integral is non-zero iff $l_1=l_2+1$.
- Case $l_2\geq\,l_1+1$ :: This case is analogous to the first case. The only difference is
  that we move the derivative to the other side. The integral is non-zero iff $l_2=l_1+1$.
- Case $l_1=l_2+1=:l$ :: In that case we can move all derivatives to either side and get

  $$
  I_3(b=0) \sim \int_{-1}^{+1} x(x^2-1)^l = 0,
  $$

  since the integrand is an odd function (meaning $f(-x)=-f(x)$) over a symmetric
  interval.

It remains to do the following

- Proof of the [[exercise-7.16-lemma][lemma]] :: We prove this by mathematical induction. The case $n=0$ is trivial
  ($D_0$ is the identity and $\lambda_{0l}=1$). Let us assume the claim for an $n\geq0$
  and consider

  $$
  D_{n+1} P_l = \partial_x^{n+1} (1-x^2)^{n+1} \partial_x^{n+1} P_l .
  $$

  Note that $D_{n+1}$ consists of three blocks, the left and right one contain only
  derivatives and the middle one is a function of the position operator. The idea is to
  commute one of the derivatives from the right block through the middle block and
  similarly one factor $(1-x^2)$ from the middle block through the left block (including
  the one derivative which was moved there from the right block). This should lead to an
  expression of the form $(\ldots)D_nP_l$ and we can apply the induction hypothesis.

  Let us start:

  \begin{align*}
  D_{n+1} P_l &= \partial_x^{n+2} (1-x^2)^{n+1} \partial_x^{n} P_l +
      2(n+1)\partial_x^{n+1} x(1-x^2)^{n} \partial_x^{n} P_l \\
  &=: A + 2(n+1)B .
  \end{align*}

  Next, let us treat $B$ by commuting the $x$ to the left:

  \begin{align*}
  B &= x\partial_x D_n P_l + (n+1)D_nP_l \\
  &= \lambda_{nl} \left(xP_l' + (n+1)P_l \right) .
  \end{align*}

  Now we go on with $A$. We first make a simple thing: just take one of the factors
  $(1-x^2)$ and split the some with $1$ and $-x^2$:

  \begin{align*}
  A &= \partial_x^2 D_n P_l - \partial_x^{n+2} x^2 (1-x^2)^n \partial_x^n P_l \\
  &=: \lambda_{nl} P_l'' - A_1 .
  \end{align*}

  For $A_1$ we try to commute the $x^2$ to the left:

  \begin{align*}
  A_1 &= x^2 \partial_x^2 D_nP_l + 2(n+2) x \partial_x D_nP_l + (n+2)(n+1)D_nP_l \\
  &= \lambda_{nl} \left(x^2P_l'' + 2(n+2)xP_l' + (n+2)(n+1)P_l  \right) .
  \end{align*}

  Gathering what we obtained leads to

  $$
  D_{n+1} P_l = \lambda_{nl} \left((1-x^2)P_l'' - 2x P_l' + n(n+1) P_l \right) .
  $$

  Recall /Legendre's differential equation/:

  $$
  (1-x^2)P_l'' - 2xP_l' + n(n+1)P_l = 0 .
  $$

  I won't prove it (it is well known) but let me mention the following. Legendre's
  differential equation is actually the special case $n=1$: $D_1P_l=-l(l+1)P_l$. I am
  confident that one can prove it by induction on $l$ (I did not check this claim
  though). The case $l=0$ is trivial. The induction step can probably be proved by moving
  around derivatives and position operators as we have seen above.

  Using Legendre's differential equation we obtain

  $$
  D_{n+1} P_l = \lambda_{nl} \left(n(n+1) - l(l+1)\right) P_l ,
  $$

  which essentially proves the claim. QED.

*** Appendix
Before I finally managed to find the solution to this exercise I had to do a lot of
experiments with sage math. I do not want to collect them all here but let me just give
one example.

I was a bit surprised by the fact that the [[exercise-7.16-lemma][lemma]] should be true. By what the exercise
demanded to show and what I already managed to prove it had to be true. On the other hand
I typically do a lot of mistakes when doing such calculations so I needed a way to verify
the claim for concrete values of $n$ and $l$. It turned out that my experiments backed the
lemma and I could go on proving it.

#+name: exercise_7.16_appendix-1
#+begin_src sage
  # We have to call it xx to not overwrite the position operator.
  xx = SR.var('x', domain='real')

  def Pl(l):
      """Legendre polynomial of order l."""
      q = xx^2 - 1
      return diff(q^l, xx, l)

  def Dn(f, n=1):
      """Differential operator dx^n (1-x^2)^n dx^n."""
      return diff((1-xx^2)^n * diff(f, xx, n), xx, n)


  def lambda_nl(n, l):
      """Eigenvalues: DnPl=lambda_nl*Pl."""
      result = 1
      for j in range(n):
          result *= j*(j+1) - l*(l+1)
      return result
#+end_src

Test the claim of the lemma:

#+name: exercise_7.16_appendix-2
#+begin_src sage :results replace :tangle no :cache yes
  for l in range(6):
      for n in range(l+1):
          q = Dn(Pl(l), n) - lambda_nl(n, l)*Pl(l)
          assert q == 0
  "PASSED"
#+end_src

#+RESULTS[021c6f7f8020eb04cfac32a360208225ade56f4d]: exercise_7.16_appendix-2
: 'PASSED'

** Exercise 7.17 (Eigenstates of the Jaynes–Cummings Hamiltonian)
Show that

\begin{align*}
  \ket{\chi_n} &= \frac{1}{\sqrt{2}} \left[ \ket{n,1} + \ket{n+1,0} \right] \\
  \ket{\overline{\chi}_n} &= \frac{1}{\sqrt{2}} \left[ \ket{n,1} - \ket{n+1,0} \right]
\end{align*}

are eigenstates of the Jaynes–Cummings Hamiltonian (7.71) for $\omega=\delta=0$, with the
eigenvalues

\begin{align*}
  H \ket{\chi_n} &= g \sqrt{n+1} \; \ket{\chi_n} \\
  H \ket{\overline{\chi}_n} &= -g \sqrt{n+1} \; \ket{\overline{\chi}_n}
\end{align*}

where the labels in the ket are $\ket{\mathrm{field},\mathrm{atom}}$.

*** Proof
Recall

$$
  \sigma_{+} = \begin{pmatrix} 0 & 0 \\ 1 & 0  \end{pmatrix} \quad \text{and} \quad
  \sigma_{-} = \begin{pmatrix} 0 & 1 \\ 0 & 0  \end{pmatrix} .
$$

Using this with the [[spectrum-harmonic-oscillator-1d][theorem on the ladder operators]] for the harmonic oscillator yields

\begin{align*}
  a^\dagger \sigma_{-} \ket{n,1}   &= \sqrt{n+1} \; \ket{n+1,0}, \\
  a^\dagger \sigma_{-} \ket{n+1,0} &= 0, \\
  a \sigma_{+ } \ket{n,1}          &= 0, \\
  a \sigma_{+} \ket{n+1,0}         &= \sqrt{n+1} \; \ket{n,0}.
\end{align*}

Finally, using this with

$$
  H = g ( a^\dagger \sigma_{-} + a \sigma_{+} ) ,
$$

yields the claim. QED.

** Exercise 7.18 (Rabi oscillations)
Show that (7.77):

\begin{align*}
  U = e^{-\ii Ht} = \; & e^{-\ii\delta t} \proj{00} \\
    &+ \left(\cos(\Omega t) + \ii \frac{\delta}{\Omega} \sin(\Omega t) \right) \proj{01} \\
    &+ \left(\cos(\Omega t) - \ii \frac{\delta}{\Omega} \sin(\Omega t) \right) \proj{10} \\
    &- \ii \frac{g}{\Omega} \sin(\Omega t) \left(\ket{01}\bra{10} + \ket{10}\bra{01}\right) ,
\end{align*}

(with $\Omega=\sqrt{g^2+\delta^2}$) is correct by using

$$
  e^{\ii \vec{n}\cdot\vec{\sigma}} = \cos(\abs{\vec{n}}) + \ii \abs{\vec{n}}\inv \, \vec{n}\cdot\vec{\sigma} \cos(\abs{\vec{n}}) .
$$

to exponentiate $H$. This is an unusually simple derivation of the Rabi oscillations and
the Rabi frequency; ordinarily, one solves coupled differential equations to obtain
$\Omega$, but here we obtain the essential dynamics just by focusing on the single-atom,
single-photon subspace!

- Remarks ::
  - In the original hint the formula for $e^{\ii\vec{n}\cdot\vec{\sigma}}$ is wrong. I
    corrected this here.
  - The formula (7.76) for $H$ is wrong, it contains some sign errors and should read

    $$
    H = \begin{bmatrix} \delta & 0 & 0 \\ 0 & -\delta & g \\ 0 & g & \delta \end{bmatrix}
    $$

    (as in the book the basis states are $\ket{00}$, $\ket{01}$, $\ket{10}$ with the left
    one for the field and the right one for the atom). To see that this is correct recall
    that $H$ without $N$ is

    $$
      H = \delta Z + g ( a^\dagger \sigma_{-} + a \sigma_{+} ) .
    $$

*** Proof
Observe that $H$ is block-diagonal

\begin{bmatrix} H_1 & 0 \\ 0 & H_2 \end{bmatrix}

with $H_1=\delta$ and

$$
  H_2 = \begin{bmatrix} -\delta & g \\ g & \delta \end{bmatrix} .
$$

Hence

$$
  e^{-\ii Ht} = \begin{bmatrix} e^{-\ii\delta} & 0 \\ 0 & e^{-\ii H_2} \end{bmatrix} .
$$

which already explains the term $e^{-\ii\delta\,t}\proj{00}$ in the formula we have to
prove. Hence we can restrict our analysis to the sub-space spanned by $\ket{01}$,
$\ket{10}$ on which $H_2$ acts. Note how $H_2$ can be represented as a sum of Pauli
matrices (where we relabel the two basis vectors by $0$ and $1$ respectively to have the
standard matrix representations of these operators):

$$
  H_2 = \Omega \left( \frac{g}{\Omega} X - \frac{\delta}{\Omega} Z \right) .
$$

Here we factored out $\Omega$ to make the norm apparent. Using the hint we get

$$
  e^{-\ii H_2 t} = \cos(\Omega t) I -
  \ii \left( \frac{g}{\Omega} X - \frac{\delta}{\Omega} Z \right) \sin(\Omega t) .
$$

Plugging in $Z=\proj{01}-\proj{10}$, $X=\ket{01}\bra{10}+\ket{10}\bra{01}$, and
$I=\proj{01}+\proj{10}$ yields the claim. QED.

** Exercise 7.19 (Lorentzian absorption profile)
Plot the photon-absorbtion probability (7.79)

$$
  \chi_r = \abs{\bra{01}U\ket{10}}^2 = \frac{g^2}{g^2+\delta^2} \sin(\Omega t)^2,
$$

for $t=1$ and $g=1.2$, as a function of the detuning $\delta$, and (if you know it)
the corresponding classical result. What are the oscillations due to?

*** Proof
First of all observe that for $\delta\to\infty$ we have the following asymptotics

$$
  \chi_r \sim \frac{g^2}{\delta^2} \sin(\delta t)^2 .
$$

This is consistent with the following plot:

#+caption: Absorbtion probability $\chi_r$ against $\delta$ for $t=1$ and $g=1.2$.
[[file:images/exercise_7_19_plot.png]]

The plot is produced by the following function

#+name: exercise-7.19-1
#+begin_src sage
  def make_plot_ex719(interval=(-15, 15), t=1, g=1.2):
      """Plot for exercise 7.19."""
      d = SR.var('delta')
      Omega2 = g^2 + d^2
      Omega = sqrt(Omega2)

      chi_r = (g^2/Omega2) * sin(Omega*t)^2

      return parametric_plot((d, chi_r), (d,)+interval, aspect_ratio='automatic')
#+end_src

Note that the peak of $\chi_r$ at $0$ has value $\sin(g)\approx0.932$. For other values of
$g$ or $t$ the maximum is not necessarily at $\delta=0$.

