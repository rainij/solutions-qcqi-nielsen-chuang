#+title:  Chapter 8
#+author: Reinhard Stahn
#+setupfile: ./inc/setupfile.org
#+include: ./inc/latex-macros.org
#+property: header-args:sage :session *sage-chapter-8* :tangle chapter_8.sage
# python session just for qiskit since sage integers dont work well with qiskit:
#+property: header-args:python :session *python-chapter-7* :tangle no

#+toc: headlines 2

* Setup
** Imports
#+name: imports-chapter-8-1
#+begin_src sage
  from itertools import product
  from utils_sage import Id, X, Y, Z, delta, eps, inner, trace, kron, P0, P1
#+end_src

Let us set up some variables

#+name: global-variables
#+begin_src sage
  p, g = SR.var('p g', domain='positive')
  x, y, z, e = SR.var('x y z e', domain='real')
  u, v, w, s = SR.var('u v w s', domain='complex')

  # We want to use p as a probability, not sure if this does anything, but looks useful:
  assume(p <= 1)
  assume(g <= 1)
#+end_src

# Hidden imports
#+name: imports-chapter-7-2
#+begin_src python :exports none
  from qiskit.circuit import QuantumCircuit, QuantumRegister, Parameter as Param
#+end_src

** Utility code
#+begin_src sage
  def make_operation(Es: list[matrix]):
      """Given a list of Kraus matrix return their quantum operation."""
      def op(rho):
          return sum(E * rho * E.H for E in Es)
      return op
#+end_src

** Bloch sphere representation
*** Derivation of the formula for $M_{jk}$ and $c_k$
:PROPERTIES:
:CUSTOM_ID: chapter-8-bloch-M-c
:END:
For convenience I provide a derivation of formulas (8.91) and (8.92) for the
representation of a quantum operation on a single-qubit principal system:

\begin{align*}
  M_{jk} &= \sum_l \left[
    a_{lj} a_{lk}^* + a_{lj}^* a_{lk} +
    \left(\abs{\alpha_l}^2 - \sum_p \abs{a_{lp}}^2 \right) \delta_{jk} +
    \ii \sum_p \epsilon_{jkp} (\alpha_l a_{lp}^* - \alpha_l^* a_{lp})
  \right] , \\
  c_k &= 2 \ii \sum_l \sum_{jp} \epsilon_{jpk} a_{lj} a_{lp}^* .
\end{align*}

Here we assume that the operation elements look as follows:

$$
  E_l = \alpha_l + \sum_{k=1}^3 a_{lk} \sigma_k ,
$$

and satisfy the trace-preserving property $\sum_lE_l^\dagger\?E_l=1$ (here and in the
following I write scalars in place of multiples of the identity matrix). Let

$$
  \rho = \frac{1}{2} (1 + r_k \sigma_k) .
$$

Throughout this section (about Bloch sphere) let us apply a variant of *Einsteins
summation convention*. Whenever a free index appears twice in a /product expression/ we
sum over it. For example, the above formula for $\rho$ means

$$
  \rho = \frac{1}{2} (1 + \sum_k r_k \sigma_k) .
$$

As a special case we implicitly read $\abs{x_i}^2$ as $x_ix_i^*$ and hence imply summation
over $i$ in case of a square of an absolute value too. Let us start by considering the
trace-preserving condition

$$
  1 = E_l^\dagger E_l = (\alpha_l^* + a_{lp}^*\sigma_p) (\alpha_l + a_{lp}\sigma_q)
  = \abs{\alpha_l}^2 + \alpha_l^* a_{lp} \sigma_p + \alpha_l a_{lp}^* \sigma_p + a_{lp}^*a_{lp} \sigma_p \sigma_q .
$$

Note that $\sigma_p\sigma_q=\ii\epsilon_{pqk}\sigma_k+\delta_{pq}$. Hence

$$
  1 = \abs{\alpha_l}^2 + \abs{a_{lp}}^2 +
    2\Re(\alpha_la_{lk}^*)\sigma_k + \ii\epsilon_{pqk} a_{lp}^* a_{lq} \sigma_k .
$$

We can deduce the two constraints the coefficients $\alpha$ and $a$ have to satisfy in
order for the trace-preserving property to hold (we use here that the Pauli operators
together with the identity are a (orthonormal) basis of $\CC^{2\times2}$):

<<trace-preserving-condition-1>>
$$
  \abs{\alpha_l}^2 + \abs{a_{lp}}^2 = 1 ,
$$

and

<<trace-preserving-condition-2>>
$$
  \forall k: \; \ii\epsilon_{pqk} a_{lp}^* a_{lq} = -2\Re(\alpha_la_{lk}^*) .
$$

Now consider

$$
  2 \calE(\rho) =
  2 E_l \rho E_l^\dagger = (\alpha_l + a_{lp}\sigma_p) (1 + r_k \sigma_k) (\alpha_l^* + a_{lq}^*\sigma_q)
  =: I_1 + I_2 ,
$$

where the $I_1$ corresponds to the $1$ in the middle factor and $I_2$ to the $r_k\sigma_k$
in the middle factor. In case you wonder: the factor of $2$ at the front should make the
expression a bit simpler by avoiding numerous factors $1/2$ in the following.

\begin{align*}
  I_1 &= \abs{\alpha_l}^2 + \alpha_l a_{lp}^* \sigma_p + \alpha_l^* a_{lp} \sigma_p +
    a_{lp}a_{lq}^* \sigma_p\sigma_q \\
  &= \abs{\alpha_l}^2 + \abs{a_{lp}}^2 + 2\Re(\alpha_la_{lk}^*)\sigma_k
    + \ii\epsilon_{pqk} a_{lp} a_{lq}^* \sigma_k \\
  &= 1 + 2\ii \epsilon_{pqk} a_{lp} a_{lq}^* \sigma_k .
\end{align*}

For the last equality we used the [[trace-preserving-condition-1][first]] and [[trace-preserving-condition-2][second]] formula for the
trace-preservation. Since $I_1$ contains the part of $2\calE(\rho)=1+r_k'\sigma_k$ which
does not depend on on the $r_k$ we already proved the formula for $c_k$. The treatment of
$I_2$ is a bit more involved.

\begin{align*}
  I_2 &= \abs{\alpha_l}^2 r_k \sigma_k +
    \alpha_l a_{lp}^* r_k \sigma_k\sigma_p + \alpha_l^* a_{lp} r_k \sigma_p\sigma_k +
    a_{lp} a_{lq}^* r_k \sigma_p \sigma_k \sigma_q \\
  &= \abs{\alpha_l}^2 r_k \sigma_k +
    \alpha_l a_{lp}^* r_k (\ii\epsilon_{kpj}\sigma_j + \delta_{kp}) +
    \alpha_l^* a_{lp} r_k (-\ii\epsilon_{kpj}\sigma_j + \delta_{kp}) +
    a_{lp} a_{lq}^* r_k \sigma_p \sigma_k \sigma_q \\
  &=: I_{20} + I_{21} + I_{22} .
\end{align*}

Here $I_{22}$ stands for the last summand (the product of three Paulis), $I_{21}$ stands for
the two commands involving the $\delta_{kp}$. Finally $I_{20}$ is the rest. Note that
$I_{20}$ already corresponds to terms appearing in the formula for $M_{jk}$ and hence
needs no further treatment. We have

$$
  I_{21} = \alpha_l a_{lk}^* r_k + \alpha_l^* a_{lk} r_k
  = 2 \Re(\alpha_l a_{lk}^*) r_k .
$$

Note that this term cannot have any possible correspondence in $M$ (or $c$). Hence we
expect it to be cancelled by $I_{22}$. Therefore let us finally consider $I_{22}$. Note
that

$$
  \sigma_p \sigma_k \sigma_q = - \sigma_p \sigma_q \sigma_k + 2 \sigma_p \delta_{qk} .
$$

Hence

$$
  I_{22} = (- a_{lp} a_{lq}^* \sigma_p\sigma_q) r_k\sigma_k + 2 a_{lp} a_{lk}^* r_k \sigma_p
  =: J_1 + J_2 .
$$

Moreover

$$
  J_1 = (-\abs{a_{lp}}^2 - \ii\epsilon_{pqs}a_{lp}a_{lq}^* \sigma_s) r_k\sigma_k
  = -\abs{a_{lp}}^2 r_k\sigma_k - \ii\epsilon_{pqk}a_{lp}a_{lq}^* -
    \ii \epsilon_{pqs} a_{lp}a_{lq}^* \sigma_s (r_p \sigma_p + r_q \sigma_q)
  =: J_{10} + J_{11} + J_{12} .
$$

For the second equality I splitted the summation over $k$ into $k=s$ and $k\neq\?s$ (this
leads to a tiny clash with out summation convention but shouldn't be a major hurdle for
understanding the calculations). Note that $J_{10}$ corresponds to a term in $M$, so with
this one we are done. By [[trace-preserving-condition-2][trace-preservation]] we have that

$$
  J_{11} + I_{21} = 0 ,
$$

which is the awaited cancellation for $I_{21}$. So these terms are also done. Recall that
the only still open terms are $J_{12}$ and $J_2$. We will go on with the former.

$$
  J_{12} = -\ii a_{lp} a_{lq}^* \left( \epsilon_{pqs}
    \left[ r_p \ii \epsilon_{spq}\sigma_q + r_q \ii \epsilon_{sqp}\sigma_p \right] \right)
  = a_{lp} a_{lq}^* (r_p\sigma_q - r_q \sigma_p) .
$$

This can be nicely combined with $J_2$:

$$
  J_{12} + J_2 = a_{lp} a_{lq}^* r_p \sigma_q + a_{lp} a_{lq}^* r_q \sigma_p
  = a_{lk} a_{lj}^* r_k \sigma_j + a_{lj} a_{lk}^* r_k \sigma_j ,
$$

which yields the remaining terms in the formula for $M$. QED.

*** Sage code
The following function implements $M$ and $c$ from the previous section in sage.

#+name: single-qubit-operation-elements-bloch
#+begin_src sage
  def affine(*Es):
      """Maps a list of single-qubit operation elements to their Bloch-representation

      Args:
          Es: A list of single qubit operation elements, trace preserving.

      Returns:
          M, c: as in formulas (8.91) and (8.92)
      """
      N = len(Es)

      assert N != 0, "Need at least one operation element"
      assert all([len(E) == 4 for E in Es]), "Need 4 coefficients for each Ei"

      alpha = [E[0] for E in Es]
      bs = [E[1:4] for E in Es]

      M = [[sum(
            bs[l][j] * bs[l][k].conjugate() + bs[l][j].conjugate() * bs[l][k]
            + delta(j, k) * alpha[l] * alpha[l].conjugate()
            - delta(j, k) * sum(bs[l][p] * bs[l][p].conjugate() for p in range(3))
            + I * sum(eps(j, k, p)*(alpha[l]*bs[l][p].conjugate() - alpha[l].conjugate()*bs[l][p])
                      for p in range(3))
          for l in range(N)) for k in range(3)] for j in range(3)]

      c = [sum(sum(sum(
               2*i * eps(j, p, k) * bs[l][j] * bs[l][p].conjugate()
             for p in range(3)) for j in range(3)) for l in range(N))
           for k in range(3)]

      return simplify(matrix(M)), simplify(vector(c))


  def toPauli(M):
      """Takes a 2x2 matrix and returns its coefficients in the Pauli-Basis (I,X,Y,Z)."""
      assert M.dimensions() == (2, 2), "Expected a two dimensional matrix"
      return [inner(p/2, M) for p in [Id, X, Y, Z]]


  def affine2(*Ps):
      """Like affine but applies toPauli before that to accept the operator elements
      directly as matrices."""
      return affine(*[toPauli(p) for p in Ps])
#+end_src

Now we are prepared to consider the examples from the book. First let us consider the
*bit-flip*:

#+name: sage-bit-flip
#+begin_src sage :tangle no :results replace
  E0 = sqrt(p) * Id
  E1 = sqrt(1-p) * X

  M, c = affine(*[toPauli(E) for E in [E0, E1]])

  assert M == matrix.diagonal([1, 2*p-1, 2*p-1])
  assert c == vector([0, 0, 0])
  "PASSED"
#+end_src

#+RESULTS: sage-bit-flip
: 'PASSED'

Next consider the *amplitude damping*:

#+begin_src sage :tangle no :results replace
  E0 = matrix.diagonal([1, sqrt(1-g)])
  E1 = matrix([[0, sqrt(g)], [0, 0]])

  M, c = affine(*[toPauli(E) for E in [E0, E1]])
  M = M.expand()

  assert M == matrix.diagonal([sqrt(1-g), sqrt(1-g), 1-g])
  assert c == vector([0, 0, g])
  "PASSED"
#+end_src

#+RESULTS:
: 'PASSED'

For some exercises I think it is convenient to be able to perform (possibly
non-trace-preserving) single qubit operations. For this the Bloch sphere is not sufficient
anymore, but we still have a representation in $\RR^4$ (see my solution of [[#exercise-8.16][exercise
8.16]]). More precisely on the real subspace of $\CC^{2\times2}$ generated by the
Pauli-operators $(I,X,Y,Z)$.

#+name: single-qubit-operation
#+begin_src sage
  # A generalized density matrix and an arbitrary operator E:
  rho_gen = (e + x*X + y*Y + z*Z)
  E_gen = s*Id + u*X + v*Y + w*Z

  def qop1d(Es, rho):
      """Take a list of operator elements and apply the quantum operation to the given mixed
      single-qubit state. It returns the result as a vector in RR^4 wrt the Pauli basis."""
      rho1 = make_operation(Es)(rho)
      return simplify(vector(toPauli(rho1)))
#+end_src

The corresponding matrix is given by

#+name: make_qop1d_matrix_4d
#+begin_src sage
  def make_qop1d_matrix_4d(Es):
      """Return the matrix representation on RR^4 of the operation given by the
      elements. Works for non-trace-preserving operations too."""
      bs = matrix.identity(4)
      return simplify(matrix([[b*qop1d(Es, rho=A) for A in [Id, X, Y, Z]] for b in bs]))
#+end_src

** Process tomography for single qubit
I found it highly non-trivial to see equation (8.179)

<<chi-for-single-qubit>>
$$
  \chi = \Lambda \begin{bmatrix} \rho'_0 & \rho'_1 \\ \rho'_2 & \rho'_3 \end{bmatrix} \Lambda
$$

in Box 8.5 on process tomography for a single qubit. For later reference let us call the
matrix in the middle $R'$ (Note that I shifted the indices my one - this makes matters
slightly simpler). Therefore I try to prove it here in a way which is hopefully systematic
enough to gain some insight. Recall that

$$
  \Lambda = \frac{1}{2} (Z\otimes I + X \otimes X) .
$$

I take this as a motivation to define $L_0=Z$, $L_1=X$, $R_0=I$, $R_1=X$. This implies
$\Lambda=\sum_aL_a\otimes\?R_a$, and in particular

$$
  4 \Lambda (\rho\otimes \rho') \Lambda = \sum_{ab} L_a \rho L_b \otimes R_a \rho' R_b .
$$

Let $P_{kl}=\ket{k}\bra{l}$. Recall that the $\rho'_i$ are given by
$\rho'_i=\calE(\rho_i)$ and $\rho_i=P_{kl}$ where $kl$ is the binary representation of
$i$. Observe that $P_{kl}=R_kP_{00}R_l$ (we will use it later). A crucial thing in box 8.5
is the definition of the basis for the operation elements:

$$
  N_0=I, \; N_1=X, \; N_2=\tilde{Y}=-\ii Y, \; N_3=Z .
$$

One important property of this basis is the following:

$$
  X N_i X = \begin{cases} N_i & \text{for } i=0,1 \\ -N_i & \text{for } i=2,3 \end{cases}
$$

But of course this does not explain the peculiar form of $N_2$ ($Y$ would do the
same). Another "nice" property is $XN_0=N_1$, and $XN_2=N_3$. Also note that $N_2=XZ$
which says that $N_2$ is a bit- and a phase-flip in one. This turns out to be important
further below. Before we start proving the claim we note that

$$
  R' = \sum_i \rho_i \otimes \rho_i' = \sum_{klmn} \chi_{mn} P_{kl} \otimes N_m P_{kl} N_n .
$$

The $\chi_{mn}$ are unknown and our goal is to prove the [[chi-for-single-qubit][above formula]] for them. Now
consider

\begin{align*}
  \Lambda R' \Lambda &= \frac{1}{4} \sum_{klmn} \chi_{mn} \Lambda (P_{kl} \otimes N_m P_{kl} N_n) \Lambda \\
  &= \frac{1}{4} \sum_{klmnab} \chi_{mn} \, L_a P_{kl} L_b \otimes R_a N_m P_{kl} N_n R_b \\
  &= \frac{1}{4} \sum_{klmnab} \chi_{mn} \, L_aR_a P_{kl} R_bL_b \otimes R_a N_m R_a P_{kl} R_b N_n R_b .
\end{align*}

For the last equality we used the fact that

$$
  \{P_{kl}|kl\} = \{R_aP_{kl}R_b|kl\}
$$

for all $a,b$. In the following let us define the negation of a bit-value by $\bar{a}$ and
the most significant bit of $m$ as

$$
  \tilde{m} = \begin{cases} 0 & \text{if } m = 0,1 \\ 1 & \text{if } m = 2, 3 \end{cases} .
$$

In the same way $\tilde{n}$ is defined to be the most significant bit of $n$. These
definitions are handy because they help us further simplify $\Lambda\?R'\Lambda$. Before
doing so observe

$$
  R_a N_n R_a = (-1)^{a\tilde{n}} N_n
$$

and

$$
  L_a R_a P_{kl} R_b L_b = (-1)^{\bar{a}k+\bar{b}l} P_{kl} .
$$

Hence

\begin{align*}
  \Lambda R' \Lambda &= \frac{1}{4} \sum_{klmnab} \chi_{mn} (-1)^{\bar{a}k+a\tilde{m}+\bar{b}l+b\tilde{n}}
  P_{kl} \otimes N_m P_{kl} N_n \\
  &= \sum_{mn} (-1)^{\tilde{m}+\tilde{n}} \chi_{mn} P_{\tilde{m}\tilde{n}} \otimes N_m P_{\tilde{m}\tilde{n}} N_n .
\end{align*}

The second equality can be seen from the fact that the term $P_{kl}\otimes\?N_mP_{kl}N_n$
does not depend on $a$ or $b$. Hence the summation over $a,b$ does only yield a non-zero
term if $k=\tilde{m}$ and $l=\tilde{n}$. Let $m=m_1m_0$ and $n=n_1n_0$ be the binary
representation of $m$ and $n$. By what we have shown:

$$
  \Lambda R' \Lambda = \sum_{mn} (-1)^{m_1+n_1} \chi_{mn} P_{m_1n_1} \otimes N_m P_{m_1n_1} N_n .
$$

Observe that

$$
  N_m \ket{m_1} = (-1)^{m_1} \ket{\overline{m_1}^{\bar{m_0}}} ,
$$

where $\bar{a}^0:=a$ and $\bar{a}^1=\bar{a}$. Hence [[chi-for-single-qubit][the claim]] follows. QED.

* Exercises
** Exercise 8.1 (Unitary evolution as a quantum operation)
:PROPERTIES:
:CUSTOM_ID: exercise-8.1
:END:
Pure states evolve under unitary transforms as $\ket{\psi}\mapsto\,U\ket{\psi}$. Show
that, equivalently, we may write $\rho\mapsto\calE(\rho)\equiv\,U\rho\,U^\dagger$,
for $\rho=\proj{\psi}$.

*** Proof
**** Preparation
This is a really basic question and it should actually be obvious /if/ you really
understand the bra-ket notation (beware however that it might /appear/ to be obvious if
you merely feel /familiar/ with it).

To give a meaningful solution beyond "it is obvious" we take a step back and recall what
the [[https://en.wikipedia.org/wiki/Bra%E2%80%93ket_notation][bra-ket]] notation actually means. In quantum mechanics we always work on (complex)
Hilbert spaces. A Hilbert space is a (complex) vector space $\calH$ equipped with a
[[https://en.wikipedia.org/wiki/Sesquilinear_form][sesquilinear form]], which has to obey certain axioms to be mentioned soon. We denote it by

$$
  (\ket{\psi}, \ket{\varphi}) .
$$

A sesquilinear form is a mapping $\calH\times\calH\to\CC$ which is linear in the second
argument and conjugate-linear in the first argument. For a Hilbert space we require in
addition that (*positivity*)

$$
  \forall \ket{\psi} \in \calH\backslash\{0\}: \; (\ket{\psi}, \ket{\psi}) > 0 ,
$$

and that (*symmetry*)

$$
  \forall \ket{\psi}, \ket{\varphi} \in \calH : \; (\ket{\psi},\ket{\varphi})
  = \overline{(\ket{\varphi},\ket{\psi})} .
$$

(The bar means /complex conjugation/.) Such a sesquilinear form is called a *scalar
product*. This is what makes a complex /[[https://en.wikipedia.org/wiki/Hilbert_space][Hilbert space]]/, a complex vector space equipped
with a scalar product. There are also real Hilbert spaces but we do not need them
here. Note that we are always talking about /finite/ dimensional vector spaces. In case of
infinite dimensional vector spaces a *[[https://en.wikipedia.org/wiki/Complete_metric_space][completeness]]* axiom is required in addition. (In the
finite dimensional case this axiom follows automatically from the [[https://en.wikipedia.org/wiki/Completeness_of_the_real_numbers][completeness]] axiom of
real (and hence complex) numbers).

In the framework of Dirac's bra-ket notation the kets are just the elements of the Hilbert
space. We already used this in the above introduction. The bras are elements of the /dual
space/ $\calH^*$ which is just the space of linear maps $\calH\to\CC$. For any
$\ket{\psi}\in\calH$ an example of such a map is given by

$$
  \ket{\varphi} \mapsto (\ket{\psi}, \ket{\varphi}) .
$$

<<exercise-8.1-canonical-iso>> In fact, one can prove that any element of $\calH^*$ (that
is, any linear map $\calH\to\CC$) can be written like this for a /unique/
$\ket{\psi}$. This is called the *canonical isomorphism* between $\calH$ and $\calH^*$ (it
is called /canonical/ since we could define it without knowing anything concrete about our
Hilbert space, in other words it is definable in the most abstract terms). Note that the
canonical isomorphism $\iota$ is /conjugate/ linear, that is,
$\iota(\lambda\ket{\psi})=\bra{\psi}\lambda^*$.

This canonical isomorphism actually motivates the bras. A bra $\bra{\psi}\in\calH^*$ is
the linear map given by

$$
  \bra{\psi} : \; \ket{\varphi} \mapsto \sprod{\psi}{\varphi} := (\ket{\psi}, \ket{\varphi}) .
$$

Having said that, terms like

$$
  \bra{\psi} U \ket{\varphi}
$$

are the same as

$$
  \bra{\psi} U \ket{\varphi} = (\bra{\psi}\circ U) \ket{\varphi} = (\ket{\psi}, U\ket{\varphi}) ,
$$

where $\circ$ denotes the composition of functions ($U$ and $\bra{\psi}$ as linear maps
are both functions of course). And of course, the bra-ket notation also makes the above
introduced notation for the scalar product obsolete since basically the bra and the ket
are the left and right half of the scalar product.

**** The actual proof
Let us return to the actual exercise. We use the above notation $(\ldots,\ldots)$ for the
scalar product to make the derivation clearer. Let $U$ be a unitary operator on $\calH$
and let

$$
  \rho = \rho_{\ket{\psi}} = \proj{\psi} .
$$

The task is to calculate $\rho_{U\ket{\psi}}$ and to show that it is equal to
$\calE(\rho_{\ket{\psi}})$. Note that $\rho$ is a linear map $\calH\to\calH$ given by

$$
  \rho \ket{\varphi} = \ket{\psi} \sprod{\psi}{\varphi} = (\ket{\psi}, \ket{\varphi}) \ket{\varphi} .
$$

Hence

\begin{align*}
  \rho_{U\ket{\psi}} \ket{\varphi} &= (U\ket{\psi}, \ket{\varphi}) U\ket{\psi} \\
  &= (\ket{\psi}, U^\dagger\ket{\varphi}) U\ket{\psi} \\
  &= \bra{\psi} U^\dagger \ket{\varphi} U\ket{\psi} \\
  &= U\ket{\psi} \bra{\psi} U^\dagger \ket{\varphi} \\
  &= U\rho_{\ket{\psi}} U^\dagger \ket{\varphi} .
\end{align*}

The first equality follows from the definition of $\rho_{\ket{\psi}}$. The second one
follows from the definition of the (hermitian) [[https://en.wikipedia.org/wiki/Hermitian_adjoint][adjoint]] operator. The third one is the
definition of the bras. The fourth equality just moves the complex number
$\bra{\psi}U\ket{\varphi}$ to the right. The last equation is again the definition of
$\rho_{\ket{\psi}}$.

Since the above eqation holds for all $\varphi$ we have

$$
  \rho_{U\ket{\psi}} = U\rho_{\ket{\psi}} U^\dagger .
$$

QED.

- Remark :: There is another slightly deeper way to see this result. Let us denote the
  above mentioned [[exercise-8.1-canonical-iso][canonical isomorphism]] by

  $$
  \iota(\ket{\psi}) = \bra{\psi} .
  $$

  It is not hard to show and extension of the conjugate-linearity property of the
  canonical isomorphism:

  $$
  \iota(U\ket{\psi}) = \iota(\ket{\psi}) U^\dagger = \bra{\psi} U^\dagger .
  $$

  In fact, the proof is even shorter than the proof of the claim of the exercise. That is,
  the /canonical/ way to associate a linear mapping $\calH^*\to\calH^*$ with $U$ is the
  right composition with $U^\dagger$. From this the claim of the exercise follows too.

** Exercise 8.2 (Measurement as a quantum operation)
Recall from Section 2.2.3 (on page 84) that a quantum measurement with outcomes labeled by
$m$ is described by a set of measurement operators $M_m$ such that
$\sum_mM_m^\dagger\,M_m=I$. Let the state of the system immediately before the measurement
be $\rho$. Show that for $\calE_m(\rho)=M_m\rho\,M_m^\dagger$, the state of the system
immediately after the measurement is

$$
  \frac{\calE_m(\rho)}{\trace{\calE_m(\rho)}} .
$$

Also show that the probability of obtaining this measurement result is
$p(m)=\trace{\calE_m(\rho)}$.

*** Solution
Clearly the exercise assumes that we only know the postulates of quantum mechanics for
/pure/ states as detailed in section 2.2.3 and not the generalization to mixed states as
detailed in 2.4.1. Otherwise the exercise would be a trivial restatement of the evolution
and measurement postulates.

Let us first assume that $\rho=\proj{\psi}$ is a pure state (just expressed as a density
matrix). In order to compute the trace of an operator $\sigma$ we can take /any/
orthonormal basis $\{e\}$ and have $\trace{\sigma}=\sum_{e}\bra{e}\sigma\ket{e}$. Let
$\sigma=\proj{\nu}$ with a not necessarily normalized non-zero $\ket{\nu}$. Without loss
of generality we may assume that $\ket{\nu}/\norm{\nu}$ is part of our chosen ONB (this
follows from the [[https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process][Gram-Schmidt]] procedure). Hence

<<exercise-8.2-1>>
$$
  \trace{\sigma} = \sum_{e} \bra{e}\proj{\nu}\ket{e}
  = \sprod{\nu}{\nu} \sprod{\nu}{\nu} / \norm{\nu}^2
  = \norm{\nu}^2 .
$$

Given a state $\ket{\psi}$ the post measurement state after measuring $m$ is
$M_m\ket{\psi}/\norm{M_m\ket{\psi}}$. From the reasoning in [[#exercise-8.1][exercise 1]] (that $U$ was a
unitary operator didn't really matter there) we deduce that the corresponding density
matrix is

$$
  \frac{M_m \rho M_m^\dagger}{\norm{M_m\ket{\psi}}^2} .
$$

Using [[exercise-8.2-1][the equation]] involving $\sigma$ (with $\ket{\nu}=M_m\ket{\psi}$) we see that
$\norm{M_m\ket{\psi}}^2=\trace{M_m\rho\,M_m^\dagger}$. Hence the post measurement state is

$$
  \frac{M_m \rho M_m^\dagger}{\trace{M_m\rho M_m^\dagger}} .
$$

The probability to measure $m$ (and to obtain this state) is

$$
  p(m) = \norm{M_m\ket{\psi}}^2 = \trace{M_m \rho M_m^\dagger} .
$$

This concludes the exercise for pure states. Let $\rho$ be a mixed state now. This means
that there is a decomposition

$$
  \rho = \sum_j p_j \proj{\psi_j}
$$

with strictly positive coefficients $p_j$ such that $\sum_jp_j=1$. The ensemble
interpretation says that having such a mixed state means that, upon measurement, one of
the pure states $\ket{\psi_j}$ is taken at random with probability $p_j$ and then the
measurement proceeds as in the pure state case. It is not a-priori clear that this is well
defined since the decomposition of $\rho$ into pure states is not unique (c.f. Theorem 2.6
on page 103). But let us ignore this for the moment. We will see that the obtained
probabilities and the post measurement states are independent of the concrete
decomposition, hence the problem solves itself.

According to the ensemble interpretation the probability to measure $m$ is

$$
  p(m) = \sum_j p(m|j) \, p_j = \sum_j \trace{M_m\proj{\psi_j} M_m^\dagger} \, p_j
  = \trace{M_m \rho M_m^\dagger} .
$$

Here

$$
  p(m|j) = \norm{M_m \ket{\psi_j}}^2 = \trace{M_m \proj{\psi_j} M_m^\dagger}
$$

is the conditioned probability to measure $m$ if we already know that the pure state
$\psi_j$ was chosen. Suppose we know that we measured $m$. What is the (mixed) state in
that case? We already know that with (conditioned) probability $p(j|m)$ the post
measurement state is

$$
  \frac{M_m\proj{\psi_j} M_m^\dagger}{p(m|j)} .
$$

But this is an ensemble of pure states! Hence, using $p(m|j)p_j=p(m\cap\,j)=p(j|m)p(m)$,
the corresponding mixed state is

$$
  \sum_j p(j|m) \frac{M_m\proj{\psi_j} M_m^\dagger}{p(m|j)}
  = \frac{1}{p(m)} \sum_j p_j M_m\proj{\psi_j} M_m^\dagger
  = \frac{M_m\rho M_m^\dagger}{p(m)}
  = \frac{M_m\rho M_m^\dagger}{\trace{M_m\rho M_m^\dagger}} .
$$

This concludes the exercise. But let is briefly mention that in case we do /not/ obtain
the outcome of the measurement the post measurement state is

<<exercise-8.2-measurment-operation>>
$$
  \sum_{m,j} p(m|j) p_j \, \frac{M_m \proj{\psi_j} M_m^\dagger}{p(m|j)}
  = \sum_m M_m \rho M_m^\dagger .
$$

- Remark :: Together with the conclusions from the actual exercise we can draw the
  following additional conclusion: If now we somehow obtain the measurement result than
  [[exercise-8.2-measurment-operation][this sum]] "collapses" into one of its summands (just normalized), with probability
  $\trace{M_m\rho\,M_m^\dagger}$ for each of them. That is, the measurement can be
  decomposed into two parts: First the state changes to

  $$
    \calE(\rho) = \sum_m \calE_m(\rho)
  $$

  and then (if we obtain the measurement result) the state "collapses" into
  $\calE_m(\rho)/\trace{\calE_m(\rho)}$ for one of the $m$ with probability
  $\trace{\calE_m(\rho)}$. So, the first part of the measurement can be expressed in terms
  of quantum operations. But not the second part (since it is not deterministic and cannot
  give rise to a mathematical function on density matrices).

** Exercise 8.3
Our derivation of the operator-sum representation implicitly assumed that the input and
output spaces for the operation were the same. Suppose a composite system AB initially in
an unknown quantum state $\rho$ is brought into contact with a composite system CD initially in
some standard state $\ket{0}$, and the two systems interact according to a unitary interaction
$U$. After the interaction we discard systems A and D, leaving a state $\rho'$ of system
BC. Show that the map $\calE(\rho)=\rho'$ satisfies

$$
  \calE(\rho) = \sum_k E_k \rho E_k^\dagger ,
$$

for some set of linear operators $E_k$ from the state space of system AB to the state
space of system BC, and such that $\sum_kE_k^\dagger\,E_k$.

*** Proof
Basically we do the same as the book for the special case of equal input and output
space. According to the exercise we have

$$
  \calE(\rho) = \ptrace{AD}{U \rho \otimes \proj{0} U^\dagger} .
$$

Let $\{e_k\}$ be an orthonormal basis of AD. Hence

$$
  \calE(\rho) = \sum_k \bra{e_k} U \rho \otimes \proj{0} U^\dagger \ket{e_k} .
$$

Thus, defining $E_k=\bra{e_k}U\ket{0}$, we have

$$
  \calE(\rho) = \sum_k E_k \rho E_k^\dagger .
$$

That is, $\calE$ has the required form. We only have to show the completeness property
$J:=\sum_kE_k^\dagger\,E_k=I_{AB}$. Note that $J=J^\dagger$ is self-adjoint and
positive-definite. Let $\rho$ be arbitrary and consider

$$
  1 = \ptrace{BC}{\calE(\rho)} = \sum_k \ptrace{BC}{E_k\rho E_k^\dagger}
  = \sum_k \ptrace{AB}{E_k^\dagger E_k \rho}
  = \ptrace{AB}{J^\dagger \rho} .
$$

Here we used the well-known /cyclicity property/ of the trace (which also holds for
(matching) rectangular matrices). Thus

<<exercise-8.3-1>>
$$
  \forall \rho : \; \trace{(J-I)^\dagger \rho} = 0 .
$$

Here the quantification runs over all density operators (hermitian, positive definite,
trace 1).

- Remark :: In case you wonder why I take the adjoint of $J$. We have that
  $S,T\mapsto\trace{S^\dagger\,T}$ turns the matrix space $\CC^{n\times\?n}$ into a
  complex Hilbert space. See e.g. the definition of a Hilbert-Schmidt operator on
  [[https://en.wikipedia.org/wiki/Hilbert%E2%80%93Schmidt_operator][wikipedia]] (it is about infinite dimensional spaces but just note that in finite
  dimensions every operator is a Hilbert-Schmidt operator). For this exercise it does not
  directly help us since the quantification [[exercise-8.3-1][above]] is /not/ over all
  $\rho\in\CC^{n\times\?n}$. But still nice to know 😉.

Considering all rank-1 density matrices $\rho=\proj{\psi}$ we deduce

$$
  \forall \ket{\psi} : \; \trace{(J-I)^\dagger \proj{\psi}} = \bra{\psi} (J-I)^\dagger \ket{\psi} = 0 .
$$

(The first equality can either be seen by the cyclicity property of the trace or by using
an ONB which involves $\psi$ (but normalized) as one of its elements) But this can only be
true (for all normalized $\ket{\psi}$) if $J=I$. QED.

** Exercise 8.4 (Measurement)
:PROPERTIES:
:CUSTOM_ID: exercise-8.4
:END:
Suppose we have a single qubit principal system, interacting with a single qubit
environment through the transform

$$
  U = P_0 \otimes I + P_1 \otimes X
$$

where $X$ is the usual Pauli matrix (acting on the environment), and $P_0\equiv\proj{0}$,
$P_1\equiv\proj{1}$ are projectors (acting on the system). Give the quantum operation for
this process, in the operator-sum representation, assuming the environment starts in the
state $\ket{0}$.

*** Solution
Note that $U$ is nothing else than the =CNOT= gate. Let us label the environment by E. One
way to define the operation elements is

\begin{align*}
  E_0 &= \bra{0_E} U \ket{0_E} = P_0 \sprod{0_E}{0_E} + P_1 \bra{0_E}X\ket{0_E} = P_0 , \\
  E_1 &= \bra{1_E} U \ket{0_E} = P_0 \sprod{1_E}{0_E} + P_1 \bra{1_E}X\ket{0_E} = P_1 .
\end{align*}

(The $\ket{0_E}$ to the right of $U$ comes from the initial state of the environment.)
Hence

$$
  \calE(\rho) = P_0 \rho P_0 + P_1 \rho P_1 = \begin{bmatrix} \rho_{00} & 0 \\ 0 & \rho_{11} \end{bmatrix} .
$$

That is, this operation acts like the first part of a measurement. It describes the state
after a measurement in the Z-basis if nobody tells us the measurement outcome. Once we get
the outcome the state "collapses" to one of the two summands (properly normalized of
course). The respective probabilities are $\rho_{00}$ (for outcome $0$) and $\rho_{11}$
(for outcome $1$).

** Exercise 8.5 (Spin flips)
Just as in the previous exercise, but now let

$$
  U = \frac{1}{\sqrt{2}} \left( X \otimes I + Y \otimes X \right) .
$$

Give the quantum operation for this process, in the operator-sum representation.

*** Solution
First of all let us make sure that $U$ really is unitary. We have $U^\dagger=U$, hence

$$
  U^\dagger U = U^2 = \frac{1}{2} \left( X^2 \otimes I + Y^2 \otimes X^2 + (XY+YX)\otimes X \right) = I .
$$

Hence $U$ is indeed unitary. One way to define the operation elements is as follows:

\begin{align*}
  E_0 &= \bra{0_E} U \ket{0_E} = X \sprod{0_E}{0_E} + Y \bra{0_E}X\ket{0_E} = X , \\
  E_1 &= \bra{1_E} U \ket{0_E} = X \sprod{1_E}{0_E} + Y \bra{1_E}X\ket{0_E} = Y .
\end{align*}

Hence

$$
  \calE(\rho) = \frac{1}{2} \left(X \rho X + Y \rho Y \right)
  = \begin{bmatrix} \rho_{11} & 0 \\ 0 & \rho_{00} \end{bmatrix} .
$$

Interpreting this in the same way as in [[#exercise-8.4][exercise 8.4]] we deduce that $\calE$ corresponds to
a spin-flip followed by a measurement (without obtaining the measurement outcome).

** Exercise 8.6 (Composition of quantum operations)
Suppose $\calE$ and $\calF$ are quantum operations on the same quantum system. Show that
the composition $\calF\circ\calE$ is a quantum operation, in the sense that it has an
operator-sum representation. State and prove an extension of this result to the case where
$\calE$ and $\calF$ do not necessarily have the same input and output spaces.

*** Proof
Suppose

\begin{align*}
  \calE(\rho) &= \sum_i E_i \rho E_i^\dagger , \\
  \calF(\rho) &= \sum_j F_j \rho F_j^\dagger .
\end{align*}

Then

\begin{align*}
  \calF \circ \calE(\rho) = \sum_{ij} F_j E_i \rho E_i^\dagger F_j^\dagger .
\end{align*}

This suggests to use all the $F_jE_i$ as operation elements of the composition. That
$\calF\circ\calE$ is indeed a quantum operation now follows from

$$
  \sum_{ij} E_i^\dagger F_j^\dagger F_j E_i = \sum_i E_i^\dagger E_i = I .
$$

The proof directly generalizes (without any change) to the case where $\calE:A\to\?B$ and
$\calF:B\to\?C$. QED.

** Exercise 8.7
Suppose that instead of doing a projective measurement on the combined principal system
and environment we had performed a general measurement described by measurement operators
$\{M_m\}$. Find operator-sum representations for the corresponding quantum operations
$\{\calE_m\}$ on the principal system, and show that the respective measurement
probabilities are $\trace{\calE_m(\rho)}$.

*** Solution
Let $\sigma=\sum_jq_j\proj{j}$ be the state of the environment E. The generalized
operations look as follows

$$
  \calE_m(\rho) = \ptrace{E}{M_m U \rho\otimes\sigma U^\dagger M_m^\dagger} .
$$

According to the posutulates the probability to measure $m$, after an evolution with $U$,
is

$$
  p(m) = \trace{M_m U \rho\otimes\sigma U^\dagger M_m^\dagger} = \trace{\calE_m(\rho)} .
$$

Here we use the property $\trace{\ptrace{E}{A}}=\trace{A}$ of the partial trace. The
post-measurement state for the whole system is is

$$
  \frac{M_m U \rho\otimes\sigma U^\dagger M_m^\dagger}{p(m)} ,
$$

and for the principal system it is

$$
  \ptrace{E}{\frac{M_m U \rho\otimes\sigma U^\dagger M_m^\dagger}{p(m)}}
  = \frac{\calE_m(\rho)}{\trace{\calE_m(\rho)}} .
$$

It is not hard to see that for any choice of an ONB $\{e_k\}$ on E the operators

$$
  E_{jk} = \sqrt{q_j} \bra{e_k} M_m U \ket{j}
$$

are elements for the operation $\calE_m$, which also satisfy
$\sum\?E_{jk}^\dagger\?E_{jk}\leq\?I$:

$$
  \sum_{jk} E_{jk}^\dagger E_{jk}
  = \sum_{jk} q_j \bra{j}U^\dagger M_m^\dagger \ket{e_k} \bra{e_k} M_m U \ket{j}
  = \sum_j q_j \bra{j}U^\dagger M_m^\dagger M_m U \ket{j}
  \leq \sum_j q_j \bra{j}U^\dagger U \ket{j}
  = \sum_j q_j = I .
$$

** Exercise 8.8 (Non-trace-preserving quantum operations)
Explain how to construct a unitary operator for a system–environment model of a
non-trace-preserving quantum operation, by introducing an extra operator, $E_\infty$, into the
set of operation elements $E_k$, chosen so that when summing over the complete set of $k$,
including $k=\infty$, one obtains $\sum_kE_k^\dagger\?E_k=I$.

*** Solution
Let us define

$$
  E_\infty = \sqrt{I - \sum_k E_k^\dagger E_k} .
$$

Clearly $E_\infty^\dagger=E_\infty$ and

$$
  \sum_k' E_k^\dagger E_k = I .
$$

Here and in the following we let $\sum_k'$ denote the sum over all $k$ including
$k=\infty$. On the other hand $\sum_k$ excludes $k=\infty$.

As in the book consider an environment described by a Hilbert space which has a basis
$\{\ket{e_k}\}$ (including $k=\infty$). Define $U$ by

$$
  U \ket{\psi} \ket{e_0} \equiv \sum_k' E_k \ket{\psi} \ket{e_k}
$$

and unitary extension to the full system - exactly as in the book. Consider the following
projection defined on E:

$$
  P = I_E - \proj{e_\infty} .
$$

We shall prove that

$$
  \calE(\rho) = \ptrace{E}{PU \rho\otimes \proj{e_0} U^\dagger P} .
$$

In fact, the RHS equals

$$
  \ptrace{E}{\sum_{kl}' E_k \rho E_l^\dagger \otimes P \ket{e_k}\bra{e_l} P}
  = \ptrace{E}{\sum_{kl} E_k \rho E_l^\dagger \otimes \ket{e_k}\bra{e_l} }
  = \sum_{k} E_k \rho E_k^\dagger
  = \calE(\rho)
$$

as desired.

** Exercise 8.9 (Measurement model)
If we are given a set of quantum operations $\{\calE_m\}$ such that $\sum_m\calE_m$ is
trace-preserving, then it is possible to construct a measurement model giving rise to this
set of quantum operations. For each $m$, let $E_{mk}$ be a set of operation elements for
$\calE_m$. Introduce an environmental system, E, with an orthonormal basis $\ket{m,k}$ in
one-to-one correspondence with the set of indices for the operation elements. Analogously
to the earlier construction, define an operator $U$ such that

$$
  U \ket{\psi} \ket{e_0} = \sum_{mk} E_{mk} \ket{\psi} \ket{m,k} .
$$

Next, deﬁne projectors $P_m=\sum_k\proj{m,k}$ on the environmental system, E.  Show that
performing $U$ on $\rho\otimes\proj{e_0}$, then measuring $P_m$ gives $m$ with probability
$\trace{\calE_m(\rho)}$, and the corresponding post-measurement state of the principal
system is $\calE_m(\rho)/\trace{\calE_m(\rho)}$.

*** Solution
That $U$ can be extended to a unitary operator can be seen as in the other cases of
constructing an environment model. We have

\begin{align*}
  \ptrace{E}{P_m U \rho \otimes \proj{e_0} U^\dagger P_m}
  &= \ptrace{E}{\sum_{m,k,l} E_{mk}\rho E_{ml}^\dagger \otimes \ket{m,k}\bra{m,l}} \\
  &= \sum_{m,k} E_{mk}\rho E_{mk}^\dagger \\
  &= \calE_m(\rho) .
\end{align*}

Hence, using $\trace{\ptrace{E}{\ldots}}=\trace{\ldots}$, the probability to measure $m$
after applying $U$ to $\rho\otimes\proj{e_0}$ is

$$
  p(m) = \trace{P_m U \rho \otimes \proj{e_0} U^\dagger P_m} = \trace{\calE_m(\rho)} .
$$

The post-measurement state of the whole system is

$$
  \frac{P_m U \rho \otimes \proj{e_0} U^\dagger P_m}{p(m)} .
$$

Taking the partial trace of the environment yields the post-measurement state of the
principal system

$$
  \frac{\ptrace{E}{P_m U \rho \otimes \proj{e_0} U^\dagger P_m}}{p(m)}
  = \frac{\calE_m(\rho)}{\trace{\calE_m(\rho)}} .
$$

** Exercise 8.10
:PROPERTIES:
:CUSTOM_ID: exercise-8.10
:END:
Give a proof of Theorem 8.3 based on the freedom in the operator-sum representation, as
follows. Let $\{E_j\}$ be a set of operation elements for $\calE$. Define a matrix
$W_{jk}=\trace{E_j^\dagger\?E_k}$. Show that the matrix $W$ is Hermitian and of rank at
most $d^2$, and thus there is unitary matrix $u$ such that $uWu^\dagger$ is diagonal with
at most $d^2$ non-zero entries. Use $u$ to deﬁne a new set of at most $d^2$ non-zero
operation elements $\{F_j\}$ for $\calE$.

*** Proof 1
The statement is a direct consequence of the proof of Kraus's Theorem as given in the book
(Theorem 8.1). Recall that the diagonalization of $\sigma$

$$
  \sigma = \sum_{k=1}^M \proj{s_k}
$$

is used to define the operation elements by

$$
  E_k \ket{\psi} = \braket{\tilde{\psi}}{s_k}
$$

That is, there are precisely $M$ elements to be defined. How large can $M$ be? Recall that
$\sigma$ is a density matrix on the Hilbert space $\calH_R\otimes\calH_Q$ of the joint
system $RQ$. By assumption $d=\dim(\calH_Q)$ and by construction
$\dim(\calH_R)=\dim(\calH_Q)=d$. Hence

$$
  \dim(\calH_R\otimes\calH_Q) = d^2 .
$$

Hence $\sigma\in\CC^{d^2\times\?d^2}$ and therefore $M\leq\?d^2$. QED.

*** Proof 2
:PROPERTIES:
:CUSTOM_ID: exercise-8.10-proof-2
:END:
The matrix $W$ is hermitian because

$$
  W_{kj}^* = \trace{E_k^\dagger E_j}^*
  = \trace{(E_k^\dagger E_j)^\dagger}
  = \trace{E_j^\dagger E_k}
  = W_{jk} .
$$

Let $\calH$ be the $d$​-dimensional Hilbert space on which the $E_k$ operate. Recall that

$$
  A, B \mapsto \trace{A^\dagger B}
$$

is a scalar product on the $d^2$​-dimensional space of linear operators on $\calH$ (see
[[https://en.wikipedia.org/wiki/Hilbert%E2%80%93Schmidt_operator][Hilbert-Schmidt space]]). It turns it into a Hilbert space. The claim that
$\rank{W}\leq\?d^2$ thus follows from the following lemma (with $N=d^2$).

<<exercise-8.10-lemma>>
- Lemma :: Let $\{v_j\}_{j=1..M}$ be a bunch of vectors of an $N$​-dimensional complex
  Hilbert space $V$ and define

  $$
  W_{jk} = \braket{v_j}{v_k} .
  $$

  Then $\rank{W}\leq\?N$.

  - Proof :: Let $\{b_n\}_{n=1..N}$ be an ONB of $V$. Hence there are unique numbers
    $A_{jn}\in\CC$ such that

    $$
    v_j = \sum_{n=1}^N A_{jn} b_n .
    $$

    Interpreted as a matrix we have $A\in\CC^{M\times\?N}$, hence (the rank is always at
    most the minimum of the two dimensions)

    $$ \rank{A} \leq \min(M, N) \leq N . $$

    Consider

    $$
    W_{jk} = \braket{v_j}{v_k} = \sum_{nm} A_{jn}^* A_{km} \braket{b_n}{b_m}
    = \sum_n A_{jn}^* A_{kn}
    = (AA^\dagger)_{kj}
    $$

    Hence

    $$ W = \left(AA^\dagger \right)^\top $$

    Recall that transposition and conjugation does not change the rank of a
    matrix. Moreover $\rank{AB}\leq\min(\rank{A},\rank{B})$ for any two matrices
    $A,B$. The last thing follows easily from the characterization of the rank as the
    dimension of the image of a linear mapping. Hence

    $$
    \rank{W} \leq \rank{A} \leq N .
    $$

    QED.

Let $M$ be the number of elements. Hence $W\in\CC^{M\times\?M}$. Since $W$ is hermitian
there is a uniatry matrix $u\in\CC^{M\times\?M}$ such that

$$
  D = u W u^\dagger
$$

is diagonal. Observe that

$$
  D_{jk} = \sum_{mn} u_{jm} W_{mn} u_{kn}^*
  = \trace{\left(\sum_m u_{jm}^* E_m \right)^\dagger \left( \sum_n u_{kn}^* E_n \right)} .
$$

Hence defining $F_k=\sum_nu_{kn}^*E_n$ we have

$$
  D_{jk} = \trace{F_j^\dagger F_k} .
$$

We note two things:

- Since $D$ is diagonal the $F_j$ are actual orthogonal with respect to the
  Hilbert-Schmidt scalar product.
- Since $\rank{D}=\rank{W}\leq\?d^2$ only at most $d^2$ of the $F_j$ are non-zero
  ($\trace{F^\dagger\?F}=0$ implies $F=0$).

This concludes the proof. QED.

** Exercise 8.11
Suppose $\calE$ is a quantum operation mapping a $d$​-dimensional input space to a
$d'$​-dimensional output space. Show that $\calE$ can be described using a set of at most
$dd'$ operation elements $\{E_k\}$.

*** Proof
With a minor adaption we can reduce this to [[#exercise-8.10-proof-2][one of the proofs]] of [[#exercise-8.10][exercise 8.10]]. Let

$$
  d_1 = \max(d, d') .
$$

Then $E_k\in\CC^{d\times\?d'}$. By padding with zeros we can interpret the $E_k$ as
elements of a $dd'$​-dimensional subspace $V$ of $\CC^{d_1\times\?d_1}$. Note that this
does not affect the definition of $W$.

Now the proof of the special case $d=d'$ goes through as is. The crucial thing to note is
that the [[exercise-8.10-lemma][lemma]] has to be applied with $N=dd'$ and the just defined Hilbert space $V$. QED.

** WIP Exercise 8.12
Why can we assume that $O$ has determinant $1$ in the decomposition (8.93)?

# TODO: resolve the issue and check if det(M)>=0 is true. Remove the remark if yes.
<<remark-exercise-8.12>>
- Remark :: In /a sense/ my solution below solves the exercise. But I have the vague
  feeling that it is not meant like that. Playing around with examples I have the
  impression that $\det(M)\geq0$ just holds always. At the moment I can only prove this
  for a special case (see [[*Exercise 8.16][exercise 8.16]]). I do not even know if it is hard to prove. I
  have the feeling that it should not be so hard, but I still do not see it.

*** Maybe not a solution
As a real orthogonal matrix $O$ has $\det(O)=\pm1$. To see this recall that the
determinant is the product of all eigenvalues which are /complex/ number of modulus 1 for
an orthogonal matrix (which is unitary if interpreted as a complex matrix). But the
determinant of a real matrix is real, hence only $\pm1$ is possible for their product.

If the determinant is already $1$ we are done. Otherwise just replace $O$ by $-O$ and $S$
by $-S$. Of course this we destroy the property that $S$ is positive.

** Exercise 8.13
Show that unitary transformations correspond to rotations of the Bloch sphere.

*** Solution
Probably the idea is to prove it via the findings of the section (probably using the
formulas for $M_{jk}$ and $c_k$). Why else would this problem be in this chapter? But I
have not succeeded in doing so.

On the other hand we already know that claim and used it in earlier chapters. Therefore I
only sketch the proof. Essentially the problem is equivalent to [[file:chapter_4.org::#exercise-4-6][exercise 4.6]]. To see this
recall that every unitary operator is equal to some rotation $R_{\hat{n}}(\theta)$ around
some axis and some angle - up to a global phase. I have also made a small discussion for
the (easy) step to generalize the statement of the exercise to density matrix in [[file:chapter_4.org::thm-pauli-rotations-as-3d-rotations][chapter
4]].

** Exercise 8.14
Show that $\det(S)$ (From the polar decomposition of $M=OS$) need not be positive.

- Remark :: I assume that /positive/ means /strictly greater than zero/ here. Sometimes
  zero is included for positivity so it is always a bit unclear what is meant. There are
  two reasons to see it this way:

  - The /polar decomposition/ explicitly states that there is an orthogonal matrix $O$ and
    a /positive/ matrix $S$ (Moreover $S$ is unique and $O$ is unique on the support of
    $S$). Coincidentally here we already have a case were /positive/ actually means that
    $0$ is included, because it is defined as

    $$ \forall\ket{\psi}\neq0: \; \bra{\psi} S \ket{\psi} \geq 0 . $$

    In particular $S=0$, or more generally that the kernel of $S$ is non-trivial, is a
    possibility.

    But /positivity/ implies $\det(S)\geq0$. Hence $0$ can be the only violation of being
    "positive". Hence the strict interpretation of "positive" in this exercise. A loop
    hole would be that the exercise is somehow meant in some other way going away from the
    polar decomposition.
  - The second reason is that I believe that $\det(M)\geq0$ always holds. See also my
    [[remark-exercise-8.12][remark]] in exercise 8.12. If this is true I do not see any way to interpret the
    exercise in a way such that /not positive/ means /strictly negative/.

*** Solution
For the /bit-flip/ ($E_0=\sqrt{p}I$, $E_1=\sqrt{1-p}X$) we have

$$
  M = \diag(1, 2p-1, 2p-1)
$$

(and $c=0$). See chapter 8.3.3 or look into my [[sage-bit-flip][sage code]]. Clearly for $p=1/2$ we have
$\det(M)=0$. In that case $S=\sqrt{M^\dagger\?M}=M$ (and you could choose $O=I$). Hence
also $\det(S)=0$.

** Exercise 8.15
Suppose a projective measurement is performed on a single qubit in the basis $\ket{+}$,
$\ket{-}$, where $\ket{\pm}=(\ket{0}\pm\ket{1})/\sqrt{2}$. In the event that we are
ignorant of the result of the measurement, the density matrix evolves according to the
equation

$$
  \rho \mapsto \calE(\rho) = \proj{+}\rho\proj{+} + \proj{-}\rho\proj{-} .
$$

Illustrate this transformation on the Bloch sphere.

*** Solution
We reduce this to a known problem by transforming the operation into a phase-flip. This
has the advantage that we already know what it does in the Bloch-sphere, and even if not,
it is an easier problem since we can work in the standard basis (and know for example how
the matrices of the Pauli matrices look like in that basis).

Let $H=(X+Z)/\sqrt{2}$ be the Hadamard transform (note that $H^\dagger=H$ and
$H^2=1$). Recall that it swaps $X$ and $Z$ as well their eigenspace projectors
($\ket{\pm}$ are the eigenvectors of $X$). Hence

$$
  \calE'(\rho) = H \calE(H\rho H) H = \proj{0}\rho\proj{0} + \proj{1}\rho\proj{1} .
$$

Note that $\calE'$ is just the phase flip and we already know what it is. In the Bloch
sphere it corresponds to a projection onto the z-axis (see chapter 8.3.3):

$$
  M' = \diag(0, 0, 1), \; c' = 0 .
$$

In the Bloch-sphere interpretation of unitary operator $H$ acts as a rotation around
$(\hat{x}+\hat{z})/\sqrt{2}$ by 180°. It swaps $\hat{x}$ and $\hat{z}$ and negates
$\hat{y}$, that is, as a rotation in the Bloch sphere $\rho\mapsto\?H\rho\?H$ is given by
this matrix:

$$
  T = \begin{bmatrix}
    0 &  0 & 1 \\
    0 & -1 & 0 \\
    1 &  0 & 0 \end{bmatrix} .
$$

Hence the following describes $\calE$:

$$
  M = TM'T = \diag(1, 0, 0), \; c = Tc' = 0 .
$$

** Exercise 8.16
:PROPERTIES:
:CUSTOM_ID: exercise-8.16
:END:
The graphical method for understanding single qubit quantum operations was derived for
trace-preserving quantum operations. Find an explicit example of a non-trace-preserving
quantum operation which cannot be described as a deformation of the Bloch sphere, followed
by a rotation and a displacement.

*** Solution
Before delivering an example I find it instructive to discover what violations are even
possible from dropping the trace preservation property. Hence the solution is divided into
two parts.

**** What does still hold?
Let $E$ be an /arbitrary/ linear operator on $\CC^2$ and let us consider the following
quantum operation

$$
  \calE(\rho) = E \rho E^\dagger .
$$

Note that /physicality/ ($\trace{\calE(\rho)}\leq1$) might be violated for some $\rho$ but
for this exercise physicality is not really relevant. It could always be restored by
multiplication with a small $\varepsilon\?>\?0$.

Note that from Kraus' theorem we know that an arbitrary single-qubit operation needs up to
four operation elements (c.f. [[#exercise-8.10][exercise 8.10]]). We try to generalize some of our findings at
the end. But for now let us only consider the one-element case.

Normally $\calE$ only acts on density operators, that is, operators of the form

$$
  \rho = 1 + x X + y Y + z Z ,
$$

where $x,y,z\in\RR$. *For notational convenience we left out the factor $1/2$.* It does
not really matter for our purposes. Note that the space of such matrices is a
three-dimensional /real-affine/ subspace of $\CC^{2\times2}\equiv\CC^4$. We already know
from the book (see also [[#chapter-8-bloch-M-c][my calculations]] above) that trace-preserving operations map this
space into an real-affine subspace of $\CC^{2\times2}$. Fixing $I$ as the origin of these
spaces the resulting mapping is real-affine-linear (not sure how to say it correctly).

It simplifies matters to consider the larger (four-dimensional) real /linear/ (without the
/affine/) subspace $S$ consisting of the following matrices:

$$
  \rho = e + x X + y Y + z Z ,
$$

where $e,x,y,z\in\RR$. Note that /any/ matrix in $\CC^{2\times2}$ can be written like that
but with /complex/ coefficients (indeed: $I,X,Y,Z$ is an ONB with respect to the scalar
product $A,B\mapsto\trace{A^\dagger\?B}$). In particular $E$ can be written as

$$
  E = sI + uX + vY + wZ
$$

for $s,u,v,w\in\CC$. Let us prove the following Lemma.

- Lemma :: Any quantum operation (trace-preserving or not) acts as a /real/-linear map
  from $S$ to itself.
  - Proof :: Let us conveniently write $\rho=(e,x,y,z)$ in the basis $(I,X,Y,Z)$. We have

    $$
    X\rho Y = (-\ii z, y, x, \ii e) \text{ and } Y\rho X = (\ii z, y, x, -\ii e) .
    $$

    Certainly you can see the pattern here and generalize it to the other two
    Pauli-operator pairs. Moreover

    $$
    X\rho X = (e, x, -y, -z) ,
    $$

    and analogously for the other two Pauli-operators. Hence

    $$ \rho\mapsto\?uX\rho\?u^*X = (\chi e, \chi x, -\chi y, -\chi z) $$

    where $\chi=\abs{u}^2$. Hence this type of transformation is /real/-linear. On the
    other hand it is easy to see that

    $$
    uX \rho v^*Y + vY \rho u^* X = (-\beta z, \alpha y, \alpha x, \beta e)
    $$

    where $\alpha=2\Re(u^*v)$ and $\beta=\ii\?uv^*-\ii\?u^*v=2\Im(u^*v)$. Hence this
    transformation is also real-linear. Observe that

    $$
    E \rho E^\dagger
    $$

    can be decomposed into a sum of transformations similar to the proceeding ones. Hence
    it is real linear too. Since a quantum operation is a sum of such one-element
    operations it follows that any quantum operation is real linear. QED.


Clearly a map is trace-preserving if it maps the affine subspace described by $e=1$ into
itself (works for any affine subspace defined by setting $e$ to a non-zero
constant). Hence, we derived in a slightly different way the conclusion that
trace-preserving quantum operations correspond to mappings within the Bloch-space.

It would be interesting to /characterize/ what maps are possible. I believe they are
characterized by $\det(M)\geq0$ (where $M:\RR^4\to\RR^4$ for the non-trace-preserving
maps). At least for single-element operations we can use sage to verify that the
determinant is always non-negative:

#+name: proof-positive-det-single-element
#+begin_src sage :tangle no :results replace :cache yes
  E = s*Id + u*X + v*Y + w*Z
  M = make_qop1d_matrix_4d([E])
  factor(det(M))
#+end_src

#+RESULTS[f0410405cc09c57bb8f535542e044932c95081a9]: proof-positive-det-single-element
: (s^2 - u^2 - v^2 - w^2)^2*(conjugate(s)^2 - conjugate(u)^2 - conjugate(v)^2 - conjugate(w)^2)^2

So for $\calE(\rho)=E\rho\?E^\dagger$ with an arbitrary $E=s+uX+vY+wZ$ we have

$$
  \det(M) = \abs{s^2-u^2-v^2-w^2}^4 .
$$

In principle the most general operation can be constructed from four single operation
elements (see [[*Exercise 8.10][exercise 8.10]]). However the symbolic determinant as computed by =sage= is
too complicated to see a pattern (or to use =factor=).

**** Example
We have seen in the first part of the solution that non-trace-preserving operation can at
least be interpreted in $\RR^4$. I also conjectured that this representation has
non-negative determinant. For trace preserving operations this would imply that the
determinant with respect to $\RR^3$ is also positive.

The following is an example for a non-trace-preserving operation with negative determinant
with respect to $\RR^3$:

$$
  E_0 = I, \quad E_1 = X + \ii Y .
$$

Indeed, for $\rho=(e,x,y,z)$ with respect to the Pauli basis $(I,X,Y,Z)$ (recall $e=1$ for
density matrices) we have

$$
  \calE(\rho) = (3e-2z,x,y,2e-z) .
$$

The following sage code "proves" it:

#+begin_src sage :tangle no :results replace
  E0 = Id
  E1 = X + i*Y
  rho = e*Id + x*X + y*Y + z*Z

  qop1d([E0, E1], rho)
#+end_src

#+RESULTS:
: (3*e - 2*z, x, y, 2*e - z)

If we project this into the three dimensional Bloch-space the corresponding matrix is
$\diag(1,1,-1)$, which has a negative determinant. Hence it cannot be represented as a
deformation of the Bloch sphere, followed by a rotation and a displacement. Otherwise the
determinant would be non-negative (as is the case in $\RR^4$).

** Exercise 8.17
Verify (8.101) as follows. Define

$$
  \calE(A) = \frac{A + XAX + YAY + ZAZ}{4} ,
$$

and show that

$$
  \calE(I) = I; \quad \calE(X) = \calE(Y) = \calE(Z) = 0 .
$$

Now use the Bloch sphere representation for single qubit density matrices to verify
(8.101).

*** Solution
This is almost to simple to justify but let me try my best. The claim $\calE(I)=I$ follows
from $X^2=Y^2=Z^2=I$. That $\calE(N)=0$ for any Pauli matrix $N$ follows from the fact
that $NMN=-M$ if $M$ and $N$ are different Pauli matrices and $NMN=N$ otherwise.

Finally, that $\calE(\rho)=I/2$ for any density matrix follows from linearity of $\calE$
and the fact that any density matrix can be written as

$$
  \rho = \frac{1}{2} (I + x X + y Y + z Z)
$$

for $x,y,z\in\RR$ (see [[file:chapter_2.org::#exercise-2.72][exercise 2.72]]).

** Exercise 8.18
For $k\geq1$ show that $\trace{\rho^k}$ is never increased by the action of the
depolarizing channel.

*** Proof
Recall that in a suitable orthonormal basis every density matrix is represented by a
diagonal matrix

$$
  \diag(q_1,\ldots,q_d)
$$

with $\sum_jq_j=1$ and $q_j\geq0$. Hence

$$
  \trace{\rho^k} = f_k(q)
$$

where

$$
  f_k(q) = \sum_{j=1}^d q_j^k .
$$

Observe that $f_k$ is a strictly convex function, that is

$$
  f_k(\lambda q + (1-\lambda) q') < \lambda f_k(q) + (1-\lambda) f_k(q')
$$

for all $\lambda\in(0,1)$ and $q\neq\?q'$ (and $\sum_jq_j=1$ and $q_j\geq0$, and similarly
for $q'$). This follows from the convexity of the scalar valued function $x\mapsto\?x^k$,
which in turn follows from the positivity of the second derivatives (on the domain of
definition). By the method of [[https://en.wikipedia.org/wiki/Lagrange_multiplier][Lagrange multipliers]] we easily see that

$$
  q^\min = (1/d,\ldots,1/d)
$$

is the unique minimum of $f_k$. Now we are prepared to prove the claim. In fact:

$$
  \trace{\calE(\rho)} = f_k(pq^\min + (1-p)q) \leq p f_k(q^\min) + (1-p) f_k(q)
  \leq f_k(q) = \trace{\rho} .
$$

QED.

** Exercise 8.19
Find an operator-sum representation for a generalized depolarizing channel acting on a
$d$​-dimensional Hilbert space.

*** Solution
We will define $d^2$ operation elements

$$
  E_{mn} = \frac{1}{d} P_m Q_n \text{ for } m,n \in \{0,1,\ldots,d-1\}
$$

which implement the operation $\calE(\rho)=I/d$. Hence by scaling with $\sqrt{p}$ and
adding an additional element $\sqrt{1-p}I$ we can implement the depolarizing channel. In
the following let $\rho=\sum_{ij}\rho_{ij}\ket{i}\bra{j}$. For $n\in\{0,\ldots,d-1\}$
define

$$
  Q_n \ket{j} = e^{2\pi\ii nj/d} \ket{j} .
$$

Observe that

$$
  \sum_n Q_n \ket{j}\bra{k} Q_n^\dagger = d \delta_{jk} .
$$

Hence

$$
  \rho' = \frac{1}{d} \, \sum_n Q_n \rho Q_n^\dagger = \sum_j \rho_{jj} \proj{j} .
$$

Next, for $n\in\{0,\ldots,d-1\}$ define

$$
  P_n\ket{j} = \sum_j \ket{n+j} ,
$$

where the sum $n+j$ is interpreted modulo $d$. Hence the $P_n$ are cyclic permutations.

$$
  \frac{1}{d} \sum_n P_n \rho' P_n^\dagger
  = \frac{1}{d} \sum_{jn} \rho_{jj} \ket{n+j}\braket{j}{j}\bra{n+j}
  = \frac{1}{d} I .
$$

QED.

** Exercise 8.20 (Circuit model for amplitude damping)
:PROPERTIES:
:CUSTOM_ID: exercise-8.20
:END:
Show that the circuit in Figure 8.13 models the amplitude damping quantum operation, with
$\sin(\theta/2)^2=\gamma$.

#+RESULTS[e0b7f987e2dd275eaf27c7e092996285a70f4941]: exercise-8.20-circuit
:                  ┌───┐
: rho_in: ────■────┤ X ├
:         ┌───┴───┐└─┬─┘
:    |0>: ┤ Ry(θ) ├──■──
:         └───────┘


#+name: exercise-8.20-circuit
#+begin_src python :results replace :cache yes :exports results
  qr0 = QuantumRegister(1, "rho_in")
  qr1 = QuantumRegister(1, "|0>")

  qc = QuantumCircuit(qr0, qr1)
  params = (Param("θ"),)

  qc.cry(params[0], 0, 1)
  qc.cx(1, 0)

  qc.draw()
#+end_src

*** Solution
Let $B'$ be the action of the circuit:

\begin{align*}
  B' &= (I \otimes P_0 + X \otimes P_1) \cdot (P_0 \otimes I + P_1 \otimes R_y(\theta)) \\
  &= P_0 \otimes P_0 + P_1 \otimes P_0 R_y(\theta) + XP_0 \otimes P_1 + XP_1 \otimes P_1 R_y(\theta) .
\end{align*}

Let $\ket{\psi}=a\ket{0}+b\ket{1}$. Then

$$
  B' \ket{\psi}\ket{0} = a \ket{00} + bc\ket{10} + 0 + bs\ket{01}
  = a\ket{00} + b(c\ket{10} + s\ket{01}) ,
$$

where $c=\cos(\theta/2)$ and $s=\sin(\theta/2)$. Thus

\begin{align*}
  E_0 \ket{\psi} = \bra{0_E} B' \ket{\psi}\ket{0_E} &= a\ket{0} + bc\ket{1} \\
  E_1 \ket{\psi} = \bra{1_E} B' \ket{\psi}\ket{0_E} &= bs\ket{0} \\
\end{align*}

In other words, using $s=\sqrt{\gamma}$ and $c=\sqrt{1-\gamma}$, we have

\begin{align*}
  E_0 &= \begin{bmatrix} 1 & 0 \\ 0 & \sqrt{1-\gamma} \end{bmatrix} , \\
  E_1 &= \begin{bmatrix} 0 & \sqrt{\gamma} \\ 0 & 0 \end{bmatrix} .
\end{align*}

This is as desired.

** Exercise 8.21 (Amplitude damping of a harmonic oscillator)
Suppose that our principal system, a harmonic oscillator, interacts with an environment,
modeled as another harmonic oscillator, through the Hamiltonian

$$
  H = \chi (a^\dagger b + a b^\dagger)
$$

where $a$ and $b$ are the annihilation operators for the respective harmonic oscillators,
as deﬁned in Section 7.3.

1) Using $U=\exp(-\ii\?H\tau)$, denoting the eigenstates of $b^\dagger\?b$ as $\ket{k_b}$,
   and selecting the vacuum state $\ket{0_b}$ as the initial state of the environment, show
   that the operation elements $E_k=\bra{k_b}U\ket{0_b}$ are found to be

   $$
   E_k = \sum_{n=k}^\infty \sqrt{\binom{n}{k}} \sqrt{(1-\gamma)^{n-k}\gamma^k} \ket{n-k}\ket{n} ,
   $$

   where $\gamma=1-\cos(\chi\tau)^2$ is the probability of loosing a single quantum of
   energy, and states such as $\ket{n}$ are eigenstates of $a^\dagger\?a$.
2) Show that the operation elements $E_k$ define a trace-preserving quantum operation.

- Remark :: I am not really sure why it is /this/ formula for $H$. From the text I would
  have expected $H=\chi(a^\dagger\?b-ab^\dagger)$. But it doesn't matter for this exercise, both work.

*** Proof of part 1
Recall a [[https://en.wikipedia.org/wiki/Baker%E2%80%93Campbell%E2%80%93Hausdorff_formula#An_important_lemma_and_its_application_to_a_special_case_of_the_Baker%E2%80%93Campbell%E2%80%93Hausdorff_formula][lemma]] related to the Baker-Campbell-Hausdorff formula and Lie-Algebras:

$$
  e^{\lambda G} A e^{-\lambda G} = \sum_n \frac{\lambda^n}{n!} [(G)^n, A] ,
$$

where $[(G)^n,A]$ is the $n$​-fold commutator. We will apply it to
$G=a^\dagger\?b+ab^\dagger$ and $A=a$. We have

$$
  [G, a] = -b \text{ and } [G, b] = -a .
$$

In particular $[(G)^2,A]=A$ for $A\in\{a,b\}$. Hence (with $\lambda=-\ii\chi\tau$,
and abbreviating $\theta=\chi\tau$)

$$
  U a U^\dagger = \sum_n \frac{\theta^n}{n!} c_n
$$

where

$$
  c_n = \begin{cases} i^n a & \text{if } n \text{ is even.} \\ i^n b & \text{else.} \end{cases} .
$$

Thus, using the Taylor expansion of $\sin$ and $\cos$, we get

$$
  U a U^\dagger = \cos(\theta) a  + \sin(\theta) b .
$$

In the following let us abbreviate $c=\cos(\theta)$ and $s=\sin(\theta)$. Observe
$s^2=\gamma$ and $c^2=1-\gamma$. Using the formula for $UaU^\dagger$ we obtain

$$
  U \ket{n_a, 0_b} = \frac{1}{\sqrt{n!}} \, U (a^\dagger)^n \ket{0_a, 0_b}
  = \frac{1}{\sqrt{n!}} \, (ca^\dagger+sb^\dagger)^n U \ket{0_a, 0_b}
  = \frac{1}{\sqrt{n!}} \, (ca^\dagger+sb^\dagger)^n \ket{0_a, 0_b} .
$$

Since $a$ and $b$ commute we can use the binomial formula to obtain

$$
  U \ket{n_a, 0_b} = \frac{1}{\sqrt{n!}} \sum_n \binom{n}{k} (ca^\dagger)^{n-k} (sb^\dagger)^k \ket{0_a,0_b}
  = \sum_{n\geq k} \sqrt{\binom{n}{k}} c^{n-k} s^k \ket{(n-k)_a,k_b} ,
$$

which is the claim of part 1. QED.

*** Proof of part 2
This can be proved in very abstract terms. We just have to use that
$E_k=\bra{k_b}U\ket{0_b}$ where $U$ is unitary (on the combined system) and
$\{\ket{k_b}\}$ is an ONB on the environment. In fact (denoting by $Q$ the principal
system)

$$
  \sum_k E_k^\dagger E_k = \sum_k \bra{0_b} U^\dagger \proj{k_b} U \ket{0_b}
  = \bra{0_b} U^\dagger U \ket{0_b} = I_Q .
$$

This identity characterizes trace-preserving operations. QED.

- Remark :: It wouldn't be hard to directly verify the claim using the closed formula for
  $E_k$ from this concrete example. Writing
  $a_{nk}=\sqrt{\binom{n}{k}(1-\gamma)^{n-k}\gamma^k}$ the claim essentially follows from the binomial formula

  $$
  \sum_{k=0}^n a_{nk}^2 = ((1-\gamma) + \gamma)^n = 1 .
  $$

** Exercise 8.22 (Amplitude damping of single qubit density matrix)
:PROPERTIES:
:CUSTOM_ID: exercise-8.22
:END:
For the general single qubit state

$$
  \rho = \begin{bmatrix} a & b \\ b^* & c \end{bmatrix} .
$$

show that amplitude damping leads to

$$
  \calE_{AD}(\rho) = \begin{bmatrix}
    1 - (1-\gamma)(1-a) & b\sqrt{1-\gamma} \\
    b^*\sqrt{1-\gamma}  & c(1-\gamma) \end{bmatrix} .
$$

*** Solution
The claim essentially follows from this small sage script

#+begin_src sage :tangle no :results replace :cache yes
  E0 = matrix.diagonal([1, sqrt(1-g)])
  E1 = matrix([[0, sqrt(g)], [0, 0]])

  a, c = SR.var('a c', domain='real')
  b = SR.var('b', domain='complex')

  rho = matrix([
      [a, b],
      [b.conjugate(), c],
  ])

  A = simplify(E0*rho*E0.H + E1*rho*E1.H)

  assert A == matrix([
      [               c*g + a, b*sqrt(1-g)],
      [sqrt(1-g)*conjugate(b), c*(1-g)]
  ])
  "PASSED"
#+end_src

#+RESULTS[50b46336e486c1bd5cf3c638500a734f4ee18308]:
: 'PASSED'

The upper left corner looks different to what was stated in the exercise. But using
$a+c=1$ equality is easily verified.

** Exercise 8.23 (Amplitude damping of dual-rail qubits)
Suppose that a single qubit state is represented by using two qubits, as

$$
  \ket{\psi} = a \ket{01} + b \ket{10} .
$$

Show that $\calE_{AD}\otimes\calE_{AD}$ applied to this state gives a process which can be
described by the operation elements

\begin{align*}
  E_0^{\mathrm{dr}} &= \sqrt{1-\gamma} I, \\
  E_1^{\mathrm{dr}} &= \sqrt{\gamma} \ket{00}\bra{01}, \\
  E_2^{\mathrm{dr}} &= \sqrt{\gamma} \ket{00}\bra{10},
\end{align*}

that is, either nothing ($E_0^{\mathrm{dr}}$) happens to the qubit, or the qubit is
transformed ($E_1^{\mathrm{dr}}$, $E_2^{\mathrm{dr}}$) into the state $\ket{00}$, which is
orthogonal to $\ket{\psi}$. This is a simple error-detection code, and is also the basis
for the robustness of the ‘dual-rail’ qubit discussed in Section 7.4.

- Remark :: The original exercise statement contained a small error. It claimed that only
  two operation elements are needed: $E_0^{\mathrm{dr}}$ and
  $E_1^{\mathrm{dr}}+E_2^{\mathrm{dr}}$. This is incorrect.

  Moreover the formula for $E_0^{\mathrm{dr}}$ needs a proper interpretation. But we will
  make it precise in the solution below.

*** Proof
Let us abbreviate $c=\sqrt{1-\gamma}$ and $s=\sqrt{\gamma}$. Recall the operation elements
of $\calE_{AD}$:

$$
  E_0 = \begin{bmatrix} 1 & 0 \\ 0 & c \end{bmatrix}, \quad
  E_1 = \begin{bmatrix} 0 & s \\ 0 & 0 \end{bmatrix}.
$$

A set of operation elements for $\calE_{AD}\otimes\calE_{AD}$ is given by the four
elements $\{E_{ij}:=E_i\otimes\?E_j\}$ which we will write down in the following:

$$
  E_{00} = \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & c & 0 & 0 \\
    0 & 0 & c & 0 \\
    0 & 0 & 0 & c^2 \end{bmatrix}
$$
and

$$
  E_{01} = \begin{bmatrix}
    0 & s & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & sc \\
    0 & 0 & 0 & 0 \end{bmatrix}
$$

and

$$
  E_{10} = \begin{bmatrix}
    0 & 0 & s & 0 \\
    0 & 0 & 0 & sc \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \end{bmatrix}
$$

and

$$
  E_{11} = \begin{bmatrix}
    0 & 0 & 0 & s^2 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \end{bmatrix} .
$$

Here the rows and columns correspond to the basis

$$ (\ket{00},\ket{01},\ket{10},\ket{11}) $$

in that order. Observe that all four operation elements leave the subspace spanned by the
first three basis elements invariant. This is important since $\ket{\psi}$ lives in that
subspace. *With respect to this subspace* we have

\begin{align*}
  E_{00} &= \proj{00} + c(\proj{01} + \proj{10}) , \\
  E_{01} &= s \ket{00}\bra{01} , \\
  E_{10} &= s \ket{00}\bra{10} , \\
  E_{11} &= 0 .
\end{align*}

In particular we only need three elements and we have $E_1^{\mathrm{dr}}=E_{01}$ and
$E_2^{\mathrm{dr}}=E_{10}$. Moreover $E_{00}$ corresponds to $cI$ if we understand $I$ to
be the identity on the subspace spanned by $\ket{01}$ and $\ket{10}$, which makes sense
since $\ket{\psi}$ even lives in that space. QED.

** Exercise 8.24 (Spontaneous emission is amplitude damping)
A single atom coupled to a single mode of electromagnetic radiation undergoes spontaneous
emission, as was described in Section 7.6.1. To see that this process is just amplitude
damping, take the unitary operation resulting from the Jaynes–Cummings interaction,
Equation (7.77), with detuning $\delta=0$, and give the quantum operation resulting from
tracing over the field.

*** Solution
The Jaynes-Cummings interaction is /essentially/ given by

$$
  H = \delta Z + g(a^\dagger \sigma_- + a \sigma_+) ,
$$

where $Z$, $\sigma_\pm=X\mp\ii\?Y$ act on the two-level atom and $a$, $a^\dagger$ on the
field (see e.g. equation (7.71)). In the basis $(\ket{00},\ket{01},\ket{10})$ (the right
qubit is the atom) the Hamiltonian looks as follows:

$$
  H = \begin{bmatrix}
    \delta & 0 & 0 \\
    0 & -\delta & g \\
    0 & g & \delta \end{bmatrix} .
$$

Let $U=\exp(-\ii\?H\tau/2)$ and define $c=\cos(g\tau)$, $s=\sin(g\tau)$. For $\delta=0$ we
have

$$
  U = \begin{bmatrix}
    1 & 0 & 0 \\
    0 & c & -\ii s \\
    0 & -\ii s & c \end{bmatrix} .
$$

Let $\rho_E=\proj{0}$ be the state of the field - the environment - with zero
photons. Then the following clearly describes spontaneous emission:

$$
  \calE(\rho) = \ptrace{E}{U \rho_E\otimes\rho U^\dagger} .
$$

We have to show that this is actually amplitude damping.

#+begin_src sage :results replace :tangle no
  theta = SR.var('theta', domain='real')
  b, b1 = SR.var('b b1', domain='complex')

  c = cos(theta)
  s = sin(theta)

  # NOTE: we add a trivial action on |11>. The point of this is solely to be able to
  # calculate the partial trace below. The function we use only accepts matrices with
  # dimensions being a power of 2 (more precisely: Hilbert spaces described by qubits). If a
  # state would actually be equal to |11> the Jaynes-Cummings interaction would not leave it
  # invariant. But we can use this as a mathematical trick since the three-dimensional
  # subspace we consider contains everything we need and is left invariant by the
  # interaction.
  U = matrix([
      [1,    0,    0, 0],
      [0,    c, -i*s, 0],
      [0, -i*s,    c, 0],
      [0,    0,    0, 1],
  ])

  # An arbitrary mixed state (as input for the operation).
  # b1 is actually conjugate(b) but this looks ugly and we do not need it.
  rho = matrix([
      [p, b],
      [b1, 1-p],
  ])

  M = U * kron(P0, rho) * U.H

  # TODO: reimplement for sage
  from chapter_2 import ptrace
  from sympy import Matrix

  M = Matrix(M)
  T = ptrace(M, [1]) # bits are counted from the right.
  T
#+end_src

#+RESULTS:
: Matrix([
: [p - (p - 1)*sin(theta)**2,           b*cos(theta)],
: [            b1*cos(theta), -(p - 1)*cos(theta)**2]])

Comparing this with the statement of [[#exercise-8.22][exercise 8.22]] we see that this is indeed amplitude
damping with $\gamma=s^2=\sin(g\tau)^2$.

** Exercise 8.25
If we deﬁne the temperature $T$ of a qubit by assuming that in equilibrium the
probabilities of being in the $\ket{0}$ and $\ket{1}$ states satisfy a Boltzmann
distribution, that is $p_0=e^{-E_0/k_BT}/\mathcal{Z}$ and $p_1=e^{-E_1/k_BT}/\mathcal{Z}$,
where $E_0$ is the energy of the state $\ket{0}$, $E_1$ the energy of the state $\ket{1}$,
and $\mathcal{Z}=e^{-E_0/k_BT}+e^{-E_1/k_BT}$, what temperature describes the state
$\rho_\infty$?

*** Solution
By convention we typically assume $E_0\?<\?E_1$ (and we assume those values to be given
constants). Note that this is equivalent to $p_0\?>\?1/2$. The limit case $E_0=E_1$
corresponds to $p_0=1/2$. It is easy to see that

$$
  p_0 = \frac{1}{1 + \exp(-(E_1-E_0)/k_BT)}
$$

This can be used to get a formula for $T$ in terms of $p=p_0$:

$$
  T = \frac{E_1 - E_0}{k_B \log(p/(1-p))} .
$$

This is the temperature of a state $\rho=p\proj{0}+(1-p)\proj{1}$. We have the following
edge cases:

$$
  T(p=1) = \lim_{p\to1} T(p) = 0, \quad T(p=1/2) = \lim_{p\to1/2} T(p) = \infty .
$$

** Exercise 8.26 (Circuit model for phase damping)
Show that the circuit in Figure 8.15 can be used to model the phase damping quantum
operation, provided $\theta$ is chosen appropriately.

#+RESULTS[930a50ce732943d0ccbd67a1ed775bd507219e78]: exercise-8.26-circuit
:
: rho_in: ────■────
:         ┌───┴───┐
:    |0>: ┤ Ry(θ) ├
:         └───────┘


#+name: exercise-8.26-circuit
#+begin_src python :results replace :cache yes :exports results
  qr0 = QuantumRegister(1, "rho_in")
  qr1 = QuantumRegister(1, "|0>")

  qc = QuantumCircuit(qr0, qr1)
  params = (Param("θ"),)

  qc.cry(params[0], 0, 1)

  qc.draw()
#+end_src

*** Proof
This is similar to [[#exercise-8.20][exercise 8.20]] - just simpler. Let $B$ be the operation of the circuit:

$$
  B = P_0 \otimes I + P_1 \otimes R_y(\theta) .
$$

Let $\ket{\psi}=a\ket{0}+b\ket{1}$. Then

$$
  B\ket{\psi}\ket{0_E} = a \ket{00} b\ket{1} R_y(\theta)\ket{0}
  = a\ket{00} + b(c\ket{10} + s\ket{11}) ,
$$

where $c=\cos(\theta)$ and $s=\sin(\theta)$. From here we get

\begin{align*}
  E_0\ket{\psi} &= \bra{0_E} B \ket{\psi}\ket{0_E} = a \ket{0} + bc\ket{1} , \\
  E_1\ket{\psi} &= \bra{1_E} B \ket{\psi}\ket{0_E} = bs \ket{1} .
\end{align*}

In other words:

$$
  E_0 = \begin{bmatrix} 1 & 0 \\ 0 & c \end{bmatrix}, \quad
  E_1 = \begin{bmatrix} 0 & 0 \\ 0 & s \end{bmatrix},
$$

which is phase damping (at least for e.g. $\theta\in[0,\pi/2]$) - as desired. QED.

** Exercise 8.27 (Phase damping = phase flip channel)
:PROPERTIES:
:CUSTOM_ID: exercise-8.27
:END:
Give the unitary transformation which relates the operation elements of (8.127)–(8.128) to
those of (8.129)–(8.130); that is, find $u$ such that $\tilde{E}_k=\sum_ju_{kj}E_j$.

For reference:

$$
  E_0 = \begin{bmatrix} 1 & 0 \\ 0 & \sqrt{1-\lambda} \end{bmatrix} , \;
  E_1 = \begin{bmatrix} 0 & 0 \\ 0 & \sqrt{\lambda} \end{bmatrix} .
$$

and $\tilde{E}_0=\sqrt{\alpha}I$, $\tilde{E}_1=\sqrt{1-\alpha}Z$.

*** Solution 1
In the following I prove

<<exercise-8.27-1>>
\begin{align*}
  \tilde{E}_0 &= \sqrt{\alpha} E_0 + \sqrt{1-\alpha} E_1 , \\
  \tilde{E}_1 &= \sqrt{1-\alpha} E_0 - \sqrt{\alpha} E_1 ,
\end{align*}

and briefly show how to come up with precisely this ansatz. Recall that the action of
phase damping is given by

$$
  \?\begin{bmatrix} p & b \\ b^* & q \end{bmatrix}
  \mapsto \begin{bmatrix} p & \sqrt{1-\lambda}\,b \\ \sqrt{1-\lambda}\,b^* & q \end{bmatrix} .
$$

Phase flip is given by

$$
  \?\begin{bmatrix} p & b \\ b^* & q \end{bmatrix}
  \mapsto \begin{bmatrix} p & (2\alpha-1)b \\ (2\alpha-1)b^* & q \end{bmatrix} .
$$

Hence there is a bijection $[0,1]\to[1/2,1]$ given by

<<exercise-8.27-2>>
$$ \alpha=(1+\sqrt{1-\lambda})/2 $$

which maps between phase damping and "half of" all phase flips. This already shows that
phase damping is just phase flip and we already have a formula for $\alpha$. Hence there
must be a unitary matrix $u$ such that

\begin{align*}
  \tilde{E}_0 &= u_{00} E_0 + u_{01} E_1 , \\
  \tilde{E}_1 &= u_{10} E_0 + u_{11} E_1 .
\end{align*}

By inspecting the matrices it is clear that we /must/ have $u_{00}=\sqrt{\alpha}$ and
$u_{10}=\sqrt{1-\alpha}$. Since the matrix $u$ is unitary we also must have
$\abs{u_{01}}=\sqrt{1-\alpha}$ and $\abs{u_{11}}=\sqrt{\alpha}$. Since everything else is
real we directly conclude that $u_{10}$ and $u_{11}$ have to be real too. By looking at
the $11$ entries of the operation elements we directly see that $u_{01}$ must be
positive. Hence by (e.g.) unitarity of $u$ we deduce that $u_{11}$ must be negative. Hence
the only possible $u$ is the one implicitly given in the [[exercise-8.27-1][above equation]].

Since we already know that $u$ must exist and we also showed that it is unique the
[[exercise-8.27-1][mentioned equation]] follows.

*** Solution 2
This is not really a second solution but more like a consistency check for the first
solution (we all make mistakes so it is best practice to double check things from time to
time).

The [[exercise-8.27-1][claim]] we have to prove is equivalent to:

\begin{align*}
  \sqrt{\alpha} &= \sqrt{\alpha} \sqrt{1-\lambda} + \sqrt{1-\alpha} \sqrt{\lambda} , \\
  -\sqrt{1-\alpha} &= \sqrt{1-\alpha} \sqrt{1-\lambda} - \sqrt{\alpha} \sqrt{\lambda} .
\end{align*}

In principle there would be four equations (one for each diagonal entry and each
phase-flip element) but the other two are just trivial. Let us abbreviate
$c=\sqrt{1-\lambda}$ and $s=\sqrt{\lambda}$. Note that $c,s\in[0,1]$ and $c^2+s^2=1$. The
first equation is equivalent to

$$
  \sqrt{\alpha} = \sqrt{\alpha} c + \sqrt{1-\alpha} s .
$$

Before we go on, recall [[exercise-8.27-2][this formula]] $2\alpha=1+c$. It suffices to show that twice the
square of the RHS is $2\alpha=1+c$ (squaring only hides the sign but we already see that
both sides are positive). Let us check this:

\begin{align*}
  2(\sqrt{\alpha} \, c + \sqrt{1-\alpha} \, s)^2 &= (\sqrt{1+c} \, c + \sqrt{1-c} \, s)^2 \\
  &= (1+c)c^2 + (1-c)s^2 + 2\sqrt{1-c^2} \, sc \\
  &= 1 + c(c^2+s^2) \\
  &= 1 + c .
\end{align*}

This is as desired. The second equation

$$
  -\sqrt{1-\alpha} = \sqrt{1-\alpha} c - \sqrt{\alpha} s .
$$

can also be shown by showing that twice the square of the RHS is equal to
$2(1-\alpha)=1-c$ (the sign makes no problem since the RHS is at most $\sqrt{1-\alpha}$
and this only if $c=1$, $s=0$ which implies $\alpha=1$, a special case easily treated
separately). Let us also check this:

\begin{align*}
  2(\sqrt{1-\alpha} \, c - \sqrt{\alpha} \, s)^2 &= (\sqrt{1-c} \, c - \sqrt{1+c} \, s)^2 \\
  &= (1-c)c^2 + (1+c)s^2 - 2\sqrt{1-c^2} \, sc \\
  &= 1 - c(c^2+s^2) \\
  &= 1 - c .
\end{align*}

** Exercise 8.28 (One =CNOT= phase damping model circuit)
Show that a single =CNOT= gate can be used as a model for phase damping, if we let the
initial state of the environment be a mixed state, where the amount of damping is
determined by the probability of the states in the mixture.

*** Solution
As an intermediate step let us find a solution involving a =CZ= gate instead of a =CNOT=
gate. It is relatively intuitive how to do this if we recall that phase damping is just
phase flip ([[#exercise-8.27][exercise 8.27]])

#+RESULTS[8b9bb4418759a8974222b730004e38e57042b39b]: exercise-8.28-circuit-1
:
:                rho_in: ─■─
:                         │
: p|0><0| + (1-p)|1><1|: ─■─
:

Here =CZ= gate is to be interpreted as having its control at the environment (it is
symmetric of course but it helps to imagine it that way). The environment is in the mixed state

$$ \rho_E = p\proj{0} + (1-p)\proj{1} $$

which activates the =CZ= with probability $1-p$ and does nothing otherwise. A similar
construction was done for the deploarizing channel in figure 8.12. To /rigorously/ prove
that this actually implements the phase flip let $U$ be the =CZ= gate and consider

\begin{align*}
  \ptrace{E}{U \rho\otimes\rho_E U^\dagger}
  &= p \, \ptrace{E}{U \rho\otimes P_0 U^\dagger} + (1-p) \, \ptrace{E}{U \rho\otimes P_1 U^\dagger} \\
  &= p \, \ptrace{E}{\rho\otimes P_0} + (1-p) \, \ptrace{E}{(Z\rho Z)\otimes P_1} \\
  &= p \, \rho + (1 - p) \, Z\rho Z .
\end{align*}

Hence the above circuit implements phase flip. By [[#exercise-8.27][exercise 8.27]] we know that setting
$p=(1+\sqrt{1-\lambda})/2$ (c.f. [[exercise-8.27-2][solution]] of that exercise) is phase damping with
parameter $\lambda$.


Using the Hadamard gate and $HXH=Z$ the circuit can be converted to a cicuit using a
=CNOT=:

#+RESULTS[52c921af550981981384a248599e6af61654f59b]: exercise-8.28-circuit-2
:
:                rho_in: ───────■───────
:                        ┌───┐┌─┴─┐┌───┐
: p|0><0| + (1-p)|1><1|: ┤ H ├┤ X ├┤ H ├
:                        └───┘└───┘└───┘

Note that we actually do not need the second Hadamard gate since it only acts on the
environment /after/ the interaction took place. That is, the following circuit implements
the same operation:

#+RESULTS[b65bf6c4a82ce0f00b8e239340868509c5e89f39]: exercise-8.28-circuit-3
:
:                rho_in: ───────■──
:                        ┌───┐┌─┴─┐
: p|0><0| + (1-p)|1><1|: ┤ H ├┤ X ├
:                        └───┘└───┘

The Hadamard gate maps between the eigenbasis $(\ket{0},\ket{1})$ of $Z$ and the
eigenbasis $(\ket{0},\ket{1})$ of $X$. Hence the following circuit still implements phase
damping:

#+RESULTS[ca4b07efc50e57effd25279b3fd2bed296c7e566]: exercise-8.28-circuit-4
:
:                rho_in: ──■──
:                        ┌─┴─┐
: p|+><+| + (1-p)|-><-|: ┤ X ├
:                        └───┘

That is, the environment is in the state $p\proj{+}+(1-p)\proj{-}$ and the =CNOT= gate has
its target there.

#+name: exercise-8.28-circuit-1
#+begin_src python :results replace :cache yes :exports results
  qr0 = QuantumRegister(1, "rho_in")
  qr1 = QuantumRegister(1, "p|0><0| + (1-p)|1><1|")

  qc = QuantumCircuit(qr0, qr1)

  qc.cz(1, 0)

  qc.draw()
#+end_src

#+name: exercise-8.28-circuit-2
#+begin_src python :results replace :cache yes :exports results
  qr0 = QuantumRegister(1, "rho_in")
  qr1 = QuantumRegister(1, "p|0><0| + (1-p)|1><1|")

  qc = QuantumCircuit(qr0, qr1)

  qc.h(1)
  qc.cx(0, 1)
  qc.h(1)

  qc.draw()
#+end_src

#+name: exercise-8.28-circuit-3
#+begin_src python :results replace :cache yes :exports results
  qr0 = QuantumRegister(1, "rho_in")
  qr1 = QuantumRegister(1, "p|0><0| + (1-p)|1><1|")

  qc = QuantumCircuit(qr0, qr1)

  qc.h(1)
  qc.cx(0, 1)

  qc.draw()
#+end_src

#+name: exercise-8.28-circuit-4
#+begin_src python :results replace :cache yes :exports results
  qr0 = QuantumRegister(1, "rho_in")
  qr1 = QuantumRegister(1, "p|+><+| + (1-p)|-><-|")

  qc = QuantumCircuit(qr0, qr1)

  qc.cx(0, 1)

  qc.draw()
#+end_src

** Exercise 8.29 (Unitality)
A quantum process $\calE$ is unital if $\calE(I)=I$. Show that the depolarizing and phase
damping channels are unital, while amplitude damping is not.

*** Solution
This is a straightforward exercise. So let us mainly mention one thing. Some formulas like
this one for the depolarizing channel

$$
  \calE_{DC}(\rho) = p d\inv I  + (1-p)\rho
$$

only holds for /density/ matrices. In that case we have to plug in $d\inv\?I$ and not $I$:

$$
  \calE_{DC}(d\inv I) = (p + (1-p)) d\inv I = d\inv I .
$$

For the phase damping let us show the claim on the (slightly) more general phase flip
operation ([[#exercise-8.27][exercise 8.27]]). We use the a formula based on operation elements where the
above remark does not apply:

$$
  \calE_{PF}(I) = p I^3 + (1-p) ZIZ = p I + (1-p) I = I .
$$

Finally consider amplitude damping with $E_0=\diag(1,c)$ and $E_1=s\ket{0}\bra{1}$ where
$c,s\geq0$ with $c^2+s^2=1$ (moreover we assume that $c\neq1$ to avoid the identity
operation which is indeed unital):

$$
  \calE_{AD}(I) = E_0E_0^\dagger + E_1E_1^\dagger = \diag(1,c^2) + \diag(s^2, 0) = \diag(1+s^2,c^2) \neq I
$$

** Exercise 8.30 ($T_2\leq2T_1$)
The $T_2$ phase coherence relaxation rate is just the exponential decay rate of the
off-diagonal elements in the qubit density matrix, while $T_1$ is the decay rate of the
diagonal elements (see Equation (7.144)). Amplitude damping has both nonzero $T_1$ and $T_2$
rates; show that for amplitude damping $T_2=2T_1$. Also show that if amplitude and phase
damping are both applied then $T_2\leq2T_1$.

*** Proof
Recall the general equation for the relaxation from (7.144):

$$
  \?\begin{bmatrix}
    (p-p_0) e^{-t/T_1} + p_0 & b e^{-t/T_2} \\
    b^* e^{-t/T_2} & (p_0-p) e^{-t/T_1} + (1-p_0)
  \?\end{bmatrix} .
$$

From [[#exercise-8.22][exercise 8.22]] we know

$$
  \calE_{AD}(\rho) = \?\begin{bmatrix}
    1 + (1-\gamma)(p-1) & b \sqrt{1-\gamma} \\
    b^* \sqrt{1-\gamma} & (1-p) (1-\gamma)
  \?\end{bmatrix} .
$$

This clearly corresponds to $p_0=1$, $T_2=2T_1$ and an unspecified $t>0$. If we apply
phase damping in addition we get

$$
  \calE_{PD}\circ\calE_{AD}(\rho) = \?\begin{bmatrix}
    1 + (1-\gamma)(p-1) & b \sqrt{(1-\lambda)(1-\gamma)} \\
    b^* \sqrt{(1-\lambda)(1-\gamma)} & (1-p) (1-\gamma)
  \?\end{bmatrix} .
$$

This clearly only makes the $T_2$​-relaxation faster. In other words $T_2$ gets
smaller. QED.

** TODO Exercise 8.31 (Exponential sensitivity to phase damping)
Using (8.126), show that the element $\rho_{nm}=\bra{n}\rho\ket{m}$ in the density matrix of a harmonic
oscillator decays exponentially as $e^{-\lambda(n-m)^2}$ under the effect of phase damping, for some
constant $\lambda$.

** Exercise 8.32
Explain how to extend quantum process tomography to the case of non-trace-preserving
quantum operations, such as arise in the study of measurement.

*** Solution
The procedure explained in the book does not rely on the fact that the operation is
trace-preserving. The only occasion it appeared was when it was mentioned that $\chi$ has
$d^4-d^2$ free parameters. In the non-trace-preserving case the $-d^2$ goes away.

** Exercise 8.33 (Specifying a quantum process)
Suppose that one wished to completely specify an arbitrary single qubit operation $\calE$
by describing how a set of points on the Bloch sphere $\{\vec{r}_k\}$ transform under
$\calE$. Prove that the set must contain at least four points.

*** Proof
First of all recall that the bloch sphere description only applies to trace-preserving
operations ([[#exercise-8.16][exercise 8.16]]). Therefore let us add the additional resctriction that the
operation has to be trace preserving.

The claim almost follows from the fact that trace-preserving operations act like affine
maps in the (three-dimensional) Bloch-space. But it is well known that an affine map on a
$d$​-dimensional space needs exactly $d+1$ affine-linearly independent points to be
specified. I say "almost" because not every affine map corresponds to a quantum operation
(at least I think so). But it still serves as good justification because we have already
seen that quantum operations give rise to several non-trivial affine maps.

We show that every map (on the Bloch-space) of the form

<<exercise-8.33-1>>
$$
  \vec{x} \mapsto RD \vec{x} + \vec{c} ,
$$

for any rotation matrix $R$ (orthogonal and determinant plus one), diagonal matrix $D$
with diagonal entries in $0\?<\?d_i\?<\?1-\gamma$, and any vector $\vec{c}$ with
$\norm{\vec{c}}\leq\gamma$ corresponds to a quantum operation. Here $0\leq\gamma\?<\?1$ is
an arbitrary parameter. There are more affine maps possible but this is sufficient for our
purpose.

Note that e.g. mirroring at the $z=0$ plane (a negative determinant map) and the mentioned
affine maps together generate the group of all invertible affine maps (this is related to
the polar decomposition of matrices). Hence it is not hard to see that even if we restrict
the space of possible maps to maps of the [[exercise-8.33-1][above form]] we need at least four points.

Now let us justify the above [[exercise-8.33-1][claim]]. Any rotation $\vec{x}\mapsto\?R\vec{x}$ is possible by
unitary evolution (keyword: Pauli rotations). The depolarizing channel implements
$\vec{x}\mapsto\alpha\vec{x}$ for any $\alpha\in(0,1)$. The phase flip can implement an
operation which shrinks the x- and y-dimension by the same factor. In the same way a
similar shrinkage of any pair of axes is possible. Hence a mapping
$\vec{x}\mapsto\?D\vec{x}$ is possible where the diagonal entries just have to satisfy
$0\?<\?d_i\?<\?1$.

Finally amplitude damping together with a rotation can be used to realize a non-trivial
$\vec{c}$. Note that amplitude damping also shrinks everything, but not more than by a factor
$1-\gamma$. This is the reason why we impose the condition $0\?<\?d_i\?<\?1-\gamma$ on the
realizability of the diagonal entries. QED.

** TODO Exercise 8.34 (Process tomography for two qubits)
Show that the $\chi_2$ describing the black box operations on two qubits can be expressed
as

$$
  \chi_2 = \Lambda_2 \sigma \Lambda_2 ,
$$

where $\Lambda_2=\Lambda\otimes\Lambda$, $\Lambda$ is as deﬁned in Box 8.5, and $\sigma$ is a
block matrix of 16 measured density matrices,

$$
  \sigma = P^\top \left( \sum_{mn} \rho_{mn}\otimes\rho'_{mn} \right)  P
$$

where $\rho'_{nm}=\calE(\rho_{nm})$, $\rho_{nm}=T_n\proj{00}T_m$, $T_1=I\otimes\?I$,
$T_2=I\otimes\?X$, $T_3=X\otimes\?I$, $T_4=X\otimes\?X$, and
$P=I\otimes[(\rho_{00}+\rho_{12}+\rho_{21}+\rho_{33})\otimes\?I]$ is a permutation matrix.

- Remark :: The formula for sigma is equivalent to the one in the book. I find it to be
  more concise. It follows from the fact that the $\rho_{mn}$ are matrices with zeros
  every except at the position $(m,n)$.

** Exercise 8.35 (Process tomography example)
Consider a one qubit black box of unknown dynamics $\calE_1$. Suppose that the following
four density matrices are obtained from experimental measurements, performed according to
Equations (8.173)–(8.176):

\begin{align*}
  \rho_1' &= \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} , \\
  \rho_2' &= \begin{bmatrix} 0 & \sqrt{1-\gamma} \\ 0 & 0 \end{bmatrix} , \\
  \rho_3' &= \begin{bmatrix} 0 & 0 \\ \sqrt{1-\gamma} & 0 \end{bmatrix} , \\
  \rho_4' &= \begin{bmatrix} \gamma & 0 \\ 0 & 1-\gamma \end{bmatrix} .
\end{align*}

where $\gamma$ is a numerical parameter. From an independent study of each of these
input–output relations, one could make several important observations: the ground state
$\ket{0}$ is left invariant by $\calE_1$, the excited state $\ket{1}$ partially decays to
the ground state, and superposition states are damped. Determine the $\chi$ matrix for
this process.

- Remark :: I wonder why we should stop at computing the chi matrix. Anyway, let us extend
  the exercise by the requirement to also compute operation elements for this operation.

*** Solution 1
Let us first set up two routines to compute $\beta$ and $\lambda$. We specialize those to
the single qubit case. Moreover we use the basis $(\rho_j)=(P_0, P_0X, XP_0,XP_0X)$ (where
$P_0=\proj{0}$) for the space where the density matrices live.

#+name: exercise-8.35-1
#+begin_src sage
  def compute_beta_835(Es: list[matrix]):
      """Compute beta for single qubit and specific choice of basis of input matrix space."""
      rho = [P0, P0*X, X*P0, X*P0*X] # basis

      # As suggested in the text we flatten the space of 4x4 matrices to a 16-dimensional
      # vector. This turns beta into a 16x16 matrix.
      beta = matrix.zero(16)

      for m, n in product(range(4), range(4)):
          for j, k in product(range(4), range(4)):
              rho_prime = Es[m] * rho[j] * Es[n].H
              beta[j+4*k, m+4*n] = trace(rho[k].H * rho_prime)

      return beta


  def compute_lambda_835(rho_prime):
      """Compute lambda for single qubit and specific choice of basis of input matrix
      space."""
      rho = [P0, P0*X, X*P0, X*P0*X] # basis

      lam = [None for _ in range(16)]

      for j in range(4):
          for k in range(4):
              lam[j+4*k] = trace(rho[k].H * rho_prime[j])

      return vector(lam)


  def toMatrix_835(vec):
      """Convert a flattened 4x4 matrix back to matrix representation."""
      return matrix(SR, 4, 4, lambda m,n: vec[m+4*n])
#+end_src

The following code computes $\chi$. As a basis for the operation elements we use the
$\tilde{E}_j$ as in box 8.5: $(I,X,-\ii\?Y,Z)$.

#+name: exercise-8.35-2
#+begin_src sage
  rho_prime_835 = [
      matrix([
          [1, 0],
          [0, 0],
      ]),
      matrix([
          [0, sqrt(1-g)],
          [0, 0],
      ]),
      matrix([
          [0, 0],
          [sqrt(1-g), 0],
      ]),
      matrix([
          [g, 0],
          [0, 1-g],
      ]),
  ]

  # E-tilde as basis for beta as in box 8.5:
  beta_835 = compute_beta_835([Id, X, -i*Y, Z])
  kappa_835 = beta_835.pseudoinverse()
  assert beta_835 == beta_835 * kappa_835 * beta_835, "kappa is pseudoinverse of beta"

  lam_835 = compute_lambda_835(rho_prime_835)
  chi_835 = toMatrix_835(kappa_835 * lam_835)
#+end_src

Let us print $\chi$:

#+begin_src sage :tangle no :results replace :cache yes
  chi_835
#+end_src

#+RESULTS[fd8824bc31ed58b7e4dfd66eb6c33ded52985dd4]:
: [-1/4*g + 1/2*sqrt(-g + 1) + 1/2                               0                               0                           1/4*g]
: [                              0                           1/4*g                          -1/4*g                               0]
: [                              0                          -1/4*g                           1/4*g                               0]
: [                          1/4*g                               0                               0 -1/4*g - 1/2*sqrt(-g + 1) + 1/2]

To obtain the operation elements we have to diagonalize $\chi$.

#+begin_src sage :tangle no :results replace :cache yes
  D, U = chi_835.eigenmatrix_right()
  print(f"D (diagonalized chi matrix) = \n{D}")
  print(f"\nU (columns contain eigenvectors) = \n{U}")
#+end_src

#+RESULTS[104dde2e610ca9d7d5e9c9d8f4e01763a731fb2e]:
#+begin_example
D (diagonalized chi matrix) =
[-1/2*g + 1          0          0          0]
[         0          0          0          0]
[         0          0          0          0]
[         0          0          0      1/2*g]

U (columns contain eigenvectors) =
[                          1                           1                           0                           0]
[                          0                           0                           1                           1]
[                          0                           0                           1                          -1]
[-(g + 2*sqrt(-g + 1) - 2)/g  (g - 2*sqrt(-g + 1) - 2)/g                           0                           0]
#+end_example

There are only two non-zero eigenvalues of the chi-matrix:

$$
  d_0 = 1 - \frac{g}{2}, \quad d_3 = \frac{g}{4} .
$$

The corresponding normalized eigenvectors are

$$
  u_0 = (1, 0, 0, r)/\sqrt{1+r^2}, \quad u_3 = (0, 1, -1, 0)/\sqrt{2} ,
$$

where $r=g\inv(1-\sqrt{1-g})^2$. This implies the following operation elements:

\begin{align*}
  E_0 &= \sqrt{\frac{1-\frac{g}{2}}{1+r^2}} \, (I + rZ) , \\
  E_3 &= \frac{\sqrt{g}}{2} \, (X + \ii Y) .
\end{align*}

#+name: exercise-8.35-3
#+begin_src sage
  r = (1/g)*(1-sqrt(1-g))^2
  op835 = make_operation([sqrt(1-g/2)*(Id+r*Z)/sqrt(1+r^2), sqrt(g)*(X+i*Y)/2])
#+end_src

Let us briefly verify that this operation indeed implements the desired operation:

#+begin_src sage :results replace :tangle no :cache yes
  rho = [P0, P0*X, X*P0, X*P0*X]
  assert op835(rho[0]) == rho_prime_835[0]
  assert op835(rho[1]) == rho_prime_835[1]
  assert op835(rho[2]) == rho_prime_835[2]
  assert op835(rho[3]) == rho_prime_835[3]
  "PASSED"
#+end_src

#+RESULTS[41ca0b83c6ea08f04cd21c39e299f2ad2e585945]:
: 'PASSED'

*** Solution 2
In box 8.5 another way to compute the chi-matrix was suggested. It is based on a matrix
$\Lambda$:

#+name: exercise-8.35-4
#+begin_src sage
  Lambda1 = (kron(Z, Id) + kron(X, X)) / 2
#+end_src

In addition we need the following utility functions. The first one computes the matrix

$$
  R' := \begin{bmatrix} \rho_1' & \rho_2' \\ \rho_3' & \rho_4' \end{bmatrix} .
$$

It utilizes the fact that this matrix is just $\sum_j\rho_j\otimes\rho_j'$ for our
specific choice of $(\rho_j)$. The second one just implements the formula (see (8.179))

$$
  \chi = \Lambda R' \Lambda .
$$

#+name: exercise-8.35-5
#+begin_src sage
  def compute_rho_prime_matrix_835(rho_primes):
      rhos = [P0, P0*X, X*P0, X*P0*X]
      lam = matrix.zero(4)

      for rho, rho_prime in zip(rhos, rho_primes):
          lam += kron(rho, rho_prime)

      return lam


  def compute_chi_via_Lambda_835(rho_primes):
      return Lambda1 * compute_rho_prime_matrix_835(rho_primes) * Lambda1
#+end_src

So let us compute $\chi$:

#+name: exercise-8.35-6
#+begin_src sage
  # Take rho_prime_835 from solution 1:
  chi_835_2 = compute_chi_via_Lambda_835(rho_prime_835)
#+end_src

Let us do a quick check that we got the same thing as in the first solution. Note that
this is only possible because we used the same bases for the operation elements
($(I,X,-\ii\?Y,Z)$) and the matrix space where the density matrices live
($(P_0,P_0X,XP_0,XP_0X)$). Otherwise the number might be completely different of course.

#+begin_src sage :results replace
  assert chi_835 == chi_835_2
  "PASSED"
#+end_src

#+RESULTS:
: 'PASSED'

The rest is the same as in the first solution.
