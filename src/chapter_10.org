#+title:  Chapter 10
#+author: Reinhard Stahn
#+setupfile: ./inc/setupfile.org
#+include: ./inc/latex-macros.org
#+property: header-args:sage :session *sage-chapter-10* :tangle chapter_10.sage
# python session just for qiskit since sage integers dont work well with qiskit:
#+property: header-args:python :session *python-chapter-10* :tangle no

#+toc: headlines 2

* Setup
** Imports
#+name: chapter-10-imports
#+begin_src sage
  from utils_sage import ket
  from chapter_8_sage import make_operation
#+end_src

# Hidden imports
#+name: chapter-10-hidden-imports
#+begin_src python :exports none
  from qiskit.circuit import QuantumCircuit, QuantumRegister, Parameter as Param
#+end_src

** Utility code
#+name: chapter-10-global-vars
#+begin_src sage
  g = SR.var("g", domain="positive")
  a, b = SR.var("a b", domain="complex")
  assume(g <= 1)
#+end_src

** Notation
Below we use $\BB=\mathrm{GF}(2)=\ZZ_2$ to denote the field of two elements. This models a
/bit/.

* Exercises
** Exercise 10.1
Verify that the encoding circuit in Figure 10.2 works as claimed.

#+RESULTS[b2469d3d1659de6df9ffe41f5443d686897376b9]: exercise-10.1-circuit
:
: |psi>: ──■────■──
:        ┌─┴─┐  │
: |0>_0: ┤ X ├──┼──
:        └───┘┌─┴─┐
: |0>_1: ─────┤ X ├
:             └───┘


#+name: exercise-10.1-circuit
#+begin_src python :results replace :cache yes :exports results
  qr0 = QuantumRegister(1, "|psi>")
  qr1 = QuantumRegister(2, "|0>")

  qc = QuantumCircuit(qr0, qr1)
  qc.cx(0, 1)
  qc.cx(0, 2)

  qc.draw()
#+end_src

*** Solution
This is really just observing that the circuit acts like this on the /relevant/ basis
states

\begin{align*}
  \ket{000} &\mapsto \ket{000} , \\
  \ket{100} &\mapsto \ket{111} . \\
\end{align*}

The rest follows by linearity.

** Exercise 10.2
The action of the bit flip channel can be described by the quantum operation
$\calE(\rho)=(1-p)\rho+pX\rho\?X$. Show that this may be given an alternate operator-sum
representation, as $\calE(\rho)=(1-2p)\rho+2pP_+\rho\?P_++2pP_-\rho\?P_-$ where $P_+$ and
$P_-$ are projectors onto the $+1$ and $-1$ eigenstates of $X$,
$(\ket{0}+\ket{1})/\sqrt{2}$ and $(\ket{0}+\ket{1})/\sqrt{2}$ respectively.  This latter
representation can be understood as a model in which the qubit is left alone with
probability $1-2p$, and is ‘measured’ by the environment in the $\ket{+}$, $\ket{-}$ basis
with probability $2p$.

*** Proof
Note that $X=P_+-P_-$ and $I=P_++P_-$. Hence

$$
  X\rho X = P_+\rho P_+ + P_-\rho P_- - (P_+\rho P_- + P_-\rho P_+)
  = 2(P_+\rho P_+ + P_-\rho P_-) - I\rho I .
$$

Hence

$$
  \calE(\rho) = (1 - 2p)\rho + 2p (P_+\rho P_+ + P_-\rho P_-) .
$$

QED.

** Exercise 10.3
Show by explicit calculation that measuring $Z_1Z_2$ followed by $Z_2Z_3$ is equivalent,
up to labeling of the measurement outcomes, to measuring the four projectors defined by
(10.5)–(10.8), in the sense that both procedures result in the same measurement statistics
and post-measurement states.

*** Proof
Let us define the following projectors

\begin{align*}
  P_{ijk} &= \proj{i}\otimes\proj{j}\otimes\proj{k} , \\
  P_{ijx} &= \proj{i}\otimes\proj{j}\otimes I , \\
  P_{xij} &= I\otimes \proj{i}\otimes\proj{j} .
\end{align*}

That is, the $x$ has a special meaning and just stands for the place where the identity
acts. Define $P_{ixx}$ etc analogously. Note that e.g. $P_{ixx}P_{xjx}=P_{ijx}$. Since
$Z_1=P_{0xx}-P_{1xx}$, $Z_2=P_{x0x}-P_{x1x}$, $Z_3=P_{xx0}-P_{xx1}$ we have the following
spectral decomposition of the observables we are interested in:

\begin{align*}
  Z_1Z_2 &= P_{00x} + P_{11x} - P_{01x} - P_{10x} , \\
  Z_2Z_3 &= P_{x00} + P_{x11} - P_{x01} - P_{x10} .
\end{align*}

Let $P_j$ for $j=0..3$ be the projectors from (10.5) to (10.8). If we measure for example
$+1$ for $Z_1Z_2$ and $+1$ for $Z_2Z_3$ this corresponds to the projection (note: the
order of the projections is not important due to commutativity)

$$
  (+1, +1):\; (P_{00x} + P_{11x})(P_{x00} + P_{x11}) = P_{000} + P_{111} = P_0 .
$$

In the same way the other three measurements can be mapped to the $P_j$ too:

\begin{align*}
  &(+1, -1):\; (P_{00x} + P_{11x})(P_{x01} + P_{x10}) = P_{001} + P_{110} = P_3 , \\
  &(-1, +1):\; (P_{01x} + P_{10x})(P_{x00} + P_{x11}) = P_{100} + P_{011} = P_1 , \\
  &(-1, -1):\; (P_{01x} + P_{10x})(P_{x01} + P_{x10}) = P_{101} + P_{010} = P_2 .
\end{align*}

Hence the projectors are the same, just re-labeled. QED.

** Exercise 10.4
Consider the three qubit bit flip code. Suppose we had performed the error syndrome
measurement by measuring the eight orthogonal projectors corresponding to projections onto
the eight computational basis states.

1. Write out the projectors corresponding to this measurement, and explain how the
   measurement result can be used to diagnose the error syndrome: either no bits flipped or
   bit number $j$ flipped, where $j$ is in the range one to three.
2. Show that the recovery procedure works only for computational basis states.
3. What is the minimum fidelity for the error-correction procedure?

*** Solution for part 1
The projectors are just $P_j=\proj{j}$ for $j=0..7$ (note that the binary representations
for the numbers from $0$ to $7$ are $000$, $001$, $010$, $011$, $100$, $101$, $110$,
$111$). A corresponding observable distinguishing each projector would be
$A=\sum_jj\proj{j}$.

Recall that the encoded state is $\ket{\psi}=a\ket{000}+b\ket{111}$. Let $k=0$, $k=1$,
$k=2$, $k=3$ denote /no error/, /error in qubit 1, 2, 3/, respectively. Let $M_k$ be the
set of possible measurements for the given value of $k$. We have

<<exercise-10.4-1>>
\begin{align*}
  M_0 &= \{0, 7\} , \\
  M_1 &= \{1, 6\} , \\
  M_2 &= \{2, 5\} , \\
  M_3 &= \{4, 3\} .
\end{align*}

Moreover, measuring $0,1,2,4$ happens with probability $\abs{a}^2$ and measuring $7,6,5,3$
happens with probability $\abs{b}^2$ (conditioned on a fixed value for $j$). Since the
four sets are disjoint we can infer the error (the value of $k$) from the measurement
(e.g. measuring $1$ or $6$ implies that the error was $k=1$ (with certainty if only one of
the three errors "is allowed" to happen)).

*** Solution for part 2
If we measure $j$ and infer the error $k$ [[exercise-10.4-1][according to the sets]] $M_k$ we would correct it
by applying $X_k$ to the state. In other words, the following quantum operation $\calR$
performs the syndrome measurement followed by the recovery:

\begin{align*}
  E_0 &= P_0, \\ E_1 &= X_1P_1, \\ E_2 &= X_2P_2, \\ E_3 &= X_3P_3, \\
  E_4 &= X_3P_4, \\ E_5 &= X_2P_5, \\ E_6 &= X_1P_6, \\ E_7 &= P_7 .
\end{align*}

Note that $\calR$ is indeed trace-preserving ($\sum_jE_j^\dagger\?E_j)=I$). Recall
$\ket{\psi}=a\ket{000}+b\ket{111}$. Then (the /no error/ case)

$$
  \calR(\ket{\psi}) = \abs{a}^2 \proj{000} + \abs{b}^2 \proj{111} .
$$

The same formula holds for $\ket{\psi}$ replaced by $X_k\ket{\psi}$ ($k=1..3$). Hence, for
the following noise channel

$$
  \calE(\rho) = (1-p)\rho + \sum_{k=1}^3 p_k X_k \rho X_k
$$

where $\sum_kp_k=p$ (this implies trace-preservation, but we could more generally assume
$\ldots\leq\?p$, meaning that with certain probability "something else" happens), which
stands for having at most one bit-flip (with probability $p_k$ at position $k$), we have

$$
  \calR\circ\calE(\ket{\psi}) = \abs{a}^2 \proj{000} + \abs{b}^2 \proj{111} .
$$

(If $\calE$ was not trace-preserving the RHS would have a factor $1-p+\sum_jp_j$.) Hence
the bit-flip error gets indeed corrected, but unwanted side effects happen too. The final
state after recovery is either $\ket{000}$ (probability $\abs{a}^2$) or $\ket{111}$
(probability $\abs{b}^2$). That is the state collapses to a basis state. This is only
correct if $\ket{\psi}$ was already in a basis state ($a=0$ or $b=0$) - which is the claim
of part 2.

*** Solution for part 3
Let us consider two cases:

1. We do not get to know the results of the syndrome measurement.
2. We get to know the results of the syndrome measurement.

In case 1 the whole error correction procedure is given by $\calR$ from the solution of
part 2. Let $\rho=\calR\circ\calE(\ket{\psi})$. Then

$$
  F(\ket{\psi},\rho) = \sqrt{\bra{\psi}\rho\ket{\psi}} = \sqrt{\abs{a}^4 + \abs{b}^4} .
$$

Since $\abs{a}^2+\abs{b}^2=1$ the minimum occurs at $\abs{a}^2=\abs{b}^2=1/2$. Hence
$F\geq1/\sqrt{2}$ in that case.

In case 2 the post-correction state $\rho$ is either $\ket{000}$ (probability $\abs{a}^2$)
or $\ket{111}$ (probability $\abs{b}^2$). Hence

$$
  F(\ket{\psi},\rho) \in \left\{\sqrt{\braket{\psi}{i_L}\braket{i_L}{\psi}} \; \middle| \; i\in\{0,1\}\right\}
  = \{\abs{a}, \abs{b}\} ,
$$

where the first alternative occurs with probability $\abs{a}^2$ and the second one with
$\abs{b}^2$.

In both cases the minimial fidelity does not depend on the specifics of the noise model
and in particular is always bad if $\abs{a}$ and $\abs{b}$ are bounded away from $0$ (or
equivalently, from $1$).

- Remark :: The minimum fidelity of case 1 is the weighted root square mean of the two
  possible fidelities of case 2. The weights are $\abs{a}^2$ and $\abs{b}^2$. See e.g.
  [[https://en.wikipedia.org/wiki/Generalized_mean][generalized mean]] on wikipedia for more infos. To make this more apparent consider
  $\sigma=p\proj{0}+(1-p)\proj{1}$ (and $\ket{\psi}=a\ket{0}+b\ket{1}$):

  $$
  F(\ket{\psi},\sigma) = \sqrt{\bra{\psi}\sigma\ket{\psi}} = \sqrt{p\abs{a}^2+(1-p)\abs{b}^2} .
  $$

  Here $p$ and $1-p$ are the weights.

** Exercise 10.5
Show that the syndrome measurement for detecting phase flip errors in the Shor code
corresponds to measuring the observables $A=X_1X_2X_3X_4X_5X_6$ and
$B=X_4X_5X_6X_7X_8X_9$.

*** Proof
:PROPERTIES:
:CUSTOM_ID: exercise-10.5-solution
:END:
Let us denote

\begin{align*}
  \ket{\pi} &= \frac{1}{\sqrt{2}} (\ket{000} + \ket{111}) , \\
  \ket{\mu} &= \frac{1}{\sqrt{2}} (\ket{000} - \ket{111}) .
\end{align*}

With this notation the Shor code is given by

\begin{align*}
  \ket{0_L} &= \ket{\pi\pi\pi} , \\
  \ket{1_L} &= \ket{\mu\mu\mu} , \\
\end{align*}

Hence we have the following encoding:

$$
  \ket{\psi} = \ket{\psi_0} = a\ket{0_L} + b\ket{1_L} = a\ket{\pi\pi\pi} + b\ket{\mu\mu\mu} .
$$

In this notation a phase flip error is represented naturally. For example a phase flip in
one of the three qubits in the first block of $\ket{\psi}$ is given by

$$
  \ket{\psi_1} = a\ket{\mu\pi\pi} + b\ket{\pi\mu\mu} .
$$

It doesn't matter in which of the three qubits, it always leads to the same state. The
other two errors (phase flip in the second or third block) are given by

\begin{align*}
  \ket{\psi_2} &= a\ket{\pi\mu\pi} + b\ket{\mu\pi\mu} , \\
  \ket{\psi_3} &= a\ket{\pi\pi\mu} + b\ket{\mu\mu\pi} .
\end{align*}

Now observe that

\begin{align*}
  X_1X_2X_3 \ket{\pi} &= + \ket{\pi} , \\
  X_1X_2X_3 \ket{\mu} &= - \ket{\mu} .
\end{align*}

Hence e.g. measuring a state $\ket{ijk}$ ($i,j,k\in\{\pi,\mu\}$) with respect to $A$
detects if $i=j$ (measurement result: $+1$) or $i\neq\?j$ (measurement result:
$+1$). Because of their special structure this translates to $\ket{\psi_k}$. For the four
errors (first one is /no error/) we get the following measurement results (each with
certainty):

| $k$ ($\ket{\psi_k}$) | $A$ | $B$ |
|----------------------+-----+-----|
|                    0 |  +1 |  +1 |
|                    1 |  -1 |  +1 |
|                    2 |  -1 |  -1 |
|                    3 |  +1 |  -1 |

Hence we can infer the syndrome $k$ (which error occured) from the measurement - as
desired. QED.

** Exercise 10.6
Show that recovery from a phase flip on any of the first three qubits may be accomplished
by applying the operator $Z_1Z_2Z_3$.

*** Proof
Let us reuse the notation from the [[#exercise-10.5-solution][solution]] of exercise 10.5. Clearly

$$
  Z_1Z_2Z_3 \ket{\pi} = \ket{\mu} \text{ and } Z_1Z_2Z_3 \ket{\mu} = \ket{\pi} .
$$

An error in the first three qubits means that the corresponding state is
$\ket{\psi_1}$. By the above we have:

$$
  Z_1Z_2Z_3 \ket{\psi_1} = \ket{\psi} .
$$

QED.

** Exercise 10.7
Consider the three qubit bit flip code of Section 10.1.1, with corresponding projector
$P=\proj{000}+\proj{111}$. The noise process this code against has operation elements

$$
  \left\{ \sqrt{(1-p)^3}I, \sqrt{p(1-p)^2}X_1, \sqrt{p(1-p)^2}X_2, \sqrt{p(1-p)^2}X_3 \right\} ,
$$

where $p$ is the probability that a bit flips. Note that this quantum operation $\calE$ is
not trace-preserving, since we have omitted operation elements corresponding to bit flips
on two and three qubits. Verify the quantum error-correction conditions for this code and
noise process.

*** Solution
Clearly $PX_iX_jP=\delta_{ij}P$. Hence

\begin{align*}
  P E_0^\dagger E_0 P &= (1-p)^3 P , \\
  P E_i^\dagger E_i P &= p(1-p)^2 P \text{ for } i=1..3 , \\
  P E_i^\dagger E_j P &= 0 \text{ for } i\neq j .
\end{align*}

Hence the conditions are satisfied and there exists an error correction procedure $\calR$
for this noise.

*** Bonus: Calculating the recovery procedure
By the previous calculation and Theorem 10.1 there exists a trace-preserving quantum
operation $\calR$ such that

$$
  \calR \circ \calE (P\rho P) = [(1-p)^3 + 3p(1-p)^2] \, P \rho P .
$$

The concrete form of the proportionality factor follows from (10.25) as it equals
$\sum_kd_{kk}$. Since this is a trace this formula would even be true if $(d_{ij})$ wasn't
already diagonal.

To compute the Kraus matrices $R_k$ of $\calR$ we need the $E_k$ such that the $(d_{ij})$
are diagonal. This is already the case

$$
  d = \diag((1-p)^3, p(1-p)^2, p(1-p)^2, p(1-p)^2) .
$$

From the proof of Theorem 10.1 we know that

$$
  R_k = U_k^\dagger P_k
$$

where $P_k=U_kPU_k^\dagger$ and $E_kP=\sqrt{d_{kk}}U_kP$ is a polar decomposition. The
$P_k$ are the projectors onto $E_kC$ ($C$ being the code space onto which $P$
projects). The diagonality of $(d_{ij})$ implies that $P_iP_j=\delta_{ij}P_i$ (meaning
that the error subspaces are orthogonal to each other).

Clearly we can set $U_0=I$ (it is only uniquely defined on the image of $P$). For $k=1$ we
have

$$
  E_1 P = \sqrt{p(1-p)^2} X_1 P .
$$

This is already a polar decomposition (the Pauli operators are unitary). Hence we can just
set $U_1=X_1$ and analogously $U_k=X_k$ for $k=1..3$. Hence

\begin{align*}
  P_0 &= \proj{000} + \proj{111} , \\
  P_1 &= \proj{100} + \proj{011} , \\
  P_2 &= \proj{010} + \proj{101} , \\
  P_3 &= \proj{001} + \proj{110} .
\end{align*}

Note that this corresponds to the already in (10.5-10.8) introduced syndrome measurement
operators. The Kraus matrices of $\calR$ are

$$
  R_k = U_k^\dagger P_k = \begin{cases} IP = P & \text{for } k = 0 , \\
    X_k X_kPX_k = PX_k & \text{for } k \in \{1,2,3\} . \end{cases}
$$

** Exercise 10.8
Verify that the three qubit phase flip code $\ket{0_L}=\ket{+++}$, $\ket{1_L}=\ket{---}$
satisfies the quantum error-correction conditions for the set of error operators
$\{I,Z_1,Z_2,Z_3\}$.

*** Solution 1
Let us reduce this to the bit flip code. To convert to the bit flip code we only have to
conjugate the projector of the code and the Kraus operators of the errors by
$H^{\otimes3}$ (Hadamard).

$$
  \proj{000} + \proj{111} = H^{\otimes3} \proj{+++} + \proj{---} H^{\otimes3}
$$

and

$$
  X_k = H^{\otimes3} Z_k H^{\otimes3} .
$$

Let $P$ be the projector of the phase flip code and let $E_i$ be the mentioned errors it
corrects. Let $Q$ and $F_j$ be the same thing for the bit flip.

$$
  P E_i^\dagger E_j P = H^{\otimes3} Q F_i^\dagger F_j Q H^{\otimes3}
  = H^{\otimes3} \delta_{ij} Q H^{\otimes3}
  = \delta_{ij} P .
$$

In the second equality we used what we already know about the bit flip code.

*** Solution 2
We can also verify this explicitly. Let $P=P_{+++}+P_{---}$ be the projector onto the
code, where we use the notation $P_{ijk}=\proj{ijk}$. Clearly
$PE_i^\dagger\?E_iP=PIP=P$. Moreover

$$
  PZ_1P = (P_{+++} + P_{---})(P_{-++} + P_{+--}) = 0
$$

and

$$
  PZ_1Z_2P = (P_{+++} + P_{---})(P_{--+} + P_{++-}) = 0 .
$$

Similarly $PIZ_jP=PZ_iZ_jP=0$ for $i\neq\?j$. Hence $PE_i^\dagger\?E_iP=\delta_{ij}$.

** Exercise 10.9
Again, consider the three qubit phase flip code. Let $P_i$ and $Q_i$ be the projectors
onto the $\ket{0}$ and $\ket{1}$ states, respectively, of the $i$​th qubit. Prove that the
three qubit phase flip code protects against the error set ${I,P_1,Q_1,P_2,Q_2,P_3,Q_3}$.

*** Solution
Recall that the phase flip code corrects the errors $I$, $Z_1$, $Z_2$, and $Z_3$. By
theorem 10.2 linear combinations of these errors are corrected too. Note that we have

\begin{align*}
  P_k &= \frac{1}{2} (I + Z_k) , \\
  Q_k &= \frac{1}{2} (I - Z_k) .
\end{align*}

Hence those errors are corrected too.

** Exercise 10.10
Explicitly verify the quantum error-correction conditions for the Shor code, for the error
set containing $I$ and the error operators $X_j,Y_j,Z_j$ for $j=1$ through $9$.

*** Solution
Let us denote

\begin{align*}
  \ket{\pi} &= \frac{1}{\sqrt{2}} (\ket{000} + \ket{111}) , \\
  \ket{\mu} &= \frac{1}{\sqrt{2}} (\ket{000} - \ket{111}) .
\end{align*}

The Shor code is given by he projector

$$
  P = \proj{\pi\pi\pi} + \proj{\mu\mu\mu} .
$$

It is not hard to see that the errors transform the code space into a set of mutually
orthogonal subspaces - with one exception. The effect of $Z_j$ is the same if $j$ stays
within one "block" (there are three block $\{1,2,3\}$, $\{4,5,6\}$, and
$\{7,8,9\}$). Hence:

$$
  PE_i^\dagger E_j P = \begin{cases}
    1 & \text{if } i=j \text{ or } E_i,E_j \text{ both Z on same block} , \\
    0 & \text{else} .
  \end{cases}
$$

** Exercise 10.11
Construct operation elements for a single qubit quantum operation $\calE$ that upon input
of any state $\rho$ replaces it with the completely randomized state $I/2$. It is amazing
that even such noise models as this may be corrected by codes such as the Shor code!

*** Solution
We already know that the quantum operation

$$
  \calE(\rho) = \frac{1}{2} I
$$

is the depolarizing channel with $p=1$. By (8.102) this is equal to

$$
  \calE(\rho) = \frac{1}{4} (I\rho I + X\rho X + Y\rho Y + Z\rho Z) .
$$

Hence the relevant operation elements are

$$
  \left\{\frac{1}{2}I, \frac{1}{2}X, \frac{1}{2}Y, \frac{1}{2}Z \right\} .
$$

** Exercise 10.12
Show that the fidelity between the state $\ket{0}$ and $\calE(\proj{0})$ is $\sqrt{1-2p/3}$
and use this to argue that the minimum fidelity for the depolarizing channel is
$\sqrt{1-2p/3}$.

*** Solution 1
Let us follow the hint of the book.

$$
  F(\ket{0}, \calE(\proj{0}))
  = \sqrt{(1-p) + \frac{p}{3}(\bra{0}X\ket{0}^2 + \bra{0}Y\ket{0}^2 + \bra{0}Z\ket{0}^2)}
  = \sqrt{(1-p) + \frac{p}{3}}
  = \sqrt{1-\frac{2p}{3}} .
$$

Let $\ket{\psi}$ be an arbitrary state and $U$ be a unitary operator such that
$\ket{\psi}=U\ket{0}$. Then

$$
  F(\ket{\psi}, \calE(\proj{\psi})) = F(U\ket{0}, U\calE(\proj{0})U^\dagger)
  = F(\ket{0}, \calE(\proj{0})) = \sqrt{1-\frac{2p}{3}} .
$$

In the second equality we used that the fidelity is invariant under unitary transformations.

*** Solution 2
We could also prove the claim by using an alternative formula for the depolarizing channel:

$$
  \calE(\rho) = \frac{2p}{3} I + \left(1 - \frac{4p}{3}\right) \rho .
$$

(compare equations (8.100) and (8.102).) Hence

$$
  F(\ket{\psi}, \calE(\proj{\psi})) = \sqrt{\frac{2p}{3} + \left(1 - \frac{4p}{3}\right)}
  = \sqrt{1-\frac{2p}{3}} .
$$

*** Solution 3
Yet another approach first calculates

$$
  F(\ket{\psi}, \calE(\proj{\psi}))
  = \sqrt{(1-p) + \frac{p}{3}(\bra{\psi}X\ket{\psi}^2 + \bra{\psi}Y\ket{\psi}^2 + \bra{\psi}Z\ket{\psi}^2)} .
$$

Recall the representation $\proj{\psi}=2\inv(I+\vec{r}\cdot\vec{\sigma})$ of a qubit in
the bloch sphere ($\abs{\vec{r}}=1$). One can show that

$$
  \bra{\psi}Z\ket{\psi} = r_z
$$

and analogous formulas for $r_x$ and $r_y$. Hence

$$
  F(\ket{\psi}, \calE(\proj{\psi})) = \sqrt{(1-p) + \frac{p}{3}\abs{\vec{r}}^2} = \sqrt{1-\frac{2p}{3}} .
$$

- Remark :: To see e.g.

  $$
  \bra{\psi}Z\ket{\psi} = r_z
  $$

  observe that

  $$
  \bra{\psi}Z\ket{\psi} = \trace{\proj{\psi}Z} = r_z .
  $$

  In the last equality we used that the Pauli matrices (including $I$) are an orthogonal
  set with respect to the Hilbert-Schmidt scalar product (with norm $\sqrt{2}$ for each
  base vector). The other two formulas for $r_x$ and $r_y$ follow along the same lines.

  If you want to see the last equality directly, observe that (using $S=\sqrt{Z}$)

  $$
  \trace{\proj{\psi}Z} = \trace{S\proj{\psi}S}
  $$

  and use $SIS=Z$, $SXS=\ii\?X$, $SYS=\ii\?Y$, $SZS=I$ (only the last term has a non-zero
  trace).

** Exercise 10.13
Show that the minimum fidelity $F(\ket{\psi},\calE(\proj{\psi}))$ when $\calE$ is the
amplitude damping channel with parameter $\gamma$, is $\sqrt{1-\gamma}$.

*** Proof
Let us use sage to get an algebraic expression for $F$. First define amplitude damping

#+begin_src sage
  E0 = matrix.diagonal([1, sqrt(1-g)])
  E1 = matrix([[0, sqrt(g)], [0, 0]])
  AD = make_operation([E0, E1])
#+end_src

Now we can use this to compute $F$:

#+begin_src sage :tangle no :results replace :cache yes
  psi = a*ket('0') + b*ket('1')
  psi_mat = matrix(psi).T
  rho = AD(psi_mat*psi_mat.H).simplify_full()
  (psi.conjugate() * rho * psi).simplify_full()
#+end_src

#+RESULTS[a966bcbc6402d10431f26efd81c21e2bb71234f7]:
: 2*a*b*sqrt(-g + 1)*conjugate(a)*conjugate(b) + a^2*conjugate(a)^2 + b^2*conjugate(b)^2 + (a*b*conjugate(a)*conjugate(b) - b^2*conjugate(b)^2)*g

This looks a bit ugly so let us rewrite this:

$$
  F = \abs{a}^4 + \abs{b}^4 + \gamma(\abs{a}^2 \abs{b}^2 - \abs{b}^4) + 2\sqrt{1-\gamma}\, \abs{a}^2\abs{b}^2 .
$$

Substituting $x=\abs{a}^2$ and $y=\abs{b}^2$ this can be written as:

$$
  F = \sqrt{f(x,y)} = \sqrt{x^2 + y^2 + \gamma (xy - y^2) + 2\sqrt{1-\gamma} \, xy} ,
$$

under the constraint $x+y=1$ and $x,y\geq0$. Thus we have to solve a constrained
minimization problem. There are two possibilities. The first one is that the minimum
occurs at the boundaries of the feasible region. Therefore let us calculate

\begin{align*}
  f(1,0) &= 1 , \\
  f(0,1) &= 1 - \gamma .
\end{align*}

The second possibility is that the minimum occurs in the inside of the feasible region. In
that case we might try to find it by the method of Lagrange multipliers:

\begin{align*}
  \partial_x f(x, y) &= 2x + (\gamma + 2\sqrt{1-\gamma}) y = \lambda \cdot 1 , \\
  \partial_y f(x, y) &= 2(1-\gamma)y + (\gamma + 2\sqrt{1-\gamma}) x = \lambda \cdot 1 .
\end{align*}

This looks a bit complicated so let us change the strategy a bit by defining a
parameterization of the feasible region

$$
  g(s) = f(s,1-s) .
$$

Abbreviating $\alpha=\gamma+2\sqrt{1-\gamma}$ we see that

\begin{align*}
  g'(s) &= \partial_x f(s,1-s) - \partial_y f(s,1-s) \\
  &= (\alpha + 2\gamma - 2) - 2(\gamma + \alpha - 2) s \\
  &=: c_0 - c_1 s .
\end{align*}

Observe that $\alpha+\gamma\geq2$. Hence $c_0,c_1\geq0$ which implies that there is a
$s_0\geq0$ such that $g$ is increasing for $s\leq\?s_0$ and decreasing afterwards (note
also that $s_0$ could be larger than $1$). Hence a local extremum can only be a local
maximum and the minimum is indeed obtained at the boundary:

$$
  f(0, 1) = g(0) = 1 - \gamma .
$$

Hence $F_{\min}=\sqrt{1-\gamma}$ . QED.

** Exercise 10.14
Write an expression for a generator matrix encoding $k$ bits using $r$ repetitions for
each bit. This is an $[rk,k]$ linear code, and should have an $rk\times\?k$ generator
matrix.

*** Solution
:PROPERTIES:
:CUSTOM_ID: exercise-10.14-solution
:END:
Let $\mathbb{1}_r\in\?\BB^r$ be the vector containing just ones. A generator for this code
is given by the tensor product

$$
  G = I_k \otimes \mathbb{1}_r .
$$

This naturally reproduces the special cases $r=3$, $k\in\{1,2\}$ from the book.

** Exercise 10.15
Show that adding one column of $G$ to another results in a generator matrix generating the
same code.

*** Proof
Let $(e_j)$ be the standard basis vectors in $\BB^k$. The code generated by $G$ is just
the image $C=G\BB^k$ of $G$ which in turn is given by all linear combinations of
$(Ge_j)$. Let $G'$ be the generator found by adding column $k$ to column $l\neq\?k$.

We have to show that $C=G'\BB^k$. For this in turn it suffices to show that $(G'e_j)$
spans $C$. First of all observe that

$$
  Ge_j = G'e_j \text{ for } j \neq l .
$$

Hence it suffices to show that $\{Ge_k,Ge_l\}$ spans the same space as
$\{G'e_k,G'e_l\}$. Observe that

\begin{align*}
  G' e_l &= Ge_k + Ge_l , \\
  G e_l &= G'e_k + G'e_l .
\end{align*}

This shows the claim. QED.

** Exercise 10.16
Show that adding one row of the parity check matrix to another does not change the
code. Using Gaussian elimination and swapping of bits it is therefore possible to assume
that the parity check matrix has the standard form $[A|I_{n-k}]$, where $A$ is an
$(n-k)\times\?k$ matrix.

*** Proof
Recall that a code $C$, in terms of a parity check matrix $H$, is defined as

$$
  y \in C \Leftrightarrow \forall i: \sum_j H_{ij} y_j = 0 .
$$

Let $H'$ be a matrix obtained from $H$ by adding row $k$ to row $l$. Let $C'$ be the code
defined by $H'$. We have to show $C'=C$. Let $y\in\?C$. Then we have

$$
  \forall i\neq l: \sum_j H'_{ij} y_j = 0 .
$$

because for those $i$ we have $H'_{ij}=H_{ij}$. Moreover

$$
  \sum_j H'_{lj} y_j = \sum_j H_{kj} y_j + \sum_j H_{lj} y_j =  0 ,
$$

by definition of $H'$. Hence $y\in\?C'$. Since $y\in\?C$ was arbitrary this shows
$C\subseteq\?C'$. By symmetry we also have $C'\subseteq\?C$. In fact, $H$ can be obtained
from $H'$ by adding row $k$ to line $l$ so the same reasoning applies to the reverse
inclusion. QED.

** Exercise 10.17
Find a parity check matrix for the $[6,2]$ repetition code defined by the generator matrix in
(10.54).

\begin{bmatrix}
1 & 0 \\
1 & 0 \\
1 & 0 \\
0 & 1 \\
0 & 1 \\
0 & 1 \\
\end{bmatrix}

*** Solution
Let $G_1$ and $H_1$ be the generator and parity check matrix for $k=1$ bits (see equations
(10.53) and (10.58)). We have

$$
  G_2 = I_2 \otimes G_1 .
$$

(C.f. the [[#exercise-10.14-solution][solution]] of exercise 10.14.) This suggests to define

$$
  H_2 = I_2 \otimes H_1 = \begin{bmatrix}
    1 & 1 & 0 & 0 & 0 & 0 \\
    0 & 1 & 1 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1 & 1 & 0 \\
    0 & 0 & 0 & 0 & 1 & 1
  \?\end{bmatrix} .
$$

This clearly does the job. It makes sense to compare this to [[#exercise-10.18][exercise 10.18]].

** Exercise 10.18
:PROPERTIES:
:CUSTOM_ID: exercise-10.18
:END:
Show that the parity check matrix $H$ and generator matrix $G$ for the same linear code
satisfy $HG=0$.

*** Proof
Recall that a code $C$ is just the image of the generator $C=G\BB^k$. On the other hand it
can be defined as the kernel of the parity check matrix: $C=\{y|Hy=0\}$. Since every code
word has the form $y=Gx$ we see that $HGx=0$ for every $x$. Hence the claim follows. QED.

** Exercise 10.19
Suppose an $[n,k]$ linear code $C$ has a parity check matrix of the form $H=[A|I_{n-k}]$,
for some $(n-k)\times\?k$ matrix $A$. Show that the corresponding generator matrix is

$$
  G = \left[ \begin{array}{c} I_k \\ \hline -A \end{array} \right]
$$

(Note that $-A=A$ since we are working modulo 2; however, this equation also holds for
linear codes over more general ﬁelds than $\ZZ_2$.)

*** Solution
Clearly $G$ has rank $k$. Hence its images has the same dimension as the kernel of $H$
(which is $k$). We only have to verify that $HG=0$ (c.f. [[#exercise-10.18][exercise 10.18]]). But this is
easy:

$$
  HG = A I_k - I_{n-k} A = A - A = 0 .
$$

** Exercise 10.20
:PROPERTIES:
:CUSTOM_ID: exercise-10.20
:END:
Let $H$ be a parity check matrix such that any $d-1$ columns are linearly independent, but
there exists a set of $d$ linearly dependent columns. Show that the code defined by $H$
has distance $d$.

*** Proof
Let us divide the proof into two parts. In part one we assume that $H$ has $d$ linearly
/dependent/ columns. In part two we assume that every $d-1$ columns are linearly
/independent/.

- Part 1 :: Assume that $H$ has $d$ linearly /dependent/ columns $c_1,\ldots,c_d$. We want
  to show that there exists a $y\in\?C$ (i.e. $y\in\BB^n$ such that $Hy=0$) with
  $\wt{y}\leq\?d$.

  Linear dependence means that there exist coefficients $\alpha_i\in\BB$ such that

  $$
  0 = \sum_i \alpha_i c_i .
  $$

  Without loss of generality we may assume that all $\alpha_i$ are equal to $1$. Otherwise
  there would exist a subset of $d'\?<\?d$ columns such that this is fulfilled.

  Let $J=\{i_1,\ldots,i_d\}$ be the indices of these columns in $H$. Let $y\in\BB^n$ be
  such that $y_i=1$ if $i\in\?J$ and $y_i=0$ otherwise. By construction we have $\wt{y}=d$
  and $Hy=0$. This shows the claim of part 1.
- Part 2 :: Assume that every $d-1$ columns are linearly /independent/. We have to show
  that $Hy=0$ implies $\wt{y}\geq\?d$.

  The assumption implies that every for every $y$ with $\wt{y}\leq\?d-1$ we have
  $Hy\neq0$. But this is equivalent to the claim.

Taking parts 1 and 2 together we see that $d(C)=d$ is equivalent to: Any $d-1$ columns of
$H$ are linearly independent but there exist $d$ columns which are linearly
dependent. QED.

** Exercise 10.21 (Singleton bound)
Show that an $[n,k,d]$ code must satisfy $n-k\geq\?d-1$.

*** Proof
This follows directly from [[#exercise-10.20][exercise 10.20]]. In fact, the parity check matrix
$H\in\BB^{(n-k)\times\?n}$ of the code must have full rank $n-k$ in order for the code
space to be $k$​-dimensional (otherwise it would be bigger and there could be no bijection
between the code words and the elements of $\BB^k$).

On the other hand, from $d(C)=d$, using exercise 10.20, we see that the rank of $H$ must
be at least $d-1$. Hence $n-k\geq\?d-1$. QED.

** Exercise 10.22
Show that all Hamming codes have distance $3$, and thus can correct an error on a single
bit. The Hamming codes are therefore $[2^r-1,2^r-r-1,3]$ codes.

*** Proof
The parity check matrix of the Hamming code is made of matrices with columns $h_j$ being
the binary representation of $j$ for $j\in\{1,\ldots,2^r-1\}$. Hence

$$
  h_1 + h_2 = h_3 ,
$$

implying that the first three columns are linearly dependent. On the other hand any two
columns are linearly independent since

$$
  h_i + h_j = 0
$$

implies $i=j$. Now the claim follows from [[#exercise-10.20][exercise 10.20]]. QED.

** Exercise 10.23
Prove the Gilbert–Varshamov bound: Let $\varepsilon\?>0$ and $0\?<\delta\?<\?1/2$. There
exists an $[n,k,d]$ code such that

$$
  R \geq 1 - H(\delta) - \varepsilon ,
$$

where $R=k/n$ is the /rate/ of the code, $\delta\leq(d-1)/n$, and
$H(x)=-x\log_2(x)-(1-x)\log_2(x)$. In other words, for a given relative distance
$\delta\?<\?1/2$ there is always a corresponding code whose transmission rate is not too
bad.

- Remark ::
  - For $\varepsilon=0$ this is equivalent to the orginial exercise. The proof /suggests/
    that it is necessary to have $\varepsilon\?>\?0$. Moreover the [[https://en.wikipedia.org/wiki/Gilbert%E2%80%93Varshamov_bound_for_linear_codes][wikipedia]] version of
    the theorem also has it. Hence I believe this is just an error in the exercise.
  - Another reason why I reformulated the statement is the following: In the original
    statement I originally imagined $t$ to be fixed. But then if you only consider large
    $n$ this isn't really useful since a code with very many bits has also a large "attack
    surface" for errors. So it is more natural to consider a relative quantity instead.
  - The authors write "The proof of the Gilbert–Varshamov bound is quite simple ...". I do
    not share this assessment. Maybe my solution is more complicated than it should
    be. But in my opinion most exercises in part III of the book (so far) are much easier.

*** Proof
In this proof we use the so called [[https://en.wikipedia.org/wiki/Probabilistic_method][probabilistic method]] which seems to be the standard way
to prove this bound (see e.g. [[https://en.wikipedia.org/wiki/Gilbert%E2%80%93Varshamov_bound_for_linear_codes][wikipedia]] for the general case of linear codes over finite
fields).

Let $n\geq\?k$ to be fixed later. The main idea is to consider a uniform probability
distribution over $\BB^{n\times\?k}$ and draw random generators $G$ from it (let us
denote the generated code by $C$). The idea is to show that there is a non-zero
probability that the drawn generator generates a code with the desired properties.

Note one subtle thing here: the generator must be injective in order to generate a
$k$​-dimensional code. Therefore consider the following lemma.

- Lemma :: For all $n\geq\?k$ let $P(n,k)$ be the probability that a uniformly at random
  drawn matrix $G\in\BB^{n\times\?k}$ has rank $k$. There exists a constant $P_\infty\?>\?0$
  (which does not depend on $n$ or $k$) such that $P(n,k)\geq\?P_\infty$.
  - Proof :: Clearly $P(n,k)\geq\?P(k,k)$. So we only have to consider the case $n=k$. For
    $l\leq\?k$ let $P_l$ be the probability that a random matrix from $\BB^{l\times\?k}$
    has rank $l$. Clearly

    $$ P_1 = (2^k - 1) / 2^k = 1 - 2^{-k} $$

    because there is only one vector (the zero vector) which leads to a single rowed
    rank-zero matrix. Observe that

    $$ P_{l+1} = P_l \cdot (2^k - 2^l) /2^k . $$

    This is because in order to have $l+1$ independent rows you first need $l$ independent
    rows and the last row has to be outside the $l$​-dimensional subspace (with $2^l$
    elements) of the first $l$ rows. Hence

    $$ P(k,k) = P_k = \prod_{j=1}^{k} (1 - 2^{-j}) \geq \prod_{j=1}^{\infty} (1 - 2^{-j}) =: P_\infty > 0 . $$

    QED.

Our goal is to prove that for sufficiently large $n$ (considering also non-injective $G$)

<<exercise-10.23-2>>
$$
  \prob{d(C)\geq d} > P_\infty
$$

because then

$$
  \prob{d(C)\geq d \text{ and } \rank{G}=k} \geq  \prob{d(C)\geq d} - P_\infty > 0 .
$$

which proves the claim. Let us estimate the probability that the distance of the code is
/not/ at least $d$. In order to show the above [[exercise-10.23-2][bound]] it suffices to show that the
following can be made arbitrary small (for large $n$):

<<exercise-10.23-1>>
$$
  \prob{d(C)\leq d-1} \leq \sum_{x\in\BB^k\backslash\{0\}} \prob{\wt{Gx} \leq d-1} = \frac{V_d}{2^{n-k}} .
$$

The first inequality follows from the fact that the event $d(C)\leq\?d-1$ is equivalent to
the event that at least one of the code words has distance at most $d-1$ (also use the
sub-additivity of probabilities). For the equality on the right let us first note that for
each $x$ the code word $Gx$ is uniformly distributed (each column of $G$ is uniformly
distrubuted and so is each sum of columns). Hence $\prob{\wt{Gx}\leq\?d-1}=V_d/2^n$ where

$$
  V_d = \sum_{i=0}^{d-1} \binom{n}{i}
$$

is the number of potential code words with weight at most $d-1$.

- Lemma :: $V_d\leq2^{H(\delta)n}$ if $\delta\leq1/2$, where $\delta=(d-1)/n$.
  - Remark :: A reverse inequality is is also true: $V_d\geq2^{H(\delta)n+o(n)}$. This can
    be seen from Stirlings formula. Actually I knew this reverse inequality already since
    I solved exercise 6.14, see e.g. [[file:chapter_6.org::exercise-6.14-proof-probability-entropy][here]] to get an idea for the proof. This is the reason
    I found it natural to connect $V_d$ to the entropy. I was a bit surprised to find out
    that the $V_d\leq\ldots$ inequality has such a clean form (no Landau notation).
  - Proof :: The claim follows from this:

    $$
      \sum_{i=0}^{d-1} \binom{n}{i} 2^{-H(\delta)n}
      = \sum_{i=0}^{d-1} \binom{n}{i} \delta^{d-1} (1-\delta)^{n-d+1}
      \leq \sum_{i=0}^{d-1} \binom{n}{i} \delta^j (1-\delta)^{j-d}
      = (\delta + (1 - \delta))^n
      = 1 .
    $$

    For the inequality in the middle it is important to have $\delta\leq1/2$. The last
    equality is just the binomial formula. QED.

Using the lemma in [[exercise-10.23-1][the estimate]] for the probability that $d(C)\leq\?d-1$ we get

$$
  P := \prob{d(C)\leq d-1} \leq \left[\frac{2^{H(\delta)+O(n\inv)}}{2^{1-R}}\right]^n
  = \left[\frac{2^{R+o(1)}}{2^{1-H(\delta)}}\right]^n .
$$

Here we choose $d$ minimal so that $\delta\leq(d-1)/n$ holds. Hence $\delta=(d-1+O(1))/n$
which explains the $O(n\inv)$ in the inequalities above.

Let us suppose we can choose $R$ such that

\begin{align*}
  R &\geq 1 - H(\delta) - \varepsilon , \\
  R + o(1) &\leq 1 - H(\delta) - \varepsilon/2 , \\
\end{align*}

Then we would have $P\leq2^{-\varepsilon\?n/2}$ which is arbitrary small if $n$ is large
enough - which proves the [[exercise-10.23-2][desired bound]]. But since $R=k/n$ this is indeed possible if we
choose $n$ large enough and $k$ appropriately. QED.

** Exercise 10.24
Show that a code with generator matrix G is weakly self-dual if and only if $G^TG=0$.

*** Proof
Weakly self dual means $C\subseteq\?C^{\bot}$. Hence

$$
  H^{\bot} G = 0 ,
$$

where $G$ is the generator of $C$ and $H^\bot$ the parity check matrix of $C^\bot$. But
a parity check matrix of $C^\bot$ is $H^\bot=G^T$. QED.

** Exercise 10.25
Let $C$ be a linear code. Show that if $x\in\?C^\bot$ then
$\sum_{y\in\?C}(-1)^{x\cdot\?y}=\abs{C}$, while if $x\notin\?C^\bot$ then
$\sum_{y\in\?C}(-1)^{x\cdot\?y}=0$.

*** Proof
By definition of dual codes we have $x\cdot\?y=0$ if $x\in\?C^\bot$ and $y\in\?C$. Hence,
in that case

$$
  \sum_{y\in\?C}(-1)^{x\cdot y} = \sum_{y\in\?C} 1 = \abs{C}
$$

Assume now that $x\in\?C^\bot$. Hence there exists a $\tilde{y}\in\?C$ such that
$x\cdot\?\tilde{y}=1$. Let us decompose $C=C'\oplus\?\tilde{y}\BB$. Hence

$$
  \sum_{y\in C}(-1)^{x\cdot y}
  = \sum_{y'\in C'} \sum_{\alpha=0}^1 (-1)^{y'\cdot x + \alpha\tilde{y}\cdot x}
  = \sum_{y'\in C'} 0
  = 0 .
$$

QED.

** Exercise 10.26
Suppose $H$ is a parity check matrix. Explain how to compute the transformation
$\ket{x}\ket{0}\mapsto\ket{x}\ket{Hx}$ using a circuit composed entirely of =CNOT= gates.

*** Solution
Let us explain it on the specific example of the $[3,1]$ repetition code:

$$
  H = \begin{bmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \end{bmatrix} .
$$

A corresponding circuit would be

#+RESULTS[a2f14439236c60ff5cd3272265c21f9531fca42b]: exercise-10.26-circuit
#+begin_example

a_0: ──■─────────────────
       │
a_1: ──┼────■────■───────
       │    │    │
a_2: ──┼────┼────┼────■──
     ┌─┴─┐┌─┴─┐  │    │
b_0: ┤ X ├┤ X ├──┼────┼──
     └───┘└───┘┌─┴─┐┌─┴─┐
b_1: ──────────┤ X ├┤ X ├
               └───┘└───┘
#+end_example

The idea is that for each $(i,j)$ with $H_{ij}=1$ we have a =CNOT= with control at target
$b_i$ and control $a_j$.

#+name: exercise-10.26-circuit
#+begin_src python :results replace :cache yes :exports results
  qr0 = QuantumRegister(3, "a")
  qr1 = QuantumRegister(2, "b")

  qc = QuantumCircuit(qr0, qr1)

  # H_{0j}
  qc.cx(qr0[0], qr1[0])
  qc.cx(qr0[1], qr1[0])

  # H_{1j}
  qc.cx(qr0[1], qr1[1])
  qc.cx(qr0[2], qr1[1])

  qc.draw()
#+end_src

** Exercise 10.27
Show that the codes defined by

$$
  \ket{x+C_2} = \frac{1}{\sqrt{\abs{C_2}}} \sum_{y\in\?C_2} (-1)^{u\cdot y} \ket{x+y+v}
$$

and parameterized by $u$ and $v$ are equivalent to $\mathrm{CSS}(C_1,C_2)$ in the sense
that they have the same error-correcting properties. These codes, which we’ll refer to as
$\mathrm{CSS}_{u,v}(C_1,C_2)$, will be useful later in our study of quantum key
distribution, in Section 12.6.5.

*** Sketch of a solution
By using $X$, $Z$, and Hadamard gates one can unitarily transform the code into the
following form

$$
  \ket{x+C_2} = \frac{1}{\sqrt{\abs{C_2}}} \sum_{y\in\?C_2} (-1)^{u\cdot x} \ket{x+y}
  = (-1)^{u\cdot x} \frac{1}{\sqrt{\abs{C_2}}} \sum_{y\in\?C_2} \ket{x+y} .
$$

Basically we introduce errors $e_1=v$ and $e_2=u$ on purpose to accomplish that. Now it is
not hard to verify that the factor $(-1)^{u\cdot\?x}$ does not interfere with the analysis
from the book.

** Exercise 10.28
Verify that the transpose of the matrix in (10.77) is the generator of the $[7,4,3]$
Hamming code.

*** Solution
The claim follows essentially from the following calculation:

#+begin_src sage :tangle no :results replace :cache yes
  # Standard parity check matrix for the [7,3] Hamming code:
  H = matrix(GF(2), [
      [0, 0, 0, 1, 1, 1, 1],
      [0, 1, 1, 0, 0, 1, 1],
      [1, 0, 1, 0, 1, 0, 1],
  ])

  # We want to check that this is indeed a generator:
  G = matrix(GF(2), [
      [1, 0, 0, 0, 0, 1, 1],
      [0, 1, 0, 0, 1, 0, 1],
      [0, 0, 1, 0, 1, 1, 0],
      [0, 0, 0, 1, 1, 1, 1],
  ]).T

  assert H*G == matrix.zero(3, 4)
  "PASSED"
#+end_src

#+RESULTS[08edf7627e1235ea9a0bb19479584be688be03ed]:
: 'PASSED'

One important thing to note is that $G$ has full rank (obviously). Hence $HG=0$ is
sufficient to prove $G$ to be a generator of the $[7,4]$ Hamming code.
