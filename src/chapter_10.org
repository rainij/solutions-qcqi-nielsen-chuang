#+title:  Chapter 10
#+author: Reinhard Stahn
#+setupfile: ./inc/setupfile.org
#+include: ./inc/latex-macros.org
#+property: header-args:sage :session *sage-chapter-10* :tangle chapter_10.sage
# python session just for qiskit since sage integers dont work well with qiskit:
#+property: header-args:python :session *python-chapter-10* :tangle no

#+toc: headlines 2

* Setup
** Imports
#+name: chapter-10-imports
#+begin_src sage
  from utils_sage import ket
  from chapter_8_sage import make_operation
#+end_src

# Hidden imports
#+name: chapter-10-hidden-imports
#+begin_src python :exports none
  from qiskit.circuit import QuantumCircuit, QuantumRegister, Parameter as Param
#+end_src

** Utility code
#+name: chapter-10-global-vars
#+begin_src sage
  g = SR.var("g", domain="positive")
  a, b = SR.var("a b", domain="complex")
  assume(g <= 1)
#+end_src

** Notation
Below we use $\BB=\mathrm{GF}(2)=\ZZ_2$ to denote the field of two elements. This models a
/bit/.

* Exercises
** Exercise 10.1
Verify that the encoding circuit in Figure 10.2 works as claimed.

#+RESULTS[b2469d3d1659de6df9ffe41f5443d686897376b9]: exercise-10.1-circuit
:
: |psi>: ──■────■──
:        ┌─┴─┐  │
: |0>_0: ┤ X ├──┼──
:        └───┘┌─┴─┐
: |0>_1: ─────┤ X ├
:             └───┘


#+name: exercise-10.1-circuit
#+begin_src python :results replace :cache yes :exports results
  qr0 = QuantumRegister(1, "|psi>")
  qr1 = QuantumRegister(2, "|0>")

  qc = QuantumCircuit(qr0, qr1)
  qc.cx(0, 1)
  qc.cx(0, 2)

  qc.draw()
#+end_src

*** Solution
This is really just observing that the circuit acts like this on the /relevant/ basis
states

\begin{align*}
  \ket{000} &\mapsto \ket{000} , \\
  \ket{100} &\mapsto \ket{111} . \\
\end{align*}

The rest follows by linearity.

** Exercise 10.2
The action of the bit flip channel can be described by the quantum operation
$\calE(\rho)=(1-p)\rho+pX\rho\?X$. Show that this may be given an alternate operator-sum
representation, as $\calE(\rho)=(1-2p)\rho+2pP_+\rho\?P_++2pP_-\rho\?P_-$ where $P_+$ and
$P_-$ are projectors onto the $+1$ and $-1$ eigenstates of $X$,
$(\ket{0}+\ket{1})/\sqrt{2}$ and $(\ket{0}+\ket{1})/\sqrt{2}$ respectively.  This latter
representation can be understood as a model in which the qubit is left alone with
probability $1-2p$, and is ‘measured’ by the environment in the $\ket{+}$, $\ket{-}$ basis
with probability $2p$.

*** Proof
Note that $X=P_+-P_-$ and $I=P_++P_-$. Hence

$$
  X\rho X = P_+\rho P_+ + P_-\rho P_- - (P_+\rho P_- + P_-\rho P_+)
  = 2(P_+\rho P_+ + P_-\rho P_-) - I\rho I .
$$

Hence

$$
  \calE(\rho) = (1 - 2p)\rho + 2p (P_+\rho P_+ + P_-\rho P_-) .
$$

QED.

** Exercise 10.3
Show by explicit calculation that measuring $Z_1Z_2$ followed by $Z_2Z_3$ is equivalent,
up to labeling of the measurement outcomes, to measuring the four projectors defined by
(10.5)–(10.8), in the sense that both procedures result in the same measurement statistics
and post-measurement states.

*** Proof
Let us define the following projectors

\begin{align*}
  P_{ijk} &= \proj{i}\otimes\proj{j}\otimes\proj{k} , \\
  P_{ijx} &= \proj{i}\otimes\proj{j}\otimes I , \\
  P_{xij} &= I\otimes \proj{i}\otimes\proj{j} .
\end{align*}

That is, the $x$ has a special meaning and just stands for the place where the identity
acts. Define $P_{ixx}$ etc analogously. Note that e.g. $P_{ixx}P_{xjx}=P_{ijx}$. Since
$Z_1=P_{0xx}-P_{1xx}$, $Z_2=P_{x0x}-P_{x1x}$, $Z_3=P_{xx0}-P_{xx1}$ we have the following
spectral decomposition of the observables we are interested in:

\begin{align*}
  Z_1Z_2 &= P_{00x} + P_{11x} - P_{01x} - P_{10x} , \\
  Z_2Z_3 &= P_{x00} + P_{x11} - P_{x01} - P_{x10} .
\end{align*}

Let $P_j$ for $j=0..3$ be the projectors from (10.5) to (10.8). If we measure for example
$+1$ for $Z_1Z_2$ and $+1$ for $Z_2Z_3$ this corresponds to the projection (note: the
order of the projections is not important due to commutativity)

$$
  (+1, +1):\; (P_{00x} + P_{11x})(P_{x00} + P_{x11}) = P_{000} + P_{111} = P_0 .
$$

In the same way the other three measurements can be mapped to the $P_j$ too:

\begin{align*}
  &(+1, -1):\; (P_{00x} + P_{11x})(P_{x01} + P_{x10}) = P_{001} + P_{110} = P_3 , \\
  &(-1, +1):\; (P_{01x} + P_{10x})(P_{x00} + P_{x11}) = P_{100} + P_{011} = P_1 , \\
  &(-1, -1):\; (P_{01x} + P_{10x})(P_{x01} + P_{x10}) = P_{101} + P_{010} = P_2 .
\end{align*}

Hence the projectors are the same, just re-labeled. QED.

** Exercise 10.4
Consider the three qubit bit flip code. Suppose we had performed the error syndrome
measurement by measuring the eight orthogonal projectors corresponding to projections onto
the eight computational basis states.

1. Write out the projectors corresponding to this measurement, and explain how the
   measurement result can be used to diagnose the error syndrome: either no bits flipped or
   bit number $j$ flipped, where $j$ is in the range one to three.
2. Show that the recovery procedure works only for computational basis states.
3. What is the minimum fidelity for the error-correction procedure?

*** Solution for part 1
The projectors are just $P_j=\proj{j}$ for $j=0..7$ (note that the binary representations
for the numbers from $0$ to $7$ are $000$, $001$, $010$, $011$, $100$, $101$, $110$,
$111$). A corresponding observable distinguishing each projector would be
$A=\sum_jj\proj{j}$.

Recall that the encoded state is $\ket{\psi}=a\ket{000}+b\ket{111}$. Let $k=0$, $k=1$,
$k=2$, $k=3$ denote /no error/, /error in qubit 1, 2, 3/, respectively. Let $M_k$ be the
set of possible measurements for the given value of $k$. We have

<<exercise-10.4-1>>
\begin{align*}
  M_0 &= \{0, 7\} , \\
  M_1 &= \{1, 6\} , \\
  M_2 &= \{2, 5\} , \\
  M_3 &= \{4, 3\} .
\end{align*}

Moreover, measuring $0,1,2,4$ happens with probability $\abs{a}^2$ and measuring $7,6,5,3$
happens with probability $\abs{b}^2$ (conditioned on a fixed value for $j$). Since the
four sets are disjoint we can infer the error (the value of $k$) from the measurement
(e.g. measuring $1$ or $6$ implies that the error was $k=1$ (with certainty if only one of
the three errors "is allowed" to happen)).

*** Solution for part 2
If we measure $j$ and infer the error $k$ [[exercise-10.4-1][according to the sets]] $M_k$ we would correct it
by applying $X_k$ to the state. In other words, the following quantum operation $\calR$
performs the syndrome measurement followed by the recovery:

\begin{align*}
  E_0 &= P_0, \\ E_1 &= X_1P_1, \\ E_2 &= X_2P_2, \\ E_3 &= X_3P_3, \\
  E_4 &= X_3P_4, \\ E_5 &= X_2P_5, \\ E_6 &= X_1P_6, \\ E_7 &= P_7 .
\end{align*}

Note that $\calR$ is indeed trace-preserving ($\sum_jE_j^\dagger\?E_j)=I$). Recall
$\ket{\psi}=a\ket{000}+b\ket{111}$. Then (the /no error/ case)

$$
  \calR(\ket{\psi}) = \abs{a}^2 \proj{000} + \abs{b}^2 \proj{111} .
$$

The same formula holds for $\ket{\psi}$ replaced by $X_k\ket{\psi}$ ($k=1..3$). Hence, for
the following noise channel

$$
  \calE(\rho) = (1-p)\rho + \sum_{k=1}^3 p_k X_k \rho X_k
$$

where $\sum_kp_k=p$ (this implies trace-preservation, but we could more generally assume
$\ldots\leq\?p$, meaning that with certain probability "something else" happens), which
stands for having at most one bit-flip (with probability $p_k$ at position $k$), we have

$$
  \calR\circ\calE(\ket{\psi}) = \abs{a}^2 \proj{000} + \abs{b}^2 \proj{111} .
$$

(If $\calE$ was not trace-preserving the RHS would have a factor $1-p+\sum_jp_j$.) Hence
the bit-flip error gets indeed corrected, but unwanted side effects happen too. The final
state after recovery is either $\ket{000}$ (probability $\abs{a}^2$) or $\ket{111}$
(probability $\abs{b}^2$). That is the state collapses to a basis state. This is only
correct if $\ket{\psi}$ was already in a basis state ($a=0$ or $b=0$) - which is the claim
of part 2.

*** Solution for part 3
Let us consider two cases:

1. We do not get to know the results of the syndrome measurement.
2. We get to know the results of the syndrome measurement.

In case 1 the whole error correction procedure is given by $\calR$ from the solution of
part 2. Let $\rho=\calR\circ\calE(\ket{\psi})$. Then

$$
  F(\ket{\psi},\rho) = \sqrt{\bra{\psi}\rho\ket{\psi}} = \sqrt{\abs{a}^4 + \abs{b}^4} .
$$

Since $\abs{a}^2+\abs{b}^2=1$ the minimum occurs at $\abs{a}^2=\abs{b}^2=1/2$. Hence
$F\geq1/\sqrt{2}$ in that case.

In case 2 the post-correction state $\rho$ is either $\ket{000}$ (probability $\abs{a}^2$)
or $\ket{111}$ (probability $\abs{b}^2$). Hence

$$
  F(\ket{\psi},\rho) \in \left\{\sqrt{\braket{\psi}{i_L}\braket{i_L}{\psi}} \; \middle| \; i\in\{0,1\}\right\}
  = \{\abs{a}, \abs{b}\} ,
$$

where the first alternative occurs with probability $\abs{a}^2$ and the second one with
$\abs{b}^2$.

In both cases the minimial fidelity does not depend on the specifics of the noise model
and in particular is always bad if $\abs{a}$ and $\abs{b}$ are bounded away from $0$ (or
equivalently, from $1$).

- Remark :: The minimum fidelity of case 1 is the weighted root square mean of the two
  possible fidelities of case 2. The weights are $\abs{a}^2$ and $\abs{b}^2$. See e.g.
  [[https://en.wikipedia.org/wiki/Generalized_mean][generalized mean]] on wikipedia for more infos. To make this more apparent consider
  $\sigma=p\proj{0}+(1-p)\proj{1}$ (and $\ket{\psi}=a\ket{0}+b\ket{1}$):

  $$
  F(\ket{\psi},\sigma) = \sqrt{\bra{\psi}\sigma\ket{\psi}} = \sqrt{p\abs{a}^2+(1-p)\abs{b}^2} .
  $$

  Here $p$ and $1-p$ are the weights.

** Exercise 10.5
Show that the syndrome measurement for detecting phase flip errors in the Shor code
corresponds to measuring the observables $A=X_1X_2X_3X_4X_5X_6$ and
$B=X_4X_5X_6X_7X_8X_9$.

*** Proof
:PROPERTIES:
:CUSTOM_ID: exercise-10.5-solution
:END:
Let us denote

\begin{align*}
  \ket{\pi} &= \frac{1}{\sqrt{2}} (\ket{000} + \ket{111}) , \\
  \ket{\mu} &= \frac{1}{\sqrt{2}} (\ket{000} - \ket{111}) .
\end{align*}

With this notation the Shor code is given by

\begin{align*}
  \ket{0_L} &= \ket{\pi\pi\pi} , \\
  \ket{1_L} &= \ket{\mu\mu\mu} , \\
\end{align*}

Hence we have the following encoding:

$$
  \ket{\psi} = \ket{\psi_0} = a\ket{0_L} + b\ket{1_L} = a\ket{\pi\pi\pi} + b\ket{\mu\mu\mu} .
$$

In this notation a phase flip error is represented naturally. For example a phase flip in
one of the three qubits in the first block of $\ket{\psi}$ is given by

$$
  \ket{\psi_1} = a\ket{\mu\pi\pi} + b\ket{\pi\mu\mu} .
$$

It doesn't matter in which of the three qubits, it always leads to the same state. The
other two errors (phase flip in the second or third block) are given by

\begin{align*}
  \ket{\psi_2} &= a\ket{\pi\mu\pi} + b\ket{\mu\pi\mu} , \\
  \ket{\psi_3} &= a\ket{\pi\pi\mu} + b\ket{\mu\mu\pi} .
\end{align*}

Now observe that

\begin{align*}
  X_1X_2X_3 \ket{\pi} &= + \ket{\pi} , \\
  X_1X_2X_3 \ket{\mu} &= - \ket{\mu} .
\end{align*}

Hence e.g. measuring a state $\ket{ijk}$ ($i,j,k\in\{\pi,\mu\}$) with respect to $A$
detects if $i=j$ (measurement result: $+1$) or $i\neq\?j$ (measurement result:
$+1$). Because of their special structure this translates to $\ket{\psi_k}$. For the four
errors (first one is /no error/) we get the following measurement results (each with
certainty):

| $k$ ($\ket{\psi_k}$) | $A$ | $B$ |
|----------------------+-----+-----|
|                    0 |  +1 |  +1 |
|                    1 |  -1 |  +1 |
|                    2 |  -1 |  -1 |
|                    3 |  +1 |  -1 |

Hence we can infer the syndrome $k$ (which error occured) from the measurement - as
desired. QED.

** Exercise 10.6
Show that recovery from a phase flip on any of the first three qubits may be accomplished
by applying the operator $Z_1Z_2Z_3$.

*** Proof
Let us reuse the notation from the [[#exercise-10.5-solution][solution]] of exercise 10.5. Clearly

$$
  Z_1Z_2Z_3 \ket{\pi} = \ket{\mu} \text{ and } Z_1Z_2Z_3 \ket{\mu} = \ket{\pi} .
$$

An error in the first three qubits means that the corresponding state is
$\ket{\psi_1}$. By the above we have:

$$
  Z_1Z_2Z_3 \ket{\psi_1} = \ket{\psi} .
$$

QED.

** Exercise 10.7
Consider the three qubit bit flip code of Section 10.1.1, with corresponding projector
$P=\proj{000}+\proj{111}$. The noise process this code against has operation elements

$$
  \left\{ \sqrt{(1-p)^3}I, \sqrt{p(1-p)^2}X_1, \sqrt{p(1-p)^2}X_2, \sqrt{p(1-p)^2}X_3 \right\} ,
$$

where $p$ is the probability that a bit flips. Note that this quantum operation $\calE$ is
not trace-preserving, since we have omitted operation elements corresponding to bit flips
on two and three qubits. Verify the quantum error-correction conditions for this code and
noise process.

*** Solution
Clearly $PX_iX_jP=\delta_{ij}P$. Hence

\begin{align*}
  P E_0^\dagger E_0 P &= (1-p)^3 P , \\
  P E_i^\dagger E_i P &= p(1-p)^2 P \text{ for } i=1..3 , \\
  P E_i^\dagger E_j P &= 0 \text{ for } i\neq j .
\end{align*}

Hence the conditions are satisfied and there exists an error correction procedure $\calR$
for this noise.

*** Bonus: Calculating the recovery procedure
By the previous calculation and Theorem 10.1 there exists a trace-preserving quantum
operation $\calR$ such that

$$
  \calR \circ \calE (P\rho P) = [(1-p)^3 + 3p(1-p)^2] \, P \rho P .
$$

The concrete form of the proportionality factor follows from (10.25) as it equals
$\sum_kd_{kk}$. Since this is a trace this formula would even be true if $(d_{ij})$ wasn't
already diagonal.

To compute the Kraus matrices $R_k$ of $\calR$ we need the $E_k$ such that the $(d_{ij})$
are diagonal. This is already the case

$$
  d = \diag((1-p)^3, p(1-p)^2, p(1-p)^2, p(1-p)^2) .
$$

From the proof of Theorem 10.1 we know that

$$
  R_k = U_k^\dagger P_k
$$

where $P_k=U_kPU_k^\dagger$ and $E_kP=\sqrt{d_{kk}}U_kP$ is a polar decomposition. The
$P_k$ are the projectors onto $E_kC$ ($C$ being the code space onto which $P$
projects). The diagonality of $(d_{ij})$ implies that $P_iP_j=\delta_{ij}P_i$ (meaning
that the error subspaces are orthogonal to each other).

Clearly we can set $U_0=I$ (it is only uniquely defined on the image of $P$). For $k=1$ we
have

$$
  E_1 P = \sqrt{p(1-p)^2} X_1 P .
$$

This is already a polar decomposition (the Pauli operators are unitary). Hence we can just
set $U_1=X_1$ and analogously $U_k=X_k$ for $k=1..3$. Hence

\begin{align*}
  P_0 &= \proj{000} + \proj{111} , \\
  P_1 &= \proj{100} + \proj{011} , \\
  P_2 &= \proj{010} + \proj{101} , \\
  P_3 &= \proj{001} + \proj{110} .
\end{align*}

Note that this corresponds to the already in (10.5-10.8) introduced syndrome measurement
operators. The Kraus matrices of $\calR$ are

$$
  R_k = U_k^\dagger P_k = \begin{cases} IP = P & \text{for } k = 0 , \\
    X_k X_kPX_k = PX_k & \text{for } k \in \{1,2,3\} . \end{cases}
$$

** Exercise 10.8
Verify that the three qubit phase flip code $\ket{0_L}=\ket{+++}$, $\ket{1_L}=\ket{---}$
satisfies the quantum error-correction conditions for the set of error operators
$\{I,Z_1,Z_2,Z_3\}$.

*** Solution 1
Let us reduce this to the bit flip code. To convert to the bit flip code we only have to
conjugate the projector of the code and the Kraus operators of the errors by
$H^{\otimes3}$ (Hadamard).

$$
  \proj{000} + \proj{111} = H^{\otimes3} \proj{+++} + \proj{---} H^{\otimes3}
$$

and

$$
  X_k = H^{\otimes3} Z_k H^{\otimes3} .
$$

Let $P$ be the projector of the phase flip code and let $E_i$ be the mentioned errors it
corrects. Let $Q$ and $F_j$ be the same thing for the bit flip.

$$
  P E_i^\dagger E_j P = H^{\otimes3} Q F_i^\dagger F_j Q H^{\otimes3}
  = H^{\otimes3} \delta_{ij} Q H^{\otimes3}
  = \delta_{ij} P .
$$

In the second equality we used what we already know about the bit flip code.

*** Solution 2
We can also verify this explicitly. Let $P=P_{+++}+P_{---}$ be the projector onto the
code, where we use the notation $P_{ijk}=\proj{ijk}$. Clearly
$PE_i^\dagger\?E_iP=PIP=P$. Moreover

$$
  PZ_1P = (P_{+++} + P_{---})(P_{-++} + P_{+--}) = 0
$$

and

$$
  PZ_1Z_2P = (P_{+++} + P_{---})(P_{--+} + P_{++-}) = 0 .
$$

Similarly $PIZ_jP=PZ_iZ_jP=0$ for $i\neq\?j$. Hence $PE_i^\dagger\?E_iP=\delta_{ij}$.

** Exercise 10.9
Again, consider the three qubit phase flip code. Let $P_i$ and $Q_i$ be the projectors
onto the $\ket{0}$ and $\ket{1}$ states, respectively, of the $i$​th qubit. Prove that the
three qubit phase flip code protects against the error set ${I,P_1,Q_1,P_2,Q_2,P_3,Q_3}$.

*** Solution
Recall that the phase flip code corrects the errors $I$, $Z_1$, $Z_2$, and $Z_3$. By
theorem 10.2 linear combinations of these errors are corrected too. Note that we have

\begin{align*}
  P_k &= \frac{1}{2} (I + Z_k) , \\
  Q_k &= \frac{1}{2} (I - Z_k) .
\end{align*}

Hence those errors are corrected too.

** Exercise 10.10
Explicitly verify the quantum error-correction conditions for the Shor code, for the error
set containing $I$ and the error operators $X_j,Y_j,Z_j$ for $j=1$ through $9$.

*** Solution
Let us denote

\begin{align*}
  \ket{\pi} &= \frac{1}{\sqrt{2}} (\ket{000} + \ket{111}) , \\
  \ket{\mu} &= \frac{1}{\sqrt{2}} (\ket{000} - \ket{111}) .
\end{align*}

The Shor code is given by he projector

$$
  P = \proj{\pi\pi\pi} + \proj{\mu\mu\mu} .
$$

It is not hard to see that the errors transform the code space into a set of mutually
orthogonal subspaces - with one exception. The effect of $Z_j$ is the same if $j$ stays
within one "block" (there are three block $\{1,2,3\}$, $\{4,5,6\}$, and
$\{7,8,9\}$). Hence:

$$
  PE_i^\dagger E_j P = \begin{cases}
    1 & \text{if } i=j \text{ or } E_i,E_j \text{ both Z on same block} , \\
    0 & \text{else} .
  \end{cases}
$$

** Exercise 10.11
Construct operation elements for a single qubit quantum operation $\calE$ that upon input
of any state $\rho$ replaces it with the completely randomized state $I/2$. It is amazing
that even such noise models as this may be corrected by codes such as the Shor code!

*** Solution
We already know that the quantum operation

$$
  \calE(\rho) = \frac{1}{2} I
$$

is the depolarizing channel with $p=1$. By (8.102) this is equal to

$$
  \calE(\rho) = \frac{1}{4} (I\rho I + X\rho X + Y\rho Y + Z\rho Z) .
$$

Hence the relevant operation elements are

$$
  \left\{\frac{1}{2}I, \frac{1}{2}X, \frac{1}{2}Y, \frac{1}{2}Z \right\} .
$$

** Exercise 10.12
Show that the fidelity between the state $\ket{0}$ and $\calE(\proj{0})$ is $\sqrt{1-2p/3}$
and use this to argue that the minimum fidelity for the depolarizing channel is
$\sqrt{1-2p/3}$.

*** Solution 1
Let us follow the hint of the book.

$$
  F(\ket{0}, \calE(\proj{0}))
  = \sqrt{(1-p) + \frac{p}{3}(\bra{0}X\ket{0}^2 + \bra{0}Y\ket{0}^2 + \bra{0}Z\ket{0}^2)}
  = \sqrt{(1-p) + \frac{p}{3}}
  = \sqrt{1-\frac{2p}{3}} .
$$

Let $\ket{\psi}$ be an arbitrary state and $U$ be a unitary operator such that
$\ket{\psi}=U\ket{0}$. Then

$$
  F(\ket{\psi}, \calE(\proj{\psi})) = F(U\ket{0}, U\calE(\proj{0})U^\dagger)
  = F(\ket{0}, \calE(\proj{0})) = \sqrt{1-\frac{2p}{3}} .
$$

In the second equality we used that the fidelity is invariant under unitary transformations.

*** Solution 2
We could also prove the claim by using an alternative formula for the depolarizing channel:

$$
  \calE(\rho) = \frac{2p}{3} I + \left(1 - \frac{4p}{3}\right) \rho .
$$

(compare equations (8.100) and (8.102).) Hence

$$
  F(\ket{\psi}, \calE(\proj{\psi})) = \sqrt{\frac{2p}{3} + \left(1 - \frac{4p}{3}\right)}
  = \sqrt{1-\frac{2p}{3}} .
$$

*** Solution 3
Yet another approach first calculates

$$
  F(\ket{\psi}, \calE(\proj{\psi}))
  = \sqrt{(1-p) + \frac{p}{3}(\bra{\psi}X\ket{\psi}^2 + \bra{\psi}Y\ket{\psi}^2 + \bra{\psi}Z\ket{\psi}^2)} .
$$

Recall the representation $\proj{\psi}=2\inv(I+\vec{r}\cdot\vec{\sigma})$ of a qubit in
the bloch sphere ($\abs{\vec{r}}=1$). One can show that

$$
  \bra{\psi}Z\ket{\psi} = r_z
$$

and analogous formulas for $r_x$ and $r_y$. Hence

$$
  F(\ket{\psi}, \calE(\proj{\psi})) = \sqrt{(1-p) + \frac{p}{3}\abs{\vec{r}}^2} = \sqrt{1-\frac{2p}{3}} .
$$

- Remark :: To see e.g.

  $$
  \bra{\psi}Z\ket{\psi} = r_z
  $$

  observe that

  $$
  \bra{\psi}Z\ket{\psi} = \trace{\proj{\psi}Z} = r_z .
  $$

  In the last equality we used that the Pauli matrices (including $I$) are an orthogonal
  set with respect to the Hilbert-Schmidt scalar product (with norm $\sqrt{2}$ for each
  base vector). The other two formulas for $r_x$ and $r_y$ follow along the same lines.

  If you want to see the last equality directly, observe that (using $S=\sqrt{Z}$)

  $$
  \trace{\proj{\psi}Z} = \trace{S\proj{\psi}S}
  $$

  and use $SIS=Z$, $SXS=\ii\?X$, $SYS=\ii\?Y$, $SZS=I$ (only the last term has a non-zero
  trace).

** Exercise 10.13
Show that the minimum fidelity $F(\ket{\psi},\calE(\proj{\psi}))$ when $\calE$ is the
amplitude damping channel with parameter $\gamma$, is $\sqrt{1-\gamma}$.

*** Proof
Let us use sage to get an algebraic expression for $F$. First define amplitude damping

#+begin_src sage
  E0 = matrix.diagonal([1, sqrt(1-g)])
  E1 = matrix([[0, sqrt(g)], [0, 0]])
  AD = make_operation([E0, E1])
#+end_src

Now we can use this to compute $F$:

#+begin_src sage :tangle no :results replace :cache yes
  psi = a*ket('0') + b*ket('1')
  psi_mat = matrix(psi).T
  rho = AD(psi_mat*psi_mat.H).simplify_full()
  (psi.conjugate() * rho * psi).simplify_full()
#+end_src

#+RESULTS[a966bcbc6402d10431f26efd81c21e2bb71234f7]:
: 2*a*b*sqrt(-g + 1)*conjugate(a)*conjugate(b) + a^2*conjugate(a)^2 + b^2*conjugate(b)^2 + (a*b*conjugate(a)*conjugate(b) - b^2*conjugate(b)^2)*g

This looks a bit ugly so let us rewrite this:

$$
  F = \abs{a}^4 + \abs{b}^4 + \gamma(\abs{a}^2 \abs{b}^2 - \abs{b}^4) + 2\sqrt{1-\gamma}\, \abs{a}^2\abs{b}^2 .
$$

Substituting $x=\abs{a}^2$ and $y=\abs{b}^2$ this can be written as:

$$
  F = \sqrt{f(x,y)} = \sqrt{x^2 + y^2 + \gamma (xy - y^2) + 2\sqrt{1-\gamma} \, xy} ,
$$

under the constraint $x+y=1$ and $x,y\geq0$. Thus we have to solve a constrained
minimization problem. There are two possibilities. The first one is that the minimum
occurs at the boundaries of the feasible region. Therefore let us calculate

\begin{align*}
  f(1,0) &= 1 , \\
  f(0,1) &= 1 - \gamma .
\end{align*}

The second possibility is that the minimum occurs in the inside of the feasible region. In
that case we might try to find it by the method of Lagrange multipliers:

\begin{align*}
  \partial_x f(x, y) &= 2x + (\gamma + 2\sqrt{1-\gamma}) y = \lambda \cdot 1 , \\
  \partial_y f(x, y) &= 2(1-\gamma)y + (\gamma + 2\sqrt{1-\gamma}) x = \lambda \cdot 1 .
\end{align*}

This looks a bit complicated so let us change the strategy a bit by defining a
parameterization of the feasible region

$$
  g(s) = f(s,1-s) .
$$

Abbreviating $\alpha=\gamma+2\sqrt{1-\gamma}$ we see that

\begin{align*}
  g'(s) &= \partial_x f(s,1-s) - \partial_y f(s,1-s) \\
  &= (\alpha + 2\gamma - 2) - 2(\gamma + \alpha - 2) s \\
  &=: c_0 - c_1 s .
\end{align*}

Observe that $\alpha+\gamma\geq2$. Hence $c_0,c_1\geq0$ which implies that there is a
$s_0\geq0$ such that $g$ is increasing for $s\leq\?s_0$ and decreasing afterwards (note
also that $s_0$ could be larger than $1$). Hence a local extremum can only be a local
maximum and the minimum is indeed obtained at the boundary:

$$
  f(0, 1) = g(0) = 1 - \gamma .
$$

Hence $F_{\min}=\sqrt{1-\gamma}$ . QED.

** Exercise 10.14
Write an expression for a generator matrix encoding $k$ bits using $r$ repetitions for
each bit. This is an $[rk,k]$ linear code, and should have an $rk\times\?k$ generator
matrix.

*** Solution
:PROPERTIES:
:CUSTOM_ID: exercise-10.14-solution
:END:
Let $\mathbb{1}_r\in\?\BB^r$ be the vector containing just ones. A generator for this code
is given by the tensor product

$$
  G = I_k \otimes \mathbb{1}_r .
$$

This naturally reproduces the special cases $r=3$, $k\in\{1,2\}$ from the book.

** Exercise 10.15
Show that adding one column of $G$ to another results in a generator matrix generating the
same code.

*** Proof
Let $(e_j)$ be the standard basis vectors in $\BB^k$. The code generated by $G$ is just
the image $C=G\BB^k$ of $G$ which in turn is given by all linear combinations of
$(Ge_j)$. Let $G'$ be the generator found by adding column $k$ to column $l\neq\?k$.

We have to show that $C=G'\BB^k$. For this in turn it suffices to show that $(G'e_j)$
spans $C$. First of all observe that

$$
  Ge_j = G'e_j \text{ for } j \neq l .
$$

Hence it suffices to show that $\{Ge_k,Ge_l\}$ spans the same space as
$\{G'e_k,G'e_l\}$. Observe that

\begin{align*}
  G' e_l &= Ge_k + Ge_l , \\
  G e_l &= G'e_k + G'e_l .
\end{align*}

This shows the claim. QED.

** Exercise 10.16
Show that adding one row of the parity check matrix to another does not change the
code. Using Gaussian elimination and swapping of bits it is therefore possible to assume
that the parity check matrix has the standard form $[A|I_{n-k}]$, where $A$ is an
$(n-k)\times\?k$ matrix.

*** Proof
Recall that a code $C$, in terms of a parity check matrix $H$, is defined as

$$
  y \in C \Leftrightarrow \forall i: \sum_j H_{ij} y_j = 0 .
$$

Let $H'$ be a matrix obtained from $H$ by adding row $k$ to row $l$. Let $C'$ be the code
defined by $H'$. We have to show $C'=C$. Let $y\in\?C$. Then we have

$$
  \forall i\neq l: \sum_j H'_{ij} y_j = 0 .
$$

because for those $i$ we have $H'_{ij}=H_{ij}$. Moreover

$$
  \sum_j H'_{lj} y_j = \sum_j H_{kj} y_j + \sum_j H_{lj} y_j =  0 ,
$$

by definition of $H'$. Hence $y\in\?C'$. Since $y\in\?C$ was arbitrary this shows
$C\subseteq\?C'$. By symmetry we also have $C'\subseteq\?C$. In fact, $H$ can be obtained
from $H'$ by adding row $k$ to line $l$ so the same reasoning applies to the reverse
inclusion. QED.

** Exercise 10.17
Find a parity check matrix for the $[6,2]$ repetition code defined by the generator matrix in
(10.54).

\begin{bmatrix}
1 & 0 \\
1 & 0 \\
1 & 0 \\
0 & 1 \\
0 & 1 \\
0 & 1 \\
\end{bmatrix}

*** Solution
Let $G_1$ and $H_1$ be the generator and parity check matrix for $k=1$ bits (see equations
(10.53) and (10.58)). We have

$$
  G_2 = I_2 \otimes G_1 .
$$

(C.f. the [[#exercise-10.14-solution][solution]] of exercise 10.14.) This suggests to define

$$
  H_2 = I_2 \otimes H_1 = \begin{bmatrix}
    1 & 1 & 0 & 0 & 0 & 0 \\
    0 & 1 & 1 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1 & 1 & 0 \\
    0 & 0 & 0 & 0 & 1 & 1
  \?\end{bmatrix} .
$$

This clearly does the job. It makes sense to compare this to [[#exercise-10.18][exercise 10.18]].

** Exercise 10.18
:PROPERTIES:
:CUSTOM_ID: exercise-10.18
:END:
Show that the parity check matrix $H$ and generator matrix $G$ for the same linear code
satisfy $HG=0$.

*** Proof
Recall that a code $C$ is just the image of the generator $C=G\BB^k$. On the other hand it
can be defined as the kernel of the parity check matrix: $C=\{y|Hy=0\}$. Since every code
word has the form $y=Gx$ we see that $HGx=0$ for every $x$. Hence the claim follows. QED.

** Exercise 10.19
Suppose an $[n,k]$ linear code $C$ has a parity check matrix of the form $H=[A|I_{n-k}]$,
for some $(n-k)\times\?k$ matrix $A$. Show that the corresponding generator matrix is

$$
  G = \left[ \begin{array}{c} I_k \\ \hline -A \end{array} \right]
$$

(Note that $-A=A$ since we are working modulo 2; however, this equation also holds for
linear codes over more general ﬁelds than $\ZZ_2$.)

*** Solution
Clearly $G$ has rank $k$. Hence its images has the same dimension as the kernel of $H$
(which is $k$). We only have to verify that $HG=0$ (c.f. [[#exercise-10.18][exercise 10.18]]). But this is
easy:

$$
  HG = A I_k - I_{n-k} A = A - A = 0 .
$$

** Exercise 10.20
:PROPERTIES:
:CUSTOM_ID: exercise-10.20
:END:
Let $H$ be a parity check matrix such that any $d-1$ columns are linearly independent, but
there exists a set of $d$ linearly dependent columns. Show that the code defined by $H$
has distance $d$.

*** Proof
Let us divide the proof into two parts. In part one we assume that $H$ has $d$ linearly
/dependent/ columns. In part two we assume that every $d-1$ columns are linearly
/independent/.

- Part 1 :: Assume that $H$ has $d$ linearly /dependent/ columns $c_1,\ldots,c_d$. We want
  to show that there exists a $y\in\?C$ (i.e. $y\in\BB^n$ such that $Hy=0$) with
  $\wt{y}\leq\?d$.

  Linear dependence means that there exist coefficients $\alpha_i\in\BB$ such that

  $$
  0 = \sum_i \alpha_i c_i .
  $$

  Without loss of generality we may assume that all $\alpha_i$ are equal to $1$. Otherwise
  there would exist a subset of $d'\?<\?d$ columns such that this is fulfilled.

  Let $J=\{i_1,\ldots,i_d\}$ be the indices of these columns in $H$. Let $y\in\BB^n$ be
  such that $y_i=1$ if $i\in\?J$ and $y_i=0$ otherwise. By construction we have $\wt{y}=d$
  and $Hy=0$. This shows the claim of part 1.
- Part 2 :: Assume that every $d-1$ columns are linearly /independent/. We have to show
  that $Hy=0$ implies $\wt{y}\geq\?d$.

  The assumption implies that every for every $y$ with $\wt{y}\leq\?d-1$ we have
  $Hy\neq0$. But this is equivalent to the claim.

Taking parts 1 and 2 together we see that $d(C)=d$ is equivalent to: Any $d-1$ columns of
$H$ are linearly independent but there exist $d$ columns which are linearly
dependent. QED.

** Exercise 10.21 (Singleton bound)
Show that an $[n,k,d]$ code must satisfy $n-k\geq\?d-1$.

*** Proof
This follows directly from [[#exercise-10.20][exercise 10.20]]. In fact, the parity check matrix
$H\in\BB^{(n-k)\times\?n}$ of the code must have full rank $n-k$ in order for the code
space to be $k$​-dimensional (otherwise it would be bigger and there could be no bijection
between the code words and the elements of $\BB^k$).

On the other hand, from $d(C)=d$, using exercise 10.20, we see that the rank of $H$ must
be at least $d-1$. Hence $n-k\geq\?d-1$. QED.
